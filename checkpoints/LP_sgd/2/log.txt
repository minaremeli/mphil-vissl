INFO 2022-05-16 04:41:29,440 train.py:  94: Env set for rank: 0, dist_rank: 0
INFO 2022-05-16 04:41:29,441 env.py:  50: ARCH:	x86_64
INFO 2022-05-16 04:41:29,442 env.py:  50: BASH_ENV:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/lmod/lmod/init/bash
INFO 2022-05-16 04:41:29,442 env.py:  50: BASH_FUNC_ml%%:	() {  eval $($LMOD_DIR/ml_cmd "$@")
}
INFO 2022-05-16 04:41:29,443 env.py:  50: BASH_FUNC_module%%:	() {  eval $($LMOD_CMD bash "$@") && eval $(${LMOD_SETTARG_CMD:-:} -s sh)
}
INFO 2022-05-16 04:41:29,443 env.py:  50: CMAKE_LIBRARY_PATH:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/gcc/7.4.0/lib64
INFO 2022-05-16 04:41:29,444 env.py:  50: CMAKE_PREFIX_PATH:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/gcc/7.4.0
INFO 2022-05-16 04:41:29,444 env.py:  50: COLUMNS:	124
INFO 2022-05-16 04:41:29,445 env.py:  50: CONDA_ACTIVATE:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/etc/profile.d/conda.sh
INFO 2022-05-16 04:41:29,446 env.py:  50: CONDA_DEFAULT_ENV:	vissl_env
INFO 2022-05-16 04:41:29,447 env.py:  50: CONDA_EXE:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/bin/conda
INFO 2022-05-16 04:41:29,448 env.py:  50: CONDA_PREFIX:	/home/mila/r/rajkuman/.conda/envs/vissl_env
INFO 2022-05-16 04:41:29,448 env.py:  50: CONDA_PROMPT_MODIFIER:	(vissl_env) 
INFO 2022-05-16 04:41:29,449 env.py:  50: CONDA_PYTHON_EXE:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/bin/python
INFO 2022-05-16 04:41:29,449 env.py:  50: CONDA_SHLVL:	1
INFO 2022-05-16 04:41:29,450 env.py:  50: CPATH:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/include
INFO 2022-05-16 04:41:29,451 env.py:  50: CSPYTHONPREFIXES:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3
INFO 2022-05-16 04:41:29,451 env.py:  50: CUDA_VISIBLE_DEVICES:	0
INFO 2022-05-16 04:41:29,452 env.py:  50: DBUS_SESSION_BUS_ADDRESS:	unix:path=/run/user/1471600619/bus
INFO 2022-05-16 04:41:29,452 env.py:  50: ENVIRONMENT:	BATCH
INFO 2022-05-16 04:41:29,453 env.py:  50: GPU_DEVICE_ORDINAL:	0
INFO 2022-05-16 04:41:29,453 env.py:  50: HOME:	/home/mila/r/rajkuman
INFO 2022-05-16 04:41:29,454 env.py:  50: HOSTNAME:	mila01
INFO 2022-05-16 04:41:29,454 env.py:  50: ID:	debian
INFO 2022-05-16 04:41:29,455 env.py:  50: JPY_API_TOKEN:	e960c7b7a4ec499483f421b612c72a5e
INFO 2022-05-16 04:41:29,455 env.py:  50: JUPYTERHUB_ACTIVITY_URL:	http://172.16.2.123:8081/hub/api/users/rajkuman/activity
INFO 2022-05-16 04:41:29,456 env.py:  50: JUPYTERHUB_API_TOKEN:	e960c7b7a4ec499483f421b612c72a5e
INFO 2022-05-16 04:41:29,456 env.py:  50: JUPYTERHUB_API_URL:	http://172.16.2.123:8081/hub/api
INFO 2022-05-16 04:41:29,457 env.py:  50: JUPYTERHUB_BASE_URL:	/
INFO 2022-05-16 04:41:29,457 env.py:  50: JUPYTERHUB_CLIENT_ID:	jupyterhub-user-rajkuman
INFO 2022-05-16 04:41:29,458 env.py:  50: JUPYTERHUB_HOST:	
INFO 2022-05-16 04:41:29,458 env.py:  50: JUPYTERHUB_OAUTH_CALLBACK_URL:	/user/rajkuman/oauth_callback
INFO 2022-05-16 04:41:29,459 env.py:  50: JUPYTERHUB_SERVER_NAME:	
INFO 2022-05-16 04:41:29,460 env.py:  50: JUPYTERHUB_SERVICE_PREFIX:	/user/rajkuman/
INFO 2022-05-16 04:41:29,460 env.py:  50: JUPYTERHUB_USER:	rajkuman
INFO 2022-05-16 04:41:29,460 env.py:  50: JUPYTER_SERVER_ROOT:	/home/mila/r/rajkuman
INFO 2022-05-16 04:41:29,461 env.py:  50: JUPYTER_SERVER_URL:	http://0.0.0.0:46141/user/rajkuman/
INFO 2022-05-16 04:41:29,461 env.py:  50: KERNEL_LAUNCH_TIMEOUT:	40
INFO 2022-05-16 04:41:29,462 env.py:  50: LANG:	en_US.UTF-8
INFO 2022-05-16 04:41:29,462 env.py:  50: LESSCLOSE:	/bin/lesspipe %s %s
INFO 2022-05-16 04:41:29,463 env.py:  50: LESSOPEN:	| /bin/lesspipe %s
INFO 2022-05-16 04:41:29,466 env.py:  50: LIBRARY_PATH:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib:/cvmfs/ai.mila.quebec/apps/x86_64/debian/gcc/7.4.0/lib64:/cvmfs/ai.mila.quebec/apps/x86_64/debian/gcc/7.4.0/lib
INFO 2022-05-16 04:41:29,466 env.py:  50: LINES:	42
INFO 2022-05-16 04:41:29,467 env.py:  50: LMOD_CMD:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/lmod/lmod/libexec/lmod
INFO 2022-05-16 04:41:29,467 env.py:  50: LMOD_DIR:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/lmod/lmod/libexec
INFO 2022-05-16 04:41:29,468 env.py:  50: LMOD_PACKAGE_PATH:	/cvmfs/config.mila.quebec/etc/lmod/
INFO 2022-05-16 04:41:29,469 env.py:  50: LMOD_PKG:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/lmod/lmod
INFO 2022-05-16 04:41:29,469 env.py:  50: LMOD_ROOT:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/lmod
INFO 2022-05-16 04:41:29,471 env.py:  50: LMOD_SETTARG_FULL_SUPPORT:	no
INFO 2022-05-16 04:41:29,471 env.py:  50: LMOD_SYSTEM_DEFAULT_MODULES:	Mila:gcc/7.4.0
INFO 2022-05-16 04:41:29,472 env.py:  50: LMOD_VERSION:	8.3.17
INFO 2022-05-16 04:41:29,472 env.py:  50: LMOD_sys:	Linux
INFO 2022-05-16 04:41:29,474 env.py:  50: LOADEDMODULES:	Mila:gcc/7.4.0:anaconda/3
INFO 2022-05-16 04:41:29,477 env.py:  50: LOCAL_RANK:	0
INFO 2022-05-16 04:41:29,478 env.py:  50: LOGNAME:	rajkuman
INFO 2022-05-16 04:41:29,478 env.py:  50: LS_COLORS:	rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:
INFO 2022-05-16 04:41:29,480 env.py:  50: MAIL:	/var/mail/rajkuman
INFO 2022-05-16 04:41:29,482 env.py:  50: MANPATH:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/share/man:/cvmfs/ai.mila.quebec/apps/x86_64/debian/gcc/7.4.0/share/man:/cvmfs/ai.mila.quebec/apps/x86_64/debian/lmod/lmod/share/man::
INFO 2022-05-16 04:41:29,483 env.py:  50: MODULEPATH:	/cvmfs/config.mila.quebec/modules/Core:/cvmfs/config.mila.quebec/modules/Compiler:/cvmfs/config.mila.quebec/modules/Environments:/cvmfs/config.mila.quebec/modules/Cuda:/cvmfs/config.mila.quebec/modules/Pytorch:/cvmfs/config.mila.quebec/modules/Tensorflow
INFO 2022-05-16 04:41:29,483 env.py:  50: MODULEPATH_ROOT:	/cvmfs/config.mila.quebec/modules
INFO 2022-05-16 04:41:29,484 env.py:  50: MODULERCFILE:	/cvmfs/config.mila.quebec/etc/lmod/modulerc.lua
INFO 2022-05-16 04:41:29,485 env.py:  50: MODULESHOME:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/lmod/lmod
INFO 2022-05-16 04:41:29,485 env.py:  50: OLDPWD:	/home/mila/r/rajkuman
INFO 2022-05-16 04:41:29,486 env.py:  50: PATH:	/home/mila/r/rajkuman/.conda/envs/vissl_env/bin:/home/mila/r/rajkuman/.local/bin:/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/condabin:/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/bin:/opt/slurm/bin:/sbin:/bin:/usr/sbin:/usr/bin
INFO 2022-05-16 04:41:29,486 env.py:  50: PKG_CONFIG_PATH:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/pkgconfig
INFO 2022-05-16 04:41:29,486 env.py:  50: PROCESSOR_ARCHITECTURE:	amd64
INFO 2022-05-16 04:41:29,487 env.py:  50: PWD:	/home/mila/r/rajkuman/mina/mphil-vissl
INFO 2022-05-16 04:41:29,487 env.py:  50: PYTHONNOUSERSITE:	True
INFO 2022-05-16 04:41:29,488 env.py:  50: PYTHONPATH:	/cvmfs/config.mila.quebec/etc/python.d/3.7
INFO 2022-05-16 04:41:29,489 env.py:  50: PYXTERM_DIMENSIONS:	80x25
INFO 2022-05-16 04:41:29,489 env.py:  50: RANK:	0
INFO 2022-05-16 04:41:29,490 env.py:  50: ROCR_VISIBLE_DEVICES:	0
INFO 2022-05-16 04:41:29,490 env.py:  50: SACCT_FORMAT:	User,JobID,Jobname,partition,state,time,start,end,elapsed,nnodes,ncpus,reqmem,alloctres,nodelist,workdir
INFO 2022-05-16 04:41:29,491 env.py:  50: SCRATCH:	/network/scratch/r/rajkuman
INFO 2022-05-16 04:41:29,491 env.py:  50: SHELL:	/bin/bash
INFO 2022-05-16 04:41:29,492 env.py:  50: SHLVL:	3
INFO 2022-05-16 04:41:29,492 env.py:  50: SINFO_FORMAT:	%18N %.6D %.11T %.4c %.8z %.6m %.8d %.6w %.22f %80E
INFO 2022-05-16 04:41:29,493 env.py:  50: SLURMD_NODENAME:	mila01
INFO 2022-05-16 04:41:29,494 env.py:  50: SLURM_CLUSTER_NAME:	mila
INFO 2022-05-16 04:41:29,494 env.py:  50: SLURM_CONF:	/etc/slurm/slurm.conf
INFO 2022-05-16 04:41:29,495 env.py:  50: SLURM_CPUS_ON_NODE:	4
INFO 2022-05-16 04:41:29,498 env.py:  50: SLURM_CPUS_PER_TASK:	4
INFO 2022-05-16 04:41:29,500 env.py:  50: SLURM_EXPORT_ENV:	PATH,LANG,USER,HOME,SHELL,JUPYTERHUB_API_TOKEN,JPY_API_TOKEN,JUPYTERHUB_CLIENT_ID,JUPYTERHUB_HOST,JUPYTERHUB_OAUTH_CALLBACK_URL,JUPYTERHUB_USER,JUPYTERHUB_SERVER_NAME,JUPYTERHUB_API_URL,JUPYTERHUB_ACTIVITY_URL,JUPYTERHUB_BASE_URL,JUPYTERHUB_SERVICE_PREFIX
INFO 2022-05-16 04:41:29,500 env.py:  50: SLURM_GET_USER_ENV:	1
INFO 2022-05-16 04:41:29,501 env.py:  50: SLURM_GTIDS:	0
INFO 2022-05-16 04:41:29,501 env.py:  50: SLURM_JOBID:	1827791
INFO 2022-05-16 04:41:29,502 env.py:  50: SLURM_JOB_ACCOUNT:	mila
INFO 2022-05-16 04:41:29,502 env.py:  50: SLURM_JOB_CPUS_PER_NODE:	4
INFO 2022-05-16 04:41:29,503 env.py:  50: SLURM_JOB_GID:	1471600619
INFO 2022-05-16 04:41:29,503 env.py:  50: SLURM_JOB_GPUS:	3
INFO 2022-05-16 04:41:29,504 env.py:  50: SLURM_JOB_ID:	1827791
INFO 2022-05-16 04:41:29,505 env.py:  50: SLURM_JOB_NAME:	jupyterhub-rajkuman
INFO 2022-05-16 04:41:29,505 env.py:  50: SLURM_JOB_NODELIST:	mila01
INFO 2022-05-16 04:41:29,506 env.py:  50: SLURM_JOB_NUM_NODES:	1
INFO 2022-05-16 04:41:29,506 env.py:  50: SLURM_JOB_PARTITION:	unkillable
INFO 2022-05-16 04:41:29,507 env.py:  50: SLURM_JOB_QOS:	normal
INFO 2022-05-16 04:41:29,507 env.py:  50: SLURM_JOB_UID:	1471600619
INFO 2022-05-16 04:41:29,507 env.py:  50: SLURM_JOB_USER:	rajkuman
INFO 2022-05-16 04:41:29,508 env.py:  50: SLURM_LOCALID:	0
INFO 2022-05-16 04:41:29,509 env.py:  50: SLURM_MEM_PER_NODE:	24000
INFO 2022-05-16 04:41:29,510 env.py:  50: SLURM_NNODES:	1
INFO 2022-05-16 04:41:29,511 env.py:  50: SLURM_NODEID:	0
INFO 2022-05-16 04:41:29,511 env.py:  50: SLURM_NODELIST:	mila01
INFO 2022-05-16 04:41:29,511 env.py:  50: SLURM_NODE_ALIASES:	(null)
INFO 2022-05-16 04:41:29,512 env.py:  50: SLURM_NPROCS:	1
INFO 2022-05-16 04:41:29,512 env.py:  50: SLURM_NTASKS:	1
INFO 2022-05-16 04:41:29,513 env.py:  50: SLURM_PRIO_PROCESS:	0
INFO 2022-05-16 04:41:29,513 env.py:  50: SLURM_PROCID:	0
INFO 2022-05-16 04:41:29,514 env.py:  50: SLURM_SUBMIT_DIR:	/var/lib/jupyterhub
INFO 2022-05-16 04:41:29,515 env.py:  50: SLURM_SUBMIT_HOST:	jupyter
INFO 2022-05-16 04:41:29,515 env.py:  50: SLURM_TASKS_PER_NODE:	1
INFO 2022-05-16 04:41:29,516 env.py:  50: SLURM_TASK_PID:	33829
INFO 2022-05-16 04:41:29,516 env.py:  50: SLURM_TMPDIR:	/Tmp/slurm.1827791.0
INFO 2022-05-16 04:41:29,518 env.py:  50: SLURM_TOPOLOGY_ADDR:	mila01
INFO 2022-05-16 04:41:29,519 env.py:  50: SLURM_TOPOLOGY_ADDR_PATTERN:	node
INFO 2022-05-16 04:41:29,520 env.py:  50: SLURM_WORKING_CLUSTER:	mila:slurm:6817:9216:109
INFO 2022-05-16 04:41:29,520 env.py:  50: SQUEUE_FORMAT:	%.8i %.8u %.12P %.14j %.3t %16S %.10M %.5D %.4C %.10b %.7m %N (%r) %k
INFO 2022-05-16 04:41:29,521 env.py:  50: S_COLORS:	auto
INFO 2022-05-16 04:41:29,521 env.py:  50: TERM:	xterm
INFO 2022-05-16 04:41:29,522 env.py:  50: TMPDIR:	/tmp
INFO 2022-05-16 04:41:29,522 env.py:  50: USER:	rajkuman
INFO 2022-05-16 04:41:29,525 env.py:  50: WORLD_SIZE:	1
INFO 2022-05-16 04:41:29,526 env.py:  50: XDG_SESSION_ID:	c181
INFO 2022-05-16 04:41:29,526 env.py:  50: _:	/home/mila/r/rajkuman/.conda/envs/vissl_env/bin/python
INFO 2022-05-16 04:41:29,527 env.py:  50: _CE_CONDA:	
INFO 2022-05-16 04:41:29,527 env.py:  50: _CE_M:	
INFO 2022-05-16 04:41:29,528 env.py:  50: _LMFILES_:	/cvmfs/config.mila.quebec/modules/Core/Mila.lua:/cvmfs/config.mila.quebec/modules/Core/gcc/7.4.0.lua:/cvmfs/config.mila.quebec/modules/Core/anaconda/3.lua
INFO 2022-05-16 04:41:29,529 env.py:  50: _ModuleTable001_:	X01vZHVsZVRhYmxlXz17WyJNVHZlcnNpb24iXT0zLFsiY19yZWJ1aWxkVGltZSJdPWZhbHNlLFsiY19zaG9ydFRpbWUiXT1mYWxzZSxkZXB0aFQ9e30sZmFtaWx5PXt9LG1UPXtNaWxhPXtbImZuIl09Ii9jdm1mcy9jb25maWcubWlsYS5xdWViZWMvbW9kdWxlcy9Db3JlL01pbGEubHVhIixbImZ1bGxOYW1lIl09Ik1pbGEiLFsibG9hZE9yZGVyIl09MSxwcm9wVD17bG1vZD17WyJzdGlja3kiXT0xLH0sfSxbInN0YWNrRGVwdGgiXT0wLFsic3RhdHVzIl09ImFjdGl2ZSIsWyJ1c2VyTmFtZSJdPSJNaWxhIix9LGFuYWNvbmRhPXtbImZuIl09Ii9jdm1mcy9jb25maWcubWlsYS5xdWViZWMvbW9kdWxlcy9Db3JlL2FuYWNvbmRhLzMubHVhIixbImZ1bGxOYW1lIl09ImFuYWNvbmRh
INFO 2022-05-16 04:41:29,532 env.py:  50: _ModuleTable002_:	LzMiLFsibG9hZE9yZGVyIl09Myxwcm9wVD17fSxbInN0YWNrRGVwdGgiXT0wLFsic3RhdHVzIl09ImFjdGl2ZSIsWyJ1c2VyTmFtZSJdPSJhbmFjb25kYS8zIix9LGdjYz17WyJmbiJdPSIvY3ZtZnMvY29uZmlnLm1pbGEucXVlYmVjL21vZHVsZXMvQ29yZS9nY2MvNy40LjAubHVhIixbImZ1bGxOYW1lIl09ImdjYy83LjQuMCIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXtsbW9kPXtbInN0aWNreSJdPTEsfSx9LFsic3RhY2tEZXB0aCJdPTAsWyJzdGF0dXMiXT0iYWN0aXZlIixbInVzZXJOYW1lIl09ImdjYy83LjQuMCIsfSx9LG1wYXRoQT17Ii9jdm1mcy9jb25maWcubWlsYS5xdWViZWMvbW9kdWxlcy9Db3JlIiwiL2N2bWZzL2NvbmZpZy5taWxhLnF1ZWJlYy9tb2R1bGVzL0Nv
INFO 2022-05-16 04:41:29,533 env.py:  50: _ModuleTable003_:	bXBpbGVyIiwiL2N2bWZzL2NvbmZpZy5taWxhLnF1ZWJlYy9tb2R1bGVzL0Vudmlyb25tZW50cyIsIi9jdm1mcy9jb25maWcubWlsYS5xdWViZWMvbW9kdWxlcy9DdWRhIiwiL2N2bWZzL2NvbmZpZy5taWxhLnF1ZWJlYy9tb2R1bGVzL1B5dG9yY2giLCIvY3ZtZnMvY29uZmlnLm1pbGEucXVlYmVjL21vZHVsZXMvVGVuc29yZmxvdyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvY3ZtZnMvY29uZmlnLm1pbGEucXVlYmVjL21vZHVsZXMvQ29yZTovY3ZtZnMvY29uZmlnLm1pbGEucXVlYmVjL21vZHVsZXMvQ29tcGlsZXI6L2N2bWZzL2NvbmZpZy5taWxhLnF1ZWJlYy9tb2R1bGVzL0Vudmlyb25tZW50czovY3ZtZnMvY29uZmlnLm1pbGEucXVlYmVjL21vZHVsZXMvQ3VkYTovY3Zt
INFO 2022-05-16 04:41:29,533 env.py:  50: _ModuleTable004_:	ZnMvY29uZmlnLm1pbGEucXVlYmVjL21vZHVsZXMvUHl0b3JjaDovY3ZtZnMvY29uZmlnLm1pbGEucXVlYmVjL21vZHVsZXMvVGVuc29yZmxvdyIsfQ==
INFO 2022-05-16 04:41:29,534 env.py:  50: _ModuleTable_Sz_:	4
INFO 2022-05-16 04:41:29,534 env.py:  50: __Init_Default_Modules:	1
INFO 2022-05-16 04:41:29,534 env.py:  50: __LMOD_REF_COUNT_CMAKE_LIBRARY_PATH:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/gcc/7.4.0/lib64:1
INFO 2022-05-16 04:41:29,535 env.py:  50: __LMOD_REF_COUNT_CMAKE_PREFIX_PATH:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/gcc/7.4.0:1
INFO 2022-05-16 04:41:29,536 env.py:  50: __LMOD_REF_COUNT_CPATH:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/include:1
INFO 2022-05-16 04:41:29,536 env.py:  50: __LMOD_REF_COUNT_CSPYTHONPREFIXES:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3:1
INFO 2022-05-16 04:41:29,537 env.py:  50: __LMOD_REF_COUNT_LIBRARY_PATH:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib:1;/cvmfs/ai.mila.quebec/apps/x86_64/debian/gcc/7.4.0/lib64:1;/cvmfs/ai.mila.quebec/apps/x86_64/debian/gcc/7.4.0/lib:1
INFO 2022-05-16 04:41:29,540 env.py:  50: __LMOD_REF_COUNT_LOADEDMODULES:	Mila:1;gcc/7.4.0:1;anaconda/3:1
INFO 2022-05-16 04:41:29,540 env.py:  50: __LMOD_REF_COUNT_MANPATH:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/share/man:1;/cvmfs/ai.mila.quebec/apps/x86_64/debian/gcc/7.4.0/share/man:1;/cvmfs/ai.mila.quebec/apps/x86_64/debian/lmod/lmod/share/man:1
INFO 2022-05-16 04:41:29,541 env.py:  50: __LMOD_REF_COUNT_MODULEPATH:	/cvmfs/config.mila.quebec/modules/Core:1;/cvmfs/config.mila.quebec/modules/Compiler:1;/cvmfs/config.mila.quebec/modules/Environments:1;/cvmfs/config.mila.quebec/modules/Cuda:1;/cvmfs/config.mila.quebec/modules/Pytorch:1;/cvmfs/config.mila.quebec/modules/Tensorflow:1
INFO 2022-05-16 04:41:29,541 env.py:  50: __LMOD_REF_COUNT_PATH:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/bin:1;/opt/slurm/bin:1;/sbin:1;/bin:1;/usr/sbin:1;/usr/bin:1
INFO 2022-05-16 04:41:29,541 env.py:  50: __LMOD_REF_COUNT_PKG_CONFIG_PATH:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/pkgconfig:1
INFO 2022-05-16 04:41:29,542 env.py:  50: __LMOD_REF_COUNT_PYTHONPATH:	/cvmfs/config.mila.quebec/etc/python.d/3.7:1
INFO 2022-05-16 04:41:29,543 env.py:  50: __LMOD_REF_COUNT__LMFILES_:	/cvmfs/config.mila.quebec/modules/Core/Mila.lua:1;/cvmfs/config.mila.quebec/modules/Core/gcc/7.4.0.lua:1;/cvmfs/config.mila.quebec/modules/Core/anaconda/3.lua:1
INFO 2022-05-16 04:41:29,543 misc.py: 161: Set start method of multiprocessing to forkserver
INFO 2022-05-16 04:41:29,544 train.py: 105: Setting seed....
INFO 2022-05-16 04:41:29,544 misc.py: 173: MACHINE SEED: 0
INFO 2022-05-16 04:41:29,571 hydra_config.py: 132: Training with config:
INFO 2022-05-16 04:41:29,584 hydra_config.py: 141: {'CHECKPOINT': {'APPEND_DISTR_RUN_ID': False,
                'AUTO_RESUME': True,
                'BACKEND': 'disk',
                'CHECKPOINT_FREQUENCY': 20,
                'CHECKPOINT_ITER_FREQUENCY': -1,
                'DIR': './checkpoints/LP_sgd/2/',
                'LATEST_CHECKPOINT_RESUME_FILE_NUM': 1,
                'OVERWRITE_EXISTING': False,
                'USE_SYMLINK_CHECKPOINT_FOR_RESUME': False},
 'CLUSTERFIT': {'CLUSTER_BACKEND': 'faiss',
                'DATA_LIMIT': -1,
                'DATA_LIMIT_SAMPLING': {'SEED': 0},
                'FEATURES': {'DATASET_NAME': '',
                             'DATA_PARTITION': 'TRAIN',
                             'DIMENSIONALITY_REDUCTION': 0,
                             'EXTRACT': False,
                             'LAYER_NAME': '',
                             'PATH': '.',
                             'TEST_PARTITION': 'TEST'},
                'NUM_CLUSTERS': 16000,
                'NUM_ITER': 50,
                'OUTPUT_DIR': '.'},
 'DATA': {'DDP_BUCKET_CAP_MB': 25,
          'ENABLE_ASYNC_GPU_COPY': True,
          'NUM_DATALOADER_WORKERS': 4,
          'PIN_MEMORY': True,
          'TEST': {'BASE_DATASET': 'generic_ssl',
                   'BATCHSIZE_PER_REPLICA': 512,
                   'COLLATE_FUNCTION': 'default_collate',
                   'COLLATE_FUNCTION_PARAMS': {},
                   'COPY_DESTINATION_DIR': '/tmp/cifar100/',
                   'COPY_TO_LOCAL_DISK': False,
                   'DATASET_NAMES': ['CIFAR100'],
                   'DATA_LIMIT': -1,
                   'DATA_LIMIT_SAMPLING': {'IS_BALANCED': False,
                                           'SEED': 0,
                                           'SKIP_NUM_SAMPLES': 0},
                   'DATA_PATHS': [],
                   'DATA_SOURCES': ['disk_folder'],
                   'DEFAULT_GRAY_IMG_SIZE': 224,
                   'DROP_LAST': False,
                   'ENABLE_QUEUE_DATASET': False,
                   'INPUT_KEY_NAMES': ['data'],
                   'LABEL_PATHS': [],
                   'LABEL_SOURCES': ['disk_folder'],
                   'LABEL_TYPE': 'standard',
                   'MMAP_MODE': True,
                   'NEW_IMG_PATH_PREFIX': '',
                   'RANDOM_SYNTHETIC_IMAGES': False,
                   'REMOVE_IMG_PATH_PREFIX': '',
                   'TARGET_KEY_NAMES': ['label'],
                   'TRANSFORMS': [{'name': 'ToTensor'},
                                  {'mean': [0.5074, 0.4867, 0.4411],
                                   'name': 'Normalize',
                                   'std': [0.2011, 0.1987, 0.202]}],
                   'USE_DEBUGGING_SAMPLER': False,
                   'USE_STATEFUL_DISTRIBUTED_SAMPLER': False},
          'TRAIN': {'BASE_DATASET': 'generic_ssl',
                    'BATCHSIZE_PER_REPLICA': 512,
                    'COLLATE_FUNCTION': 'default_collate',
                    'COLLATE_FUNCTION_PARAMS': {},
                    'COPY_DESTINATION_DIR': '/tmp/cifar100/',
                    'COPY_TO_LOCAL_DISK': False,
                    'DATASET_NAMES': ['CIFAR100'],
                    'DATA_LIMIT': -1,
                    'DATA_LIMIT_SAMPLING': {'IS_BALANCED': False,
                                            'SEED': 0,
                                            'SKIP_NUM_SAMPLES': 0},
                    'DATA_PATHS': [],
                    'DATA_SOURCES': ['disk_folder'],
                    'DEFAULT_GRAY_IMG_SIZE': 224,
                    'DROP_LAST': False,
                    'ENABLE_QUEUE_DATASET': False,
                    'INPUT_KEY_NAMES': ['data'],
                    'LABEL_PATHS': [],
                    'LABEL_SOURCES': ['disk_folder'],
                    'LABEL_TYPE': 'standard',
                    'MMAP_MODE': True,
                    'NEW_IMG_PATH_PREFIX': '',
                    'RANDOM_SYNTHETIC_IMAGES': False,
                    'REMOVE_IMG_PATH_PREFIX': '',
                    'TARGET_KEY_NAMES': ['label'],
                    'TRANSFORMS': [{'name': 'ToTensor'},
                                   {'mean': [0.5074, 0.4867, 0.4411],
                                    'name': 'Normalize',
                                    'std': [0.2011, 0.1987, 0.202]}],
                    'USE_DEBUGGING_SAMPLER': False,
                    'USE_STATEFUL_DISTRIBUTED_SAMPLER': False}},
 'DISTRIBUTED': {'BACKEND': 'nccl',
                 'BROADCAST_BUFFERS': True,
                 'INIT_METHOD': 'tcp',
                 'MANUAL_GRADIENT_REDUCTION': False,
                 'NCCL_DEBUG': False,
                 'NCCL_SOCKET_NTHREADS': '',
                 'NUM_NODES': 1,
                 'NUM_PROC_PER_NODE': 1,
                 'RUN_ID': 'auto'},
 'EXTRACT_FEATURES': {'CHUNK_THRESHOLD': 0, 'OUTPUT_DIR': ''},
 'HOOKS': {'CHECK_NAN': True,
           'LOG_GPU_STATS': True,
           'MEMORY_SUMMARY': {'DUMP_MEMORY_ON_EXCEPTION': False,
                              'LOG_ITERATION_NUM': 0,
                              'PRINT_MEMORY_SUMMARY': True},
           'MODEL_COMPLEXITY': {'COMPUTE_COMPLEXITY': False,
                                'INPUT_SHAPE': [3, 224, 224]},
           'PERF_STATS': {'MONITOR_PERF_STATS': True,
                          'PERF_STAT_FREQUENCY': -1,
                          'ROLLING_BTIME_FREQ': -1},
           'TENSORBOARD_SETUP': {'EXPERIMENT_LOG_DIR': 'tensorboard',
                                 'FLUSH_EVERY_N_MIN': 5,
                                 'LOG_DIR': '.',
                                 'LOG_PARAMS': True,
                                 'LOG_PARAMS_EVERY_N_ITERS': 310,
                                 'LOG_PARAMS_GRADIENTS': True,
                                 'USE_TENSORBOARD': False}},
 'IMG_RETRIEVAL': {'CROP_QUERY_ROI': False,
                   'DATASET_PATH': '',
                   'DEBUG_MODE': False,
                   'EVAL_BINARY_PATH': '',
                   'EVAL_DATASET_NAME': 'Paris',
                   'FEATS_PROCESSING_TYPE': '',
                   'GEM_POOL_POWER': 4.0,
                   'IMG_SCALINGS': [1],
                   'NORMALIZE_FEATURES': True,
                   'NUM_DATABASE_SAMPLES': -1,
                   'NUM_QUERY_SAMPLES': -1,
                   'NUM_TRAINING_SAMPLES': -1,
                   'N_PCA': 512,
                   'RESIZE_IMG': 1024,
                   'SAVE_FEATURES': False,
                   'SAVE_RETRIEVAL_RANKINGS_SCORES': True,
                   'SIMILARITY_MEASURE': 'cosine_similarity',
                   'SPATIAL_LEVELS': 3,
                   'TRAIN_DATASET_NAME': 'Oxford',
                   'TRAIN_PCA_WHITENING': True,
                   'USE_DISTRACTORS': False,
                   'WHITEN_IMG_LIST': ''},
 'LOG_FREQUENCY': 200,
 'LOSS': {'CrossEntropyLoss': {'ignore_index': -1},
          'barlow_twins_loss': {'embedding_dim': 8192,
                                'lambda_': 0.0051,
                                'scale_loss': 0.024},
          'bce_logits_multiple_output_single_target': {'normalize_output': False,
                                                       'reduction': 'none',
                                                       'world_size': 1},
          'cross_entropy_multiple_output_single_target': {'ignore_index': -1,
                                                          'normalize_output': False,
                                                          'reduction': 'mean',
                                                          'temperature': 1.0,
                                                          'weight': None},
          'deepclusterv2_loss': {'BATCHSIZE_PER_REPLICA': 256,
                                 'DROP_LAST': True,
                                 'kmeans_iters': 10,
                                 'memory_params': {'crops_for_mb': [0],
                                                   'embedding_dim': 128},
                                 'num_clusters': [3000, 3000, 3000],
                                 'num_crops': 2,
                                 'num_train_samples': -1,
                                 'temperature': 0.1},
          'dino_loss': {'crops_for_teacher': [0, 1],
                        'ema_center': 0.9,
                        'momentum': 0.996,
                        'normalize_last_layer': True,
                        'output_dim': 65536,
                        'student_temp': 0.1,
                        'teacher_temp_max': 0.07,
                        'teacher_temp_min': 0.04,
                        'teacher_temp_warmup_iters': 37500},
          'moco_loss': {'embedding_dim': 128,
                        'momentum': 0.999,
                        'queue_size': 65536,
                        'temperature': 0.2},
          'multicrop_simclr_info_nce_loss': {'buffer_params': {'effective_batch_size': 4096,
                                                               'embedding_dim': 128,
                                                               'world_size': 64},
                                             'num_crops': 2,
                                             'temperature': 0.1},
          'name': 'cross_entropy_multiple_output_single_target',
          'nce_loss_with_memory': {'loss_type': 'nce',
                                   'loss_weights': [1.0],
                                   'memory_params': {'embedding_dim': 128,
                                                     'memory_size': -1,
                                                     'momentum': 0.5,
                                                     'norm_init': True,
                                                     'update_mem_on_forward': True},
                                   'negative_sampling_params': {'num_negatives': 16000,
                                                                'type': 'random'},
                                   'norm_constant': -1,
                                   'norm_embedding': True,
                                   'num_train_samples': -1,
                                   'temperature': 0.07,
                                   'update_mem_with_emb_index': -100},
          'simclr_info_nce_loss': {'buffer_params': {'effective_batch_size': 4096,
                                                     'embedding_dim': 128,
                                                     'world_size': 64},
                                   'temperature': 0.1},
          'swav_loss': {'crops_for_assign': [0, 1],
                        'embedding_dim': 128,
                        'epsilon': 0.05,
                        'normalize_last_layer': True,
                        'num_crops': 2,
                        'num_iters': 3,
                        'num_prototypes': [3000],
                        'output_dir': '.',
                        'queue': {'local_queue_length': 0,
                                  'queue_length': 0,
                                  'start_iter': 0},
                        'temp_hard_assignment_iters': 0,
                        'temperature': 0.1,
                        'use_double_precision': False},
          'swav_momentum_loss': {'crops_for_assign': [0, 1],
                                 'embedding_dim': 128,
                                 'epsilon': 0.05,
                                 'momentum': 0.99,
                                 'momentum_eval_mode_iter_start': 0,
                                 'normalize_last_layer': True,
                                 'num_crops': 2,
                                 'num_iters': 3,
                                 'num_prototypes': [3000],
                                 'queue': {'local_queue_length': 0,
                                           'queue_length': 0,
                                           'start_iter': 0},
                                 'temperature': 0.1,
                                 'use_double_precision': False}},
 'MACHINE': {'DEVICE': 'gpu'},
 'METERS': {'accuracy_list_meter': {'meter_names': ['flatten'],
                                    'num_meters': 1,
                                    'topk_values': [1, 5]},
            'enable_training_meter': True,
            'mean_ap_list_meter': {'max_cpu_capacity': -1,
                                   'meter_names': [],
                                   'num_classes': 9605,
                                   'num_meters': 1},
            'model_output_mask': False,
            'name': 'accuracy_list_meter',
            'names': ['accuracy_list_meter'],
            'precision_at_k_list_meter': {'meter_names': [],
                                          'num_meters': 1,
                                          'topk_values': [1]},
            'recall_at_k_list_meter': {'meter_names': [],
                                       'num_meters': 1,
                                       'topk_values': [1]}},
 'MODEL': {'ACTIVATION_CHECKPOINTING': {'NUM_ACTIVATION_CHECKPOINTING_SPLITS': 2,
                                        'USE_ACTIVATION_CHECKPOINTING': False},
           'AMP_PARAMS': {'AMP_ARGS': {'opt_level': 'O1'},
                          'AMP_TYPE': 'apex',
                          'USE_AMP': False},
           'BASE_MODEL_NAME': 'multi_input_output_model',
           'CUDA_CACHE': {'CLEAR_CUDA_CACHE': False, 'CLEAR_FREQ': 100},
           'FEATURE_EVAL_SETTINGS': {'EVAL_MODE_ON': True,
                                     'EVAL_TRUNK_AND_HEAD': False,
                                     'EXTRACT_TRUNK_FEATURES_ONLY': False,
                                     'FREEZE_TRUNK_AND_HEAD': False,
                                     'FREEZE_TRUNK_ONLY': True,
                                     'LINEAR_EVAL_FEAT_POOL_OPS_MAP': [['flatten',
                                                                        ['Identity',
                                                                         []]]],
                                     'SHOULD_FLATTEN_FEATS': False},
           'FSDP_CONFIG': {'AUTO_WRAP_THRESHOLD': 0,
                           'bucket_cap_mb': 0,
                           'clear_autocast_cache': True,
                           'compute_dtype': torch.float32,
                           'flatten_parameters': True,
                           'fp32_reduce_scatter': False,
                           'mixed_precision': True,
                           'verbose': True},
           'GRAD_CLIP': {'MAX_NORM': 1, 'NORM_TYPE': 2, 'USE_GRAD_CLIP': False},
           'HEAD': {'BATCHNORM_EPS': 1e-05,
                    'BATCHNORM_MOMENTUM': 0.1,
                    'PARAMS': [['eval_mlp',
                                {'dims': [512, 100], 'in_channels': 512}]],
                    'PARAMS_MULTIPLIER': 1.0},
           'INPUT_TYPE': 'rgb',
           'MULTI_INPUT_HEAD_MAPPING': [],
           'NON_TRAINABLE_PARAMS': [],
           'SHARDED_DDP_SETUP': {'USE_SDP': False, 'reduce_buffer_size': -1},
           'SINGLE_PASS_EVERY_CROP': False,
           'SYNC_BN_CONFIG': {'CONVERT_BN_TO_SYNC_BN': False,
                              'GROUP_SIZE': -1,
                              'SYNC_BN_TYPE': 'pytorch'},
           'TEMP_FROZEN_PARAMS_ITER_MAP': [],
           'TRUNK': {'CONVIT': {'CLASS_TOKEN_IN_LOCAL_LAYERS': False,
                                'LOCALITY_DIM': 10,
                                'LOCALITY_STRENGTH': 1.0,
                                'N_GPSA_LAYERS': 10,
                                'USE_LOCAL_INIT': True},
                     'EFFICIENT_NETS': {},
                     'NAME': 'resnet',
                     'REGNET': {},
                     'RESNETS': {'BLOCK': 'BasicBlock',
                                 'CONV1_KERNEL': 3,
                                 'CONV1_PADDING': 1,
                                 'CONV1_STRIDE': 1,
                                 'DEPTH': 18,
                                 'GROUPNORM_GROUPS': 32,
                                 'GROUPS': 1,
                                 'LAYER4_STRIDE': 2,
                                 'MAXPOOL': False,
                                 'NORM': 'BatchNorm',
                                 'STANDARDIZE_CONVOLUTIONS': False,
                                 'WIDTH_MULTIPLIER': 1,
                                 'WIDTH_PER_GROUP': 64,
                                 'ZERO_INIT_RESIDUAL': False},
                     'VISION_TRANSFORMERS': {'ATTENTION_DROPOUT_RATE': 0,
                                             'CLASSIFIER': 'token',
                                             'DROPOUT_RATE': 0,
                                             'DROP_PATH_RATE': 0,
                                             'HIDDEN_DIM': 768,
                                             'IMAGE_SIZE': 224,
                                             'MLP_DIM': 3072,
                                             'NUM_HEADS': 12,
                                             'NUM_LAYERS': 12,
                                             'PATCH_SIZE': 16,
                                             'QKV_BIAS': False,
                                             'QK_SCALE': False,
                                             'name': None},
                     'XCIT': {'ATTENTION_DROPOUT_RATE': 0,
                              'DROPOUT_RATE': 0,
                              'DROP_PATH_RATE': 0.05,
                              'ETA': 1,
                              'HIDDEN_DIM': 384,
                              'IMAGE_SIZE': 224,
                              'NUM_HEADS': 8,
                              'NUM_LAYERS': 12,
                              'PATCH_SIZE': 16,
                              'QKV_BIAS': True,
                              'QK_SCALE': False,
                              'TOKENS_NORM': True,
                              'name': None}},
           'WEIGHTS_INIT': {'APPEND_PREFIX': '',
                            'PARAMS_FILE': './checkpoints/trained_swav/2/model_final_checkpoint_phase499.torch',
                            'REMOVE_PREFIX': '',
                            'SKIP_LAYERS': ['num_batches_tracked'],
                            'STATE_DICT_KEY_NAME': 'classy_state_dict'},
           '_MODEL_INIT_SEED': 0},
 'MONITORING': {'MONITOR_ACTIVATION_STATISTICS': 0},
 'MULTI_PROCESSING_METHOD': 'forkserver',
 'NEAREST_NEIGHBOR': {'L2_NORM_FEATS': False, 'SIGMA': 0.1, 'TOPK': 200},
 'OPTIMIZER': {'betas': [0.9, 0.999],
               'construct_single_param_group_only': False,
               'head_optimizer_params': {'use_different_lr': False,
                                         'use_different_wd': False,
                                         'weight_decay': 1e-06},
               'larc_config': {'clip': False,
                               'eps': 1e-08,
                               'trust_coefficient': 0.001},
               'momentum': 0.9,
               'name': 'sgd',
               'nesterov': False,
               'non_regularized_parameters': [],
               'num_epochs': 100,
               'param_schedulers': {'lr': {'auto_lr_scaling': {'auto_scale': False,
                                                               'base_lr_batch_size': 256,
                                                               'base_value': 0.1,
                                                               'scaling_type': 'linear'},
                                           'end_value': 0.0,
                                           'interval_scaling': [],
                                           'lengths': [],
                                           'milestones': [30, 60],
                                           'name': 'cosine',
                                           'schedulers': [],
                                           'start_value': 0.03,
                                           'update_interval': 'epoch',
                                           'value': 0.1,
                                           'values': [0.1, 0.01, 0.001]},
                                    'lr_head': {'auto_lr_scaling': {'auto_scale': False,
                                                                    'base_lr_batch_size': 256,
                                                                    'base_value': 0.1,
                                                                    'scaling_type': 'linear'},
                                                'end_value': 0.0,
                                                'interval_scaling': [],
                                                'lengths': [],
                                                'milestones': [30, 60],
                                                'name': 'cosine',
                                                'schedulers': [],
                                                'start_value': 0.03,
                                                'update_interval': 'epoch',
                                                'value': 0.1,
                                                'values': [0.1, 0.01, 0.001]}},
               'regularize_bias': True,
               'regularize_bn': False,
               'use_larc': False,
               'use_zero': False,
               'weight_decay': 1e-06},
 'PROFILING': {'MEMORY_PROFILING': {'TRACK_BY_LAYER_MEMORY': False},
               'NUM_ITERATIONS': 10,
               'OUTPUT_FOLDER': '.',
               'PROFILED_RANKS': [0, 1],
               'RUNTIME_PROFILING': {'LEGACY_PROFILER': False,
                                     'PROFILE_CPU': True,
                                     'PROFILE_GPU': True,
                                     'USE_PROFILER': False},
               'START_ITERATION': 0,
               'STOP_TRAINING_AFTER_PROFILING': False,
               'WARMUP_ITERATIONS': 0},
 'REPRODUCIBILITY': {'CUDDN_DETERMINISTIC': False},
 'SEED_VALUE': 0,
 'SLURM': {'ADDITIONAL_PARAMETERS': {},
           'COMMENT': 'vissl job',
           'CONSTRAINT': '',
           'LOG_FOLDER': '.',
           'MEM_GB': 250,
           'NAME': 'vissl',
           'NUM_CPU_PER_PROC': 8,
           'PARTITION': '',
           'PORT_ID': 40050,
           'TIME_HOURS': 72,
           'TIME_MINUTES': 0,
           'USE_SLURM': False},
 'SVM': {'cls_list': [],
         'costs': {'base': -1.0,
                   'costs_list': [0.1, 0.01],
                   'power_range': [4, 20]},
         'cross_val_folds': 3,
         'dual': True,
         'force_retrain': False,
         'loss': 'squared_hinge',
         'low_shot': {'dataset_name': 'voc',
                      'k_values': [1, 2, 4, 8, 16, 32, 64, 96],
                      'sample_inds': [1, 2, 3, 4, 5]},
         'max_iter': 2000,
         'normalize': True,
         'penalty': 'l2'},
 'TEST_EVERY_NUM_EPOCH': 1,
 'TEST_MODEL': True,
 'TEST_ONLY': False,
 'TRAINER': {'TASK_NAME': 'self_supervision_task',
             'TRAIN_STEP_NAME': 'standard_train_step'},
 'VERBOSE': True}
INFO 2022-05-16 04:41:33,671 train.py: 117: System config:
-------------------  ---------------------------------------------------------------------------------------------------------------
sys.platform         linux
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
numpy                1.19.5
Pillow               9.0.1
vissl                0.1.6 @/home/mila/r/rajkuman/mina/mphil-vissl/vissl
GPU available        True
GPU 0                Tesla V100-SXM2-16GB
CUDA_HOME            /usr/local/cuda
torchvision          0.9.1 @/home/mila/r/rajkuman/.conda/envs/vissl_env/lib/python3.8/site-packages/torchvision
hydra                1.0.7 @/home/mila/r/rajkuman/.conda/envs/vissl_env/lib/python3.8/site-packages/hydra_core-1.0.7-py3.8.egg/hydra
classy_vision        0.7.0.dev @/home/mila/r/rajkuman/.conda/envs/vissl_env/lib/python3.8/site-packages/classy_vision
tensorboard          2.9.0
apex                 0.1 @/home/mila/r/rajkuman/.conda/envs/vissl_env/lib/python3.8/site-packages/apex
PyTorch              1.8.1 @/home/mila/r/rajkuman/.conda/envs/vissl_env/lib/python3.8/site-packages/torch
PyTorch debug build  False
-------------------  ---------------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=10.2, CUDNN_VERSION=7.6.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

CPU info:
-------------------  -----------------------------------------
Architecture         x86_64
CPU op-mode(s)       32-bit, 64-bit
Byte Order           Little Endian
CPU(s)               80
On-line CPU(s) list  0-79
Thread(s) per core   2
Core(s) per socket   20
Socket(s)            2
NUMA node(s)         2
Vendor ID            GenuineIntel
CPU family           6
Model                79
Model name           Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz
Stepping             1
CPU MHz              2962.844
CPU max MHz          3600.0000
CPU min MHz          1200.0000
BogoMIPS             4390.19
Virtualization       VT-x
L1d cache            32K
L1i cache            32K
L2 cache             256K
L3 cache             51200K
NUMA node0 CPU(s)    0-19,40-59
NUMA node1 CPU(s)    20-39,60-79
-------------------  -----------------------------------------
INFO 2022-05-16 04:41:33,674 trainer_main.py: 112: Using Distributed init method: tcp://localhost:33967, world_size: 1, rank: 0
INFO 2022-05-16 04:41:33,681 distributed_c10d.py: 187: Added key: store_based_barrier_key:1 to store for rank: 0
INFO 2022-05-16 04:41:33,681 trainer_main.py: 130: | initialized host mila01 as rank 0 (0)
INFO 2022-05-16 04:41:44,840 train_task.py: 181: Not using Automatic Mixed Precision
INFO 2022-05-16 04:41:44,842 train_task.py: 455: Building model....
INFO 2022-05-16 04:41:44,843 feature_extractor.py:  27: Creating Feature extractor trunk...
INFO 2022-05-16 04:41:44,844 resnext.py:  66: ResNeXT trunk, supports activation checkpointing. Deactivated
INFO 2022-05-16 04:41:44,845 resnext.py:  93: Building model: ResNeXt18-1x64d-w1-BatchNorm2d
INFO 2022-05-16 04:41:45,346 feature_extractor.py:  50: Freezing model trunk...
INFO 2022-05-16 04:41:45,356 train_task.py: 472: config.MODEL.FEATURE_EVAL_SETTINGS.FREEZE_TRUNK_ONLY=True, will freeze trunk...
INFO 2022-05-16 04:41:45,357 base_ssl_model.py: 195: Freezing model trunk...
INFO 2022-05-16 04:41:45,362 train_task.py: 429: Initializing model from: ./checkpoints/trained_swav/2/model_final_checkpoint_phase499.torch
INFO 2022-05-16 04:41:45,364 util.py: 276: Attempting to load checkpoint from ./checkpoints/trained_swav/2/model_final_checkpoint_phase499.torch
INFO 2022-05-16 04:41:45,769 util.py: 281: Loaded checkpoint from ./checkpoints/trained_swav/2/model_final_checkpoint_phase499.torch
INFO 2022-05-16 04:41:45,770 util.py: 240: Broadcasting checkpoint loaded from ./checkpoints/trained_swav/2/model_final_checkpoint_phase499.torch
INFO 2022-05-16 04:41:50,940 train_task.py: 435: Checkpoint loaded: ./checkpoints/trained_swav/2/model_final_checkpoint_phase499.torch...
INFO 2022-05-16 04:41:50,943 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.conv1.weight                              of shape: torch.Size([64, 3, 3, 3]) from checkpoint
INFO 2022-05-16 04:41:50,943 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.bn1.weight                                of shape: torch.Size([64]) from checkpoint
INFO 2022-05-16 04:41:50,944 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.bn1.bias                                  of shape: torch.Size([64]) from checkpoint
INFO 2022-05-16 04:41:50,944 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.bn1.running_mean                          of shape: torch.Size([64]) from checkpoint
INFO 2022-05-16 04:41:50,945 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.bn1.running_var                           of shape: torch.Size([64]) from checkpoint
INFO 2022-05-16 04:41:50,955 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.bn1.num_batches_tracked
INFO 2022-05-16 04:41:50,958 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.0.conv1.weight                     of shape: torch.Size([64, 64, 3, 3]) from checkpoint
INFO 2022-05-16 04:41:50,971 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.0.bn1.weight                       of shape: torch.Size([64]) from checkpoint
INFO 2022-05-16 04:41:50,971 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.0.bn1.bias                         of shape: torch.Size([64]) from checkpoint
INFO 2022-05-16 04:41:50,972 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.0.bn1.running_mean                 of shape: torch.Size([64]) from checkpoint
INFO 2022-05-16 04:41:50,972 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.0.bn1.running_var                  of shape: torch.Size([64]) from checkpoint
INFO 2022-05-16 04:41:50,973 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer1.0.bn1.num_batches_tracked
INFO 2022-05-16 04:41:50,979 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.0.conv2.weight                     of shape: torch.Size([64, 64, 3, 3]) from checkpoint
INFO 2022-05-16 04:41:50,980 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.0.bn2.weight                       of shape: torch.Size([64]) from checkpoint
INFO 2022-05-16 04:41:50,980 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.0.bn2.bias                         of shape: torch.Size([64]) from checkpoint
INFO 2022-05-16 04:41:50,981 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.0.bn2.running_mean                 of shape: torch.Size([64]) from checkpoint
INFO 2022-05-16 04:41:50,982 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.0.bn2.running_var                  of shape: torch.Size([64]) from checkpoint
INFO 2022-05-16 04:41:50,982 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer1.0.bn2.num_batches_tracked
INFO 2022-05-16 04:41:50,987 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.1.conv1.weight                     of shape: torch.Size([64, 64, 3, 3]) from checkpoint
INFO 2022-05-16 04:41:50,988 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.1.bn1.weight                       of shape: torch.Size([64]) from checkpoint
INFO 2022-05-16 04:41:50,988 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.1.bn1.bias                         of shape: torch.Size([64]) from checkpoint
INFO 2022-05-16 04:41:50,989 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.1.bn1.running_mean                 of shape: torch.Size([64]) from checkpoint
INFO 2022-05-16 04:41:50,989 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.1.bn1.running_var                  of shape: torch.Size([64]) from checkpoint
INFO 2022-05-16 04:41:50,990 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer1.1.bn1.num_batches_tracked
INFO 2022-05-16 04:41:50,995 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.1.conv2.weight                     of shape: torch.Size([64, 64, 3, 3]) from checkpoint
INFO 2022-05-16 04:41:50,999 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.1.bn2.weight                       of shape: torch.Size([64]) from checkpoint
INFO 2022-05-16 04:41:51,000 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.1.bn2.bias                         of shape: torch.Size([64]) from checkpoint
INFO 2022-05-16 04:41:51,001 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.1.bn2.running_mean                 of shape: torch.Size([64]) from checkpoint
INFO 2022-05-16 04:41:51,002 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.1.bn2.running_var                  of shape: torch.Size([64]) from checkpoint
INFO 2022-05-16 04:41:51,003 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer1.1.bn2.num_batches_tracked
INFO 2022-05-16 04:41:51,006 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.0.conv1.weight                     of shape: torch.Size([128, 64, 3, 3]) from checkpoint
INFO 2022-05-16 04:41:51,010 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.0.bn1.weight                       of shape: torch.Size([128]) from checkpoint
INFO 2022-05-16 04:41:51,010 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.0.bn1.bias                         of shape: torch.Size([128]) from checkpoint
INFO 2022-05-16 04:41:51,011 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.0.bn1.running_mean                 of shape: torch.Size([128]) from checkpoint
INFO 2022-05-16 04:41:51,012 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.0.bn1.running_var                  of shape: torch.Size([128]) from checkpoint
INFO 2022-05-16 04:41:51,013 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer2.0.bn1.num_batches_tracked
INFO 2022-05-16 04:41:51,022 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.0.conv2.weight                     of shape: torch.Size([128, 128, 3, 3]) from checkpoint
INFO 2022-05-16 04:41:51,025 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.0.bn2.weight                       of shape: torch.Size([128]) from checkpoint
INFO 2022-05-16 04:41:51,026 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.0.bn2.bias                         of shape: torch.Size([128]) from checkpoint
INFO 2022-05-16 04:41:51,026 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.0.bn2.running_mean                 of shape: torch.Size([128]) from checkpoint
INFO 2022-05-16 04:41:51,027 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.0.bn2.running_var                  of shape: torch.Size([128]) from checkpoint
INFO 2022-05-16 04:41:51,027 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer2.0.bn2.num_batches_tracked
INFO 2022-05-16 04:41:51,028 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.0.downsample.0.weight              of shape: torch.Size([128, 64, 1, 1]) from checkpoint
INFO 2022-05-16 04:41:51,029 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.0.downsample.1.weight              of shape: torch.Size([128]) from checkpoint
INFO 2022-05-16 04:41:51,029 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.0.downsample.1.bias                of shape: torch.Size([128]) from checkpoint
INFO 2022-05-16 04:41:51,030 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.0.downsample.1.running_mean        of shape: torch.Size([128]) from checkpoint
INFO 2022-05-16 04:41:51,030 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.0.downsample.1.running_var         of shape: torch.Size([128]) from checkpoint
INFO 2022-05-16 04:41:51,031 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer2.0.downsample.1.num_batches_tracked
INFO 2022-05-16 04:41:51,038 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.1.conv1.weight                     of shape: torch.Size([128, 128, 3, 3]) from checkpoint
INFO 2022-05-16 04:41:51,038 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.1.bn1.weight                       of shape: torch.Size([128]) from checkpoint
INFO 2022-05-16 04:41:51,039 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.1.bn1.bias                         of shape: torch.Size([128]) from checkpoint
INFO 2022-05-16 04:41:51,040 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.1.bn1.running_mean                 of shape: torch.Size([128]) from checkpoint
INFO 2022-05-16 04:41:51,040 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.1.bn1.running_var                  of shape: torch.Size([128]) from checkpoint
INFO 2022-05-16 04:41:51,041 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer2.1.bn1.num_batches_tracked
INFO 2022-05-16 04:41:51,050 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.1.conv2.weight                     of shape: torch.Size([128, 128, 3, 3]) from checkpoint
INFO 2022-05-16 04:41:51,050 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.1.bn2.weight                       of shape: torch.Size([128]) from checkpoint
INFO 2022-05-16 04:41:51,051 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.1.bn2.bias                         of shape: torch.Size([128]) from checkpoint
INFO 2022-05-16 04:41:51,052 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.1.bn2.running_mean                 of shape: torch.Size([128]) from checkpoint
INFO 2022-05-16 04:41:51,052 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.1.bn2.running_var                  of shape: torch.Size([128]) from checkpoint
INFO 2022-05-16 04:41:51,056 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer2.1.bn2.num_batches_tracked
INFO 2022-05-16 04:41:51,068 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.0.conv1.weight                     of shape: torch.Size([256, 128, 3, 3]) from checkpoint
INFO 2022-05-16 04:41:51,068 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.0.bn1.weight                       of shape: torch.Size([256]) from checkpoint
INFO 2022-05-16 04:41:51,069 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.0.bn1.bias                         of shape: torch.Size([256]) from checkpoint
INFO 2022-05-16 04:41:51,069 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.0.bn1.running_mean                 of shape: torch.Size([256]) from checkpoint
INFO 2022-05-16 04:41:51,070 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.0.bn1.running_var                  of shape: torch.Size([256]) from checkpoint
INFO 2022-05-16 04:41:51,070 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer3.0.bn1.num_batches_tracked
INFO 2022-05-16 04:41:51,077 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.0.conv2.weight                     of shape: torch.Size([256, 256, 3, 3]) from checkpoint
INFO 2022-05-16 04:41:51,082 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.0.bn2.weight                       of shape: torch.Size([256]) from checkpoint
INFO 2022-05-16 04:41:51,083 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.0.bn2.bias                         of shape: torch.Size([256]) from checkpoint
INFO 2022-05-16 04:41:51,084 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.0.bn2.running_mean                 of shape: torch.Size([256]) from checkpoint
INFO 2022-05-16 04:41:51,084 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.0.bn2.running_var                  of shape: torch.Size([256]) from checkpoint
INFO 2022-05-16 04:41:51,085 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer3.0.bn2.num_batches_tracked
INFO 2022-05-16 04:41:51,086 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.0.downsample.0.weight              of shape: torch.Size([256, 128, 1, 1]) from checkpoint
INFO 2022-05-16 04:41:51,086 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.0.downsample.1.weight              of shape: torch.Size([256]) from checkpoint
INFO 2022-05-16 04:41:51,087 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.0.downsample.1.bias                of shape: torch.Size([256]) from checkpoint
INFO 2022-05-16 04:41:51,087 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.0.downsample.1.running_mean        of shape: torch.Size([256]) from checkpoint
INFO 2022-05-16 04:41:51,088 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.0.downsample.1.running_var         of shape: torch.Size([256]) from checkpoint
INFO 2022-05-16 04:41:51,088 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer3.0.downsample.1.num_batches_tracked
INFO 2022-05-16 04:41:51,096 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.1.conv1.weight                     of shape: torch.Size([256, 256, 3, 3]) from checkpoint
INFO 2022-05-16 04:41:51,097 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.1.bn1.weight                       of shape: torch.Size([256]) from checkpoint
INFO 2022-05-16 04:41:51,098 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.1.bn1.bias                         of shape: torch.Size([256]) from checkpoint
INFO 2022-05-16 04:41:51,099 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.1.bn1.running_mean                 of shape: torch.Size([256]) from checkpoint
INFO 2022-05-16 04:41:51,099 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.1.bn1.running_var                  of shape: torch.Size([256]) from checkpoint
INFO 2022-05-16 04:41:51,100 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer3.1.bn1.num_batches_tracked
INFO 2022-05-16 04:41:51,108 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.1.conv2.weight                     of shape: torch.Size([256, 256, 3, 3]) from checkpoint
INFO 2022-05-16 04:41:51,109 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.1.bn2.weight                       of shape: torch.Size([256]) from checkpoint
INFO 2022-05-16 04:41:51,109 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.1.bn2.bias                         of shape: torch.Size([256]) from checkpoint
INFO 2022-05-16 04:41:51,110 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.1.bn2.running_mean                 of shape: torch.Size([256]) from checkpoint
INFO 2022-05-16 04:41:51,110 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.1.bn2.running_var                  of shape: torch.Size([256]) from checkpoint
INFO 2022-05-16 04:41:51,111 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer3.1.bn2.num_batches_tracked
INFO 2022-05-16 04:41:51,119 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.0.conv1.weight                     of shape: torch.Size([512, 256, 3, 3]) from checkpoint
INFO 2022-05-16 04:41:51,120 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.0.bn1.weight                       of shape: torch.Size([512]) from checkpoint
INFO 2022-05-16 04:41:51,123 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.0.bn1.bias                         of shape: torch.Size([512]) from checkpoint
INFO 2022-05-16 04:41:51,124 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.0.bn1.running_mean                 of shape: torch.Size([512]) from checkpoint
INFO 2022-05-16 04:41:51,125 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.0.bn1.running_var                  of shape: torch.Size([512]) from checkpoint
INFO 2022-05-16 04:41:51,128 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer4.0.bn1.num_batches_tracked
INFO 2022-05-16 04:41:51,134 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.0.conv2.weight                     of shape: torch.Size([512, 512, 3, 3]) from checkpoint
INFO 2022-05-16 04:41:51,135 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.0.bn2.weight                       of shape: torch.Size([512]) from checkpoint
INFO 2022-05-16 04:41:51,135 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.0.bn2.bias                         of shape: torch.Size([512]) from checkpoint
INFO 2022-05-16 04:41:51,136 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.0.bn2.running_mean                 of shape: torch.Size([512]) from checkpoint
INFO 2022-05-16 04:41:51,136 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.0.bn2.running_var                  of shape: torch.Size([512]) from checkpoint
INFO 2022-05-16 04:41:51,136 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer4.0.bn2.num_batches_tracked
INFO 2022-05-16 04:41:51,143 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.0.downsample.0.weight              of shape: torch.Size([512, 256, 1, 1]) from checkpoint
INFO 2022-05-16 04:41:51,145 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.0.downsample.1.weight              of shape: torch.Size([512]) from checkpoint
INFO 2022-05-16 04:41:51,145 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.0.downsample.1.bias                of shape: torch.Size([512]) from checkpoint
INFO 2022-05-16 04:41:51,146 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.0.downsample.1.running_mean        of shape: torch.Size([512]) from checkpoint
INFO 2022-05-16 04:41:51,147 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.0.downsample.1.running_var         of shape: torch.Size([512]) from checkpoint
INFO 2022-05-16 04:41:51,147 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer4.0.downsample.1.num_batches_tracked
INFO 2022-05-16 04:41:51,154 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.1.conv1.weight                     of shape: torch.Size([512, 512, 3, 3]) from checkpoint
INFO 2022-05-16 04:41:51,154 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.1.bn1.weight                       of shape: torch.Size([512]) from checkpoint
INFO 2022-05-16 04:41:51,157 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.1.bn1.bias                         of shape: torch.Size([512]) from checkpoint
INFO 2022-05-16 04:41:51,158 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.1.bn1.running_mean                 of shape: torch.Size([512]) from checkpoint
INFO 2022-05-16 04:41:51,158 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.1.bn1.running_var                  of shape: torch.Size([512]) from checkpoint
INFO 2022-05-16 04:41:51,159 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer4.1.bn1.num_batches_tracked
INFO 2022-05-16 04:41:51,167 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.1.conv2.weight                     of shape: torch.Size([512, 512, 3, 3]) from checkpoint
INFO 2022-05-16 04:41:51,169 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.1.bn2.weight                       of shape: torch.Size([512]) from checkpoint
INFO 2022-05-16 04:41:51,170 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.1.bn2.bias                         of shape: torch.Size([512]) from checkpoint
INFO 2022-05-16 04:41:51,173 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.1.bn2.running_mean                 of shape: torch.Size([512]) from checkpoint
INFO 2022-05-16 04:41:51,174 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.1.bn2.running_var                  of shape: torch.Size([512]) from checkpoint
INFO 2022-05-16 04:41:51,175 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer4.1.bn2.num_batches_tracked
INFO 2022-05-16 04:41:51,175 checkpoint.py: 894: Not found:		heads.0.channel_bn.weight, not initialized
INFO 2022-05-16 04:41:51,181 checkpoint.py: 894: Not found:		heads.0.channel_bn.bias, not initialized
INFO 2022-05-16 04:41:51,182 checkpoint.py: 894: Not found:		heads.0.channel_bn.running_mean, not initialized
INFO 2022-05-16 04:41:51,182 checkpoint.py: 894: Not found:		heads.0.channel_bn.running_var, not initialized
INFO 2022-05-16 04:41:51,183 checkpoint.py: 851: Ignored layer:	heads.0.channel_bn.num_batches_tracked
INFO 2022-05-16 04:41:51,183 checkpoint.py: 894: Not found:		heads.0.clf.clf.0.weight, not initialized
INFO 2022-05-16 04:41:51,184 checkpoint.py: 894: Not found:		heads.0.clf.clf.0.bias, not initialized
INFO 2022-05-16 04:41:51,184 checkpoint.py: 901: Extra layers not loaded from checkpoint: ['heads.0.projection_head.0.weight', 'heads.0.projection_head.0.bias', 'heads.0.projection_head.1.weight', 'heads.0.projection_head.1.bias', 'heads.0.projection_head.1.running_mean', 'heads.0.projection_head.1.running_var', 'heads.0.projection_head.1.num_batches_tracked', 'heads.0.projection_head.3.weight', 'heads.0.projection_head.3.bias', 'heads.0.prototypes0.weight']
INFO 2022-05-16 04:41:51,185 train_task.py: 656: Broadcast model BN buffers from primary on every forward pass
INFO 2022-05-16 04:41:51,186 classification_task.py: 387: Synchronized Batch Normalization is disabled
INFO 2022-05-16 04:41:51,370 optimizer_helper.py: 293: 
Trainable params: 4, 
Non-Trainable params: 0, 
Trunk Regularized Parameters: 0, 
Trunk Unregularized Parameters 0, 
Head Regularized Parameters: 2, 
Head Unregularized Parameters: 2 
Remaining Regularized Parameters: 0 
Remaining Unregularized Parameters: 0
INFO 2022-05-16 04:41:51,375 ssl_dataset.py: 156: Rank: 0 split: TEST Data files:
['data/CIFAR-100-dataset/test']
INFO 2022-05-16 04:41:51,375 ssl_dataset.py: 159: Rank: 0 split: TEST Label files:
['data/CIFAR-100-dataset/test']
INFO 2022-05-16 04:41:51,859 disk_dataset.py:  86: Loaded 10000 samples from folder data/CIFAR-100-dataset/test
INFO 2022-05-16 04:41:51,860 ssl_dataset.py: 156: Rank: 0 split: TRAIN Data files:
['data/CIFAR-100-dataset/train']
INFO 2022-05-16 04:41:51,861 ssl_dataset.py: 159: Rank: 0 split: TRAIN Label files:
['data/CIFAR-100-dataset/train']
INFO 2022-05-16 04:41:52,884 disk_dataset.py:  86: Loaded 50000 samples from folder data/CIFAR-100-dataset/train
INFO 2022-05-16 04:41:52,884 misc.py: 161: Set start method of multiprocessing to forkserver
INFO 2022-05-16 04:41:52,885 __init__.py: 126: Created the Distributed Sampler....
INFO 2022-05-16 04:41:52,885 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 0, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 04:41:52,886 __init__.py: 215: Wrapping the dataloader to async device copies
INFO 2022-05-16 04:41:52,887 misc.py: 161: Set start method of multiprocessing to forkserver
INFO 2022-05-16 04:41:52,888 __init__.py: 126: Created the Distributed Sampler....
INFO 2022-05-16 04:41:52,888 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 0, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 04:41:52,889 __init__.py: 215: Wrapping the dataloader to async device copies
INFO 2022-05-16 04:41:52,889 train_task.py: 384: Building loss...
INFO 2022-05-16 04:41:52,890 trainer_main.py: 268: Training 100 epochs
INFO 2022-05-16 04:41:52,891 trainer_main.py: 269: One epoch = 98 iterations.
INFO 2022-05-16 04:41:52,891 trainer_main.py: 270: Total 50000 samples in one epoch
INFO 2022-05-16 04:41:52,892 trainer_main.py: 276: Total 9800 iterations for training
INFO 2022-05-16 04:41:53,059 logger.py:  84: Mon May 16 04:41:52 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |
| N/A   40C    P0    70W / 300W |  11495MiB / 16160MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      6745      C   python                          10295MiB |
|    0   N/A  N/A     46012      C   python                           1197MiB |
+-----------------------------------------------------------------------------+

INFO 2022-05-16 04:41:53,062 trainer_main.py: 173: Model is:
 Classy <class 'vissl.models.base_ssl_model.BaseSSLMultiInputOutputModel'>:
BaseSSLMultiInputOutputModel(
  (_heads): ModuleDict()
  (trunk): FeatureExtractorModel(
    (base_model): ResNeXt(
      (_feature_blocks): ModuleDict(
        (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv1_relu): ReLU(inplace=True)
        (maxpool): Identity()
        (layer1): Sequential(
          (0): BasicBlock(
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): BasicBlock(
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (layer2): Sequential(
          (0): BasicBlock(
            (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (downsample): Sequential(
              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): BasicBlock(
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (layer3): Sequential(
          (0): BasicBlock(
            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (downsample): Sequential(
              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): BasicBlock(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (layer4): Sequential(
          (0): BasicBlock(
            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(<SUPPORTED_L4_STRIDE.two: 2>, <SUPPORTED_L4_STRIDE.two: 2>), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (downsample): Sequential(
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(<SUPPORTED_L4_STRIDE.two: 2>, <SUPPORTED_L4_STRIDE.two: 2>), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): BasicBlock(
            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
        (flatten): Flatten()
      )
    )
    (feature_pool_ops): ModuleList(
      (0): Identity()
    )
  )
  (heads): ModuleList(
    (0): LinearEvalMLP(
      (channel_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (clf): MLP(
        (clf): Sequential(
          (0): Linear(in_features=512, out_features=100, bias=True)
        )
      )
    )
  )
)
INFO 2022-05-16 04:41:53,063 trainer_main.py: 174: Loss is: CrossEntropyMultipleOutputSingleTargetLoss(
  (criterion): CrossEntropyMultipleOutputSingleTargetCriterion(
    (_losses): ModuleList()
  )
)
INFO 2022-05-16 04:41:53,064 trainer_main.py: 175: Starting training....
INFO 2022-05-16 04:41:53,064 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 0, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 04:42:29,850 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 04:42:29,853 log_hooks.py:  76: ========= Memory Summary at on_phase_start =======
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   50046 KB |   50046 KB |   50046 KB |     512 B  |
|       from large pool |   46464 KB |   46464 KB |   46464 KB |       0 B  |
|       from small pool |    3582 KB |    3582 KB |    3582 KB |     512 B  |
|---------------------------------------------------------------------------|
| Active memory         |   50046 KB |   50046 KB |   50046 KB |     512 B  |
|       from large pool |   46464 KB |   46464 KB |   46464 KB |       0 B  |
|       from small pool |    3582 KB |    3582 KB |    3582 KB |     512 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   88064 KB |   88064 KB |   88064 KB |       0 B  |
|       from large pool |   81920 KB |   81920 KB |   81920 KB |       0 B  |
|       from small pool |    6144 KB |    6144 KB |    6144 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   38018 KB |   38026 KB |   63796 KB |   25778 KB |
|       from large pool |   35456 KB |   35456 KB |   56192 KB |   20736 KB |
|       from small pool |    2562 KB |    2570 KB |    7604 KB |    5042 KB |
|---------------------------------------------------------------------------|
| Allocations           |     131    |     131    |     132    |       1    |
|       from large pool |       9    |       9    |       9    |       0    |
|       from small pool |     122    |     122    |     123    |       1    |
|---------------------------------------------------------------------------|
| Active allocs         |     131    |     131    |     132    |       1    |
|       from large pool |       9    |       9    |       9    |       0    |
|       from small pool |     122    |     122    |     123    |       1    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       7    |       7    |       7    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       3    |       3    |       3    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       7    |       7    |       8    |       1    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       3    |       3    |       4    |       1    |
|===========================================================================|


INFO 2022-05-16 04:42:29,853 state_update_hooks.py: 115: Starting phase 0 [train]
INFO 2022-05-16 04:42:31,151 log_hooks.py:  76: ========= Memory Summary at on_forward =======
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   58454 KB |    2278 MB |   13592 MB |   13535 MB |
|       from large pool |   52608 KB |    2275 MB |   13585 MB |   13533 MB |
|       from small pool |    5846 KB |       5 MB |       7 MB |       1 MB |
|---------------------------------------------------------------------------|
| Active memory         |   58454 KB |    2278 MB |   13592 MB |   13535 MB |
|       from large pool |   52608 KB |    2275 MB |   13585 MB |   13533 MB |
|       from small pool |    5846 KB |       5 MB |       7 MB |       1 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  331776 KB |    3450 MB |    6114 MB |    5790 MB |
|       from large pool |  323584 KB |    3444 MB |    6104 MB |    5788 MB |
|       from small pool |    8192 KB |       8 MB |      10 MB |       2 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   31658 KB |    2165 MB |    9057 MB |    9026 MB |
|       from large pool |   29312 KB |    2162 MB |    9042 MB |    9014 MB |
|       from small pool |    2346 KB |       4 MB |      14 MB |      12 MB |
|---------------------------------------------------------------------------|
| Allocations           |     140    |     140    |     211    |      71    |
|       from large pool |      10    |      15    |      70    |      60    |
|       from small pool |     130    |     130    |     141    |      11    |
|---------------------------------------------------------------------------|
| Active allocs         |     140    |     140    |     211    |      71    |
|       from large pool |      10    |      15    |      70    |      60    |
|       from small pool |     130    |     130    |     141    |      11    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      10    |      11    |      15    |       5    |
|       from large pool |       6    |       7    |      10    |       4    |
|       from small pool |       4    |       4    |       5    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |      11    |      43    |      35    |
|       from large pool |       4    |       8    |      33    |      29    |
|       from small pool |       4    |       5    |      10    |       6    |
|===========================================================================|


INFO 2022-05-16 04:42:31,237 log_hooks.py:  76: ========= Memory Summary at on_backward =======
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   56607 KB |    2278 MB |   13596 MB |   13540 MB |
|       from large pool |   52608 KB |    2275 MB |   13585 MB |   13533 MB |
|       from small pool |    3999 KB |       7 MB |      11 MB |       7 MB |
|---------------------------------------------------------------------------|
| Active memory         |   56607 KB |    2278 MB |   13596 MB |   13540 MB |
|       from large pool |   52608 KB |    2275 MB |   13585 MB |   13533 MB |
|       from small pool |    3999 KB |       7 MB |      11 MB |       7 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  333824 KB |    3450 MB |    6116 MB |    5790 MB |
|       from large pool |  323584 KB |    3444 MB |    6104 MB |    5788 MB |
|       from small pool |   10240 KB |      10 MB |      12 MB |       2 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   33504 KB |    2165 MB |    9067 MB |    9034 MB |
|       from large pool |   29312 KB |    2162 MB |    9042 MB |    9014 MB |
|       from small pool |    4192 KB |       5 MB |      24 MB |      20 MB |
|---------------------------------------------------------------------------|
| Allocations           |     142    |     148    |     238    |      96    |
|       from large pool |      10    |      15    |      70    |      60    |
|       from small pool |     132    |     138    |     168    |      36    |
|---------------------------------------------------------------------------|
| Active allocs         |     142    |     148    |     238    |      96    |
|       from large pool |      10    |      15    |      70    |      60    |
|       from small pool |     132    |     138    |     168    |      36    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      16    |       5    |
|       from large pool |       6    |       7    |      10    |       4    |
|       from small pool |       5    |       5    |       6    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      11    |      11    |      58    |      47    |
|       from large pool |       4    |       8    |      33    |      29    |
|       from small pool |       7    |       7    |      25    |      18    |
|===========================================================================|


INFO 2022-05-16 04:42:31,239 log_hooks.py:  76: ========= Memory Summary at on_update =======
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   56812 KB |    2278 MB |   13596 MB |   13541 MB |
|       from large pool |   52608 KB |    2275 MB |   13585 MB |   13533 MB |
|       from small pool |    4204 KB |       7 MB |      11 MB |       7 MB |
|---------------------------------------------------------------------------|
| Active memory         |   56812 KB |    2278 MB |   13596 MB |   13541 MB |
|       from large pool |   52608 KB |    2275 MB |   13585 MB |   13533 MB |
|       from small pool |    4204 KB |       7 MB |      11 MB |       7 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  333824 KB |    3450 MB |    6116 MB |    5790 MB |
|       from large pool |  323584 KB |    3444 MB |    6104 MB |    5788 MB |
|       from small pool |   10240 KB |      10 MB |      12 MB |       2 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   33300 KB |    2165 MB |    9067 MB |    9034 MB |
|       from large pool |   29312 KB |    2162 MB |    9042 MB |    9014 MB |
|       from small pool |    3988 KB |       5 MB |      24 MB |      20 MB |
|---------------------------------------------------------------------------|
| Allocations           |     146    |     148    |     244    |      98    |
|       from large pool |      10    |      15    |      70    |      60    |
|       from small pool |     136    |     138    |     174    |      38    |
|---------------------------------------------------------------------------|
| Active allocs         |     146    |     148    |     244    |      98    |
|       from large pool |      10    |      15    |      70    |      60    |
|       from small pool |     136    |     138    |     174    |      38    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      16    |       5    |
|       from large pool |       6    |       7    |      10    |       4    |
|       from small pool |       5    |       5    |       6    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      12    |      12    |      60    |      48    |
|       from large pool |       4    |       8    |      33    |      29    |
|       from small pool |       8    |       8    |      27    |      19    |
|===========================================================================|


INFO 2022-05-16 04:42:31,240 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 0; lr: 0.03; loss: 4.76401; btime(ms): 0; eta: 0:00:00; peak_mem(M): 2278;
INFO 2022-05-16 04:42:31,287 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 1; lr: 0.03; loss: 4.77514; btime(ms): 38351; eta: 4 days, 8:23:30; peak_mem(M): 2278; max_iterations: 9800;
INFO 2022-05-16 04:42:31,549 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 5; lr: 0.03; loss: 4.30198; btime(ms): 7723; eta: 21:00:56; peak_mem(M): 2278;
INFO 2022-05-16 04:42:33,040 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 10; lr: 0.03; loss: 4.12914; btime(ms): 4010; eta: 10:54:23; peak_mem(M): 2278;
INFO 2022-05-16 04:42:35,920 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 15; lr: 0.03; loss: 3.93132; btime(ms): 2776; eta: 7:32:44; peak_mem(M): 2278;
INFO 2022-05-16 04:42:37,436 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 20; lr: 0.03; loss: 3.91707; btime(ms): 2225; eta: 6:02:43; peak_mem(M): 2278;
INFO 2022-05-16 04:42:39,321 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 25; lr: 0.03; loss: 3.83604; btime(ms): 1854; eta: 5:02:09; peak_mem(M): 2278;
INFO 2022-05-16 04:42:41,599 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 30; lr: 0.03; loss: 3.75531; btime(ms): 1621; eta: 4:24:06; peak_mem(M): 2278;
INFO 2022-05-16 04:42:45,934 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 35; lr: 0.03; loss: 3.64604; btime(ms): 1456; eta: 3:57:04; peak_mem(M): 2278;
INFO 2022-05-16 04:42:48,382 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 40; lr: 0.03; loss: 3.62279; btime(ms): 1382; eta: 3:44:52; peak_mem(M): 2278;
INFO 2022-05-16 04:42:50,718 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 45; lr: 0.03; loss: 3.62066; btime(ms): 1283; eta: 3:28:37; peak_mem(M): 2278;
INFO 2022-05-16 04:42:53,317 logger.py:  84: Mon May 16 04:42:53 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |
| N/A   38C    P0    70W / 300W |  12525MiB / 16160MiB |     38%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      6745      C   python                          10295MiB |
|    0   N/A  N/A     46012      C   python                           2227MiB |
+-----------------------------------------------------------------------------+

INFO 2022-05-16 04:42:53,374 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 50; lr: 0.03; loss: 3.57379; btime(ms): 1204; eta: 3:15:41; peak_mem(M): 2278;
INFO 2022-05-16 04:42:57,783 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 55; lr: 0.03; loss: 3.51568; btime(ms): 1143; eta: 3:05:39; peak_mem(M): 2278;
INFO 2022-05-16 04:43:00,684 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 60; lr: 0.03; loss: 3.50907; btime(ms): 1124; eta: 3:02:30; peak_mem(M): 2278;
INFO 2022-05-16 04:43:03,076 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 65; lr: 0.03; loss: 3.62082; btime(ms): 1078; eta: 2:55:02; peak_mem(M): 2278;
INFO 2022-05-16 04:43:05,402 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 70; lr: 0.03; loss: 3.57647; btime(ms): 1034; eta: 2:47:50; peak_mem(M): 2278;
INFO 2022-05-16 04:43:09,413 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 75; lr: 0.03; loss: 3.54275; btime(ms): 996; eta: 2:41:27; peak_mem(M): 2278;
INFO 2022-05-16 04:43:12,240 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 80; lr: 0.03; loss: 3.51807; btime(ms): 983; eta: 2:39:19; peak_mem(M): 2278;
INFO 2022-05-16 04:43:14,625 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 85; lr: 0.03; loss: 3.4186; btime(ms): 960; eta: 2:35:34; peak_mem(M): 2278;
INFO 2022-05-16 04:43:17,260 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 90; lr: 0.03; loss: 3.41387; btime(ms): 936; eta: 2:31:30; peak_mem(M): 2278;
INFO 2022-05-16 04:43:20,864 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 95; lr: 0.03; loss: 3.43482; btime(ms): 911; eta: 2:27:26; peak_mem(M): 2278;
INFO 2022-05-16 04:43:21,604 trainer_main.py: 214: Meters synced
INFO 2022-05-16 04:43:21,617 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 528
INFO 2022-05-16 04:43:21,618 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  447.54 ms  447.61 ms
             forward:   24.66 ms   54.45 ms
        loss_compute:    1.01 ms    1.21 ms
     loss_all_reduce:    0.12 ms    0.11 ms
       meters_update:   17.03 ms   17.08 ms
            backward:    1.62 ms    1.75 ms
      optimizer_step:    0.72 ms    0.73 ms
    train_step_total:  525.63 ms  525.65 ms
INFO 2022-05-16 04:43:21,619 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 14.616000000000001}, 'top_5': {'flatten': 36.282}}
INFO 2022-05-16 04:43:21,619 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 04:43:21,626 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 04:43:21,627 log_hooks.py: 425: [phase: 0] Saving checkpoint to ./checkpoints/LP_sgd/2/
INFO 2022-05-16 04:43:21,850 checkpoint.py: 131: Saved checkpoint: ./checkpoints/LP_sgd/2//model_phase0.torch
INFO 2022-05-16 04:43:21,850 checkpoint.py: 140: Creating symlink...
INFO 2022-05-16 04:43:21,853 checkpoint.py: 144: Created symlink: ./checkpoints/LP_sgd/2//checkpoint.torch
INFO 2022-05-16 04:43:21,854 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 1, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 04:44:08,959 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 04:44:08,961 state_update_hooks.py: 115: Starting phase 1 [test]
INFO 2022-05-16 04:44:15,842 trainer_main.py: 214: Meters synced
INFO 2022-05-16 04:44:15,847 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 344
INFO 2022-05-16 04:44:15,848 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 20.06}, 'top_5': {'flatten': 45.24}}
INFO 2022-05-16 04:44:15,849 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 04:44:15,852 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 04:44:15,853 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 2, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 04:44:54,659 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 04:44:54,661 state_update_hooks.py: 115: Starting phase 2 [train]
INFO 2022-05-16 04:44:55,000 log_hooks.py: 277: Rank: 0; [ep: 1] iter: 100; lr: 0.02999; loss: 3.32754; btime(ms): 1516; eta: 4:05:11; peak_mem(M): 2605;
INFO 2022-05-16 04:45:51,429 trainer_main.py: 214: Meters synced
INFO 2022-05-16 04:45:51,434 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 579
INFO 2022-05-16 04:45:51,435 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  514.32 ms  514.32 ms
             forward:    5.96 ms   37.81 ms
        loss_compute:    1.02 ms    1.21 ms
     loss_all_reduce:    0.13 ms    0.13 ms
       meters_update:   20.68 ms   20.71 ms
            backward:    1.53 ms    1.73 ms
      optimizer_step:    0.78 ms    0.80 ms
    train_step_total:  579.05 ms  579.04 ms
INFO 2022-05-16 04:45:51,436 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 21.764}, 'top_5': {'flatten': 47.524}}
INFO 2022-05-16 04:45:51,437 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 04:45:51,440 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 04:45:51,442 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 3, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 04:46:33,622 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 04:46:33,623 state_update_hooks.py: 115: Starting phase 3 [test]
INFO 2022-05-16 04:46:40,195 trainer_main.py: 214: Meters synced
INFO 2022-05-16 04:46:40,200 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 328
INFO 2022-05-16 04:46:40,201 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 22.82}, 'top_5': {'flatten': 49.91}}
INFO 2022-05-16 04:46:40,201 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 04:46:40,204 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 04:46:40,205 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 4, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 04:47:24,557 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 04:47:24,558 state_update_hooks.py: 115: Starting phase 4 [train]
INFO 2022-05-16 04:47:26,897 log_hooks.py: 277: Rank: 0; [ep: 2] iter: 200; lr: 0.02997; loss: 3.24998; btime(ms): 1391; eta: 3:42:34; peak_mem(M): 2605;
INFO 2022-05-16 04:48:21,938 trainer_main.py: 214: Meters synced
INFO 2022-05-16 04:48:21,947 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 585
INFO 2022-05-16 04:48:21,947 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  519.04 ms  519.03 ms
             forward:    5.98 ms   37.48 ms
        loss_compute:    0.97 ms    1.23 ms
     loss_all_reduce:    0.13 ms    0.13 ms
       meters_update:   22.23 ms   22.26 ms
            backward:    1.45 ms    1.70 ms
      optimizer_step:    0.93 ms    0.95 ms
    train_step_total:  585.31 ms  585.30 ms
INFO 2022-05-16 04:48:21,948 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 24.232}, 'top_5': {'flatten': 50.852}}
INFO 2022-05-16 04:48:21,949 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 04:48:21,953 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 04:48:21,955 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 5, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 04:48:55,391 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 04:48:55,392 state_update_hooks.py: 115: Starting phase 5 [test]
INFO 2022-05-16 04:49:01,052 trainer_main.py: 214: Meters synced
INFO 2022-05-16 04:49:01,061 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 283
INFO 2022-05-16 04:49:01,062 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 25.25}, 'top_5': {'flatten': 51.66}}
INFO 2022-05-16 04:49:01,063 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 04:49:01,067 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 04:49:01,068 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 6, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 04:49:48,533 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 04:49:48,534 state_update_hooks.py: 115: Starting phase 6 [train]
INFO 2022-05-16 04:50:41,610 trainer_main.py: 214: Meters synced
INFO 2022-05-16 04:50:41,615 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 541
INFO 2022-05-16 04:50:41,616 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  479.64 ms  479.65 ms
             forward:    6.28 ms   37.02 ms
        loss_compute:    1.03 ms    1.24 ms
     loss_all_reduce:    0.13 ms    0.11 ms
       meters_update:   18.26 ms   18.29 ms
            backward:    1.79 ms    1.97 ms
      optimizer_step:    0.83 ms    0.85 ms
    train_step_total:  541.38 ms  541.39 ms
INFO 2022-05-16 04:50:41,616 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 25.97}, 'top_5': {'flatten': 53.196}}
INFO 2022-05-16 04:50:41,617 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 04:50:41,621 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 04:50:41,621 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 7, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 04:51:18,540 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 04:51:18,541 state_update_hooks.py: 115: Starting phase 7 [test]
INFO 2022-05-16 04:51:28,995 trainer_main.py: 214: Meters synced
INFO 2022-05-16 04:51:29,001 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 523
INFO 2022-05-16 04:51:29,005 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 25.430000000000003}, 'top_5': {'flatten': 52.99}}
INFO 2022-05-16 04:51:29,006 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 04:51:29,011 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 04:51:29,011 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 8, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 04:52:18,036 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 04:52:18,037 state_update_hooks.py: 115: Starting phase 8 [train]
INFO 2022-05-16 04:52:23,015 log_hooks.py: 277: Rank: 0; [ep: 4] iter: 400; lr: 0.02988; loss: 2.90216; btime(ms): 1312; eta: 3:25:36; peak_mem(M): 2605;
INFO 2022-05-16 04:53:03,502 trainer_main.py: 214: Meters synced
INFO 2022-05-16 04:53:03,507 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 463
INFO 2022-05-16 04:53:03,508 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  405.54 ms  405.60 ms
             forward:    7.17 ms   36.41 ms
        loss_compute:    1.12 ms    1.30 ms
     loss_all_reduce:    0.12 ms    0.12 ms
       meters_update:   15.33 ms   15.34 ms
            backward:    1.77 ms    1.89 ms
      optimizer_step:    0.68 ms    0.76 ms
    train_step_total:  463.71 ms  463.72 ms
INFO 2022-05-16 04:53:03,509 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 27.336}, 'top_5': {'flatten': 54.81}}
INFO 2022-05-16 04:53:03,509 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 04:53:03,513 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 04:53:03,514 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 9, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 04:53:50,180 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 04:53:50,181 state_update_hooks.py: 115: Starting phase 9 [test]
INFO 2022-05-16 04:54:00,892 trainer_main.py: 214: Meters synced
INFO 2022-05-16 04:54:00,900 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 535
INFO 2022-05-16 04:54:00,901 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 26.400000000000002}, 'top_5': {'flatten': 53.74}}
INFO 2022-05-16 04:54:00,902 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 04:54:00,906 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 04:54:00,907 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 10, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 04:54:50,054 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 04:54:50,055 state_update_hooks.py: 115: Starting phase 10 [train]
INFO 2022-05-16 04:55:29,295 trainer_main.py: 214: Meters synced
INFO 2022-05-16 04:55:29,300 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 400
INFO 2022-05-16 04:55:29,301 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  351.27 ms  351.34 ms
             forward:    7.81 ms   32.82 ms
        loss_compute:    1.00 ms    0.99 ms
     loss_all_reduce:    0.15 ms    0.16 ms
       meters_update:    9.76 ms    9.78 ms
            backward:    2.06 ms    2.07 ms
      optimizer_step:    0.71 ms    0.71 ms
    train_step_total:  400.17 ms  400.17 ms
INFO 2022-05-16 04:55:29,302 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 28.526}, 'top_5': {'flatten': 55.948}}
INFO 2022-05-16 04:55:29,302 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 04:55:29,307 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 04:55:29,307 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 11, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 04:56:18,797 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 04:56:18,798 state_update_hooks.py: 115: Starting phase 11 [test]
INFO 2022-05-16 04:56:29,155 trainer_main.py: 214: Meters synced
INFO 2022-05-16 04:56:29,160 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 518
INFO 2022-05-16 04:56:29,161 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 27.73}, 'top_5': {'flatten': 55.06999999999999}}
INFO 2022-05-16 04:56:29,162 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 04:56:29,165 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 04:56:29,166 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 12, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 04:57:12,417 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 04:57:12,419 state_update_hooks.py: 115: Starting phase 12 [train]
INFO 2022-05-16 04:57:17,283 log_hooks.py: 277: Rank: 0; [ep: 6] iter: 600; lr: 0.02973; loss: 2.96648; btime(ms): 1283; eta: 3:16:51; peak_mem(M): 2605;
INFO 2022-05-16 04:57:56,351 trainer_main.py: 214: Meters synced
INFO 2022-05-16 04:57:56,357 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 448
INFO 2022-05-16 04:57:56,358 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  396.44 ms  396.51 ms
             forward:    7.56 ms   33.72 ms
        loss_compute:    0.88 ms    0.92 ms
     loss_all_reduce:    0.15 ms    0.15 ms
       meters_update:   11.78 ms   11.80 ms
            backward:    1.80 ms    1.84 ms
      optimizer_step:    0.76 ms    0.74 ms
    train_step_total:  448.05 ms  448.06 ms
INFO 2022-05-16 04:57:56,359 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 29.194}, 'top_5': {'flatten': 56.994}}
INFO 2022-05-16 04:57:56,360 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 04:57:56,363 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 04:57:56,364 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 13, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 04:58:45,826 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 04:58:45,827 state_update_hooks.py: 115: Starting phase 13 [test]
INFO 2022-05-16 04:58:56,481 trainer_main.py: 214: Meters synced
INFO 2022-05-16 04:58:56,487 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 533
INFO 2022-05-16 04:58:56,488 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 27.77}, 'top_5': {'flatten': 55.66}}
INFO 2022-05-16 04:58:56,488 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 04:58:56,491 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 04:58:56,492 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 14, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 04:59:36,117 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 04:59:36,118 state_update_hooks.py: 115: Starting phase 14 [train]
INFO 2022-05-16 05:00:28,787 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:00:28,797 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 537
INFO 2022-05-16 05:00:28,798 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  475.10 ms  475.18 ms
             forward:    7.43 ms   36.45 ms
        loss_compute:    1.01 ms    1.14 ms
     loss_all_reduce:    0.15 ms    0.16 ms
       meters_update:   19.06 ms   19.09 ms
            backward:    1.65 ms    1.80 ms
      optimizer_step:    0.96 ms    1.02 ms
    train_step_total:  537.20 ms  537.22 ms
INFO 2022-05-16 05:00:28,799 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 29.644}, 'top_5': {'flatten': 57.964000000000006}}
INFO 2022-05-16 05:00:28,800 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:00:28,809 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:00:28,810 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 15, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:01:17,950 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:01:17,950 state_update_hooks.py: 115: Starting phase 15 [test]
INFO 2022-05-16 05:01:26,186 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:01:26,187 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 411
INFO 2022-05-16 05:01:26,198 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 28.46}, 'top_5': {'flatten': 56.230000000000004}}
INFO 2022-05-16 05:01:26,198 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:01:26,201 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:01:26,201 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 16, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:02:06,181 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:02:06,182 state_update_hooks.py: 115: Starting phase 16 [train]
INFO 2022-05-16 05:02:16,188 log_hooks.py: 277: Rank: 0; [ep: 8] iter: 800; lr: 0.02953; loss: 2.88279; btime(ms): 1274; eta: 3:11:07; peak_mem(M): 2605;
INFO 2022-05-16 05:03:06,880 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:03:06,889 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 619
INFO 2022-05-16 05:03:06,891 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  554.21 ms  554.30 ms
             forward:    6.74 ms   36.46 ms
        loss_compute:    0.96 ms    1.20 ms
     loss_all_reduce:    0.15 ms    0.13 ms
       meters_update:   21.59 ms   21.60 ms
            backward:    1.73 ms    1.93 ms
      optimizer_step:    0.98 ms    0.91 ms
    train_step_total:  619.14 ms  619.18 ms
INFO 2022-05-16 05:03:06,892 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 30.387999999999998}, 'top_5': {'flatten': 58.764}}
INFO 2022-05-16 05:03:06,893 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:03:06,897 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:03:06,898 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 17, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:03:53,163 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:03:53,165 state_update_hooks.py: 115: Starting phase 17 [test]
INFO 2022-05-16 05:03:59,948 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:03:59,953 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 339
INFO 2022-05-16 05:03:59,954 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 28.799999999999997}, 'top_5': {'flatten': 56.489999999999995}}
INFO 2022-05-16 05:03:59,955 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:03:59,958 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:03:59,959 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 18, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:04:44,728 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:04:44,729 state_update_hooks.py: 115: Starting phase 18 [train]
INFO 2022-05-16 05:05:50,350 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:05:50,356 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 669
INFO 2022-05-16 05:05:50,357 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  603.13 ms  603.25 ms
             forward:    6.25 ms   38.98 ms
        loss_compute:    1.14 ms    1.38 ms
     loss_all_reduce:    0.15 ms    0.16 ms
       meters_update:   20.19 ms   20.26 ms
            backward:    1.68 ms    1.91 ms
      optimizer_step:    0.90 ms    0.87 ms
    train_step_total:  669.38 ms  669.38 ms
INFO 2022-05-16 05:05:50,358 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 31.038}, 'top_5': {'flatten': 59.37}}
INFO 2022-05-16 05:05:50,358 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:05:50,365 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:05:50,366 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 19, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:06:41,608 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:06:41,611 state_update_hooks.py: 115: Starting phase 19 [test]
INFO 2022-05-16 05:06:49,764 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:06:49,768 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 407
INFO 2022-05-16 05:06:49,769 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 29.24}, 'top_5': {'flatten': 57.32000000000001}}
INFO 2022-05-16 05:06:49,770 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:06:49,775 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:06:49,776 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 20, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:07:48,846 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:07:48,847 state_update_hooks.py: 115: Starting phase 20 [train]
INFO 2022-05-16 05:08:02,637 log_hooks.py: 277: Rank: 0; [ep: 10] iter: 1000; lr: 0.02927; loss: 2.71865; btime(ms): 1308; eta: 3:11:51; peak_mem(M): 2605;
INFO 2022-05-16 05:08:45,057 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:08:45,061 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 573
INFO 2022-05-16 05:08:45,062 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  516.60 ms  516.64 ms
             forward:    6.27 ms   35.24 ms
        loss_compute:    0.92 ms    1.07 ms
     loss_all_reduce:    0.18 ms    0.21 ms
       meters_update:   15.27 ms   15.32 ms
            backward:    1.47 ms    1.60 ms
      optimizer_step:    0.81 ms    0.84 ms
    train_step_total:  573.34 ms  573.35 ms
INFO 2022-05-16 05:08:45,063 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 31.654}, 'top_5': {'flatten': 60.006}}
INFO 2022-05-16 05:08:45,063 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:08:45,067 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:08:45,068 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 21, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:09:33,916 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:09:33,917 state_update_hooks.py: 115: Starting phase 21 [test]
INFO 2022-05-16 05:09:44,883 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:09:44,891 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 548
INFO 2022-05-16 05:09:44,892 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 29.080000000000002}, 'top_5': {'flatten': 56.97}}
INFO 2022-05-16 05:09:44,893 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:09:44,899 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:09:44,900 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 22, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:10:41,807 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:10:41,808 state_update_hooks.py: 115: Starting phase 22 [train]
INFO 2022-05-16 05:11:30,189 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:11:30,194 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 493
INFO 2022-05-16 05:11:30,194 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  445.38 ms  445.46 ms
             forward:    7.30 ms   33.20 ms
        loss_compute:    0.93 ms    0.95 ms
     loss_all_reduce:    0.12 ms    0.12 ms
       meters_update:    9.26 ms    9.28 ms
            backward:    1.78 ms    1.79 ms
      optimizer_step:    0.62 ms    0.63 ms
    train_step_total:  493.45 ms  493.46 ms
INFO 2022-05-16 05:11:30,195 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 31.807999999999996}, 'top_5': {'flatten': 60.512}}
INFO 2022-05-16 05:11:30,196 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:11:30,201 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:11:30,201 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 23, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:12:24,880 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:12:24,882 state_update_hooks.py: 115: Starting phase 23 [test]
INFO 2022-05-16 05:12:35,880 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:12:35,886 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 550
INFO 2022-05-16 05:12:35,887 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 29.45}, 'top_5': {'flatten': 57.16}}
INFO 2022-05-16 05:12:35,887 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:12:35,892 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:12:35,893 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 24, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:13:26,901 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:13:26,903 state_update_hooks.py: 115: Starting phase 24 [train]
INFO 2022-05-16 05:13:38,637 log_hooks.py: 277: Rank: 0; [ep: 12] iter: 1200; lr: 0.02895; loss: 2.59774; btime(ms): 1323; eta: 3:09:41; peak_mem(M): 2605;
INFO 2022-05-16 05:14:17,753 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:14:17,759 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 518
INFO 2022-05-16 05:14:17,760 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  466.39 ms  466.42 ms
             forward:    7.51 ms   33.63 ms
        loss_compute:    0.94 ms    0.98 ms
     loss_all_reduce:    0.13 ms    0.14 ms
       meters_update:   12.52 ms   12.54 ms
            backward:    1.96 ms    2.01 ms
      optimizer_step:    0.71 ms    0.71 ms
    train_step_total:  518.64 ms  518.67 ms
INFO 2022-05-16 05:14:17,762 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 32.366}, 'top_5': {'flatten': 61.065999999999995}}
INFO 2022-05-16 05:14:17,762 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:14:17,771 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:14:17,776 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 25, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:15:17,548 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:15:17,549 state_update_hooks.py: 115: Starting phase 25 [test]
INFO 2022-05-16 05:15:29,040 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:15:29,048 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 574
INFO 2022-05-16 05:15:29,050 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 29.880000000000003}, 'top_5': {'flatten': 57.85}}
INFO 2022-05-16 05:15:29,050 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:15:29,054 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:15:29,055 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 26, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:16:15,624 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:16:15,625 state_update_hooks.py: 115: Starting phase 26 [train]
INFO 2022-05-16 05:17:15,748 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:17:15,755 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 613
INFO 2022-05-16 05:17:15,756 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  554.15 ms  554.27 ms
             forward:    6.33 ms   36.22 ms
        loss_compute:    0.96 ms    1.13 ms
     loss_all_reduce:    0.17 ms    0.16 ms
       meters_update:   16.86 ms   16.88 ms
            backward:    1.57 ms    1.67 ms
      optimizer_step:    0.76 ms    0.77 ms
    train_step_total:  613.27 ms  613.29 ms
INFO 2022-05-16 05:17:15,757 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 32.744}, 'top_5': {'flatten': 61.476}}
INFO 2022-05-16 05:17:15,758 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:17:15,762 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:17:15,765 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 27, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:18:09,050 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:18:09,051 state_update_hooks.py: 115: Starting phase 27 [test]
INFO 2022-05-16 05:18:16,560 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:18:16,564 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 375
INFO 2022-05-16 05:18:16,565 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 29.849999999999998}, 'top_5': {'flatten': 57.97}}
INFO 2022-05-16 05:18:16,566 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:18:16,571 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:18:16,572 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 28, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:19:03,570 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:19:03,571 state_update_hooks.py: 115: Starting phase 28 [train]
INFO 2022-05-16 05:19:22,223 log_hooks.py: 277: Rank: 0; [ep: 14] iter: 1400; lr: 0.02857; loss: 2.81717; btime(ms): 1338; eta: 3:07:26; peak_mem(M): 2605;
INFO 2022-05-16 05:20:06,898 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:20:06,905 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 646
INFO 2022-05-16 05:20:06,906 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  583.60 ms  583.62 ms
             forward:    6.22 ms   35.85 ms
        loss_compute:    1.11 ms    1.29 ms
     loss_all_reduce:    0.12 ms    0.12 ms
       meters_update:   20.21 ms   20.23 ms
            backward:    1.64 ms    1.74 ms
      optimizer_step:    0.80 ms    0.84 ms
    train_step_total:  646.00 ms  645.98 ms
INFO 2022-05-16 05:20:06,907 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 33.26}, 'top_5': {'flatten': 61.756}}
INFO 2022-05-16 05:20:06,908 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:20:06,912 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:20:06,913 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 29, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:20:52,821 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:20:52,823 state_update_hooks.py: 115: Starting phase 29 [test]
INFO 2022-05-16 05:21:00,031 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:21:00,036 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 360
INFO 2022-05-16 05:21:00,037 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 30.72}, 'top_5': {'flatten': 57.9}}
INFO 2022-05-16 05:21:00,037 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:21:00,042 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:21:00,043 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 30, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:21:50,412 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:21:50,412 state_update_hooks.py: 115: Starting phase 30 [train]
INFO 2022-05-16 05:22:52,417 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:22:52,422 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 632
INFO 2022-05-16 05:22:52,422 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  566.93 ms  567.01 ms
             forward:    6.08 ms   37.24 ms
        loss_compute:    1.10 ms    1.32 ms
     loss_all_reduce:    0.12 ms    0.12 ms
       meters_update:   21.76 ms   21.79 ms
            backward:    1.28 ms    1.44 ms
      optimizer_step:    0.92 ms    0.96 ms
    train_step_total:  632.48 ms  632.45 ms
INFO 2022-05-16 05:22:52,423 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 33.29}, 'top_5': {'flatten': 62.288}}
INFO 2022-05-16 05:22:52,424 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:22:52,427 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:22:52,428 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 31, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:23:36,290 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:23:36,291 state_update_hooks.py: 115: Starting phase 31 [test]
INFO 2022-05-16 05:23:46,871 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:23:46,884 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 529
INFO 2022-05-16 05:23:46,885 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 30.080000000000002}, 'top_5': {'flatten': 58.48}}
INFO 2022-05-16 05:23:46,885 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:23:46,903 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:23:46,904 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 32, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:24:42,465 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:24:42,466 state_update_hooks.py: 115: Starting phase 32 [train]
INFO 2022-05-16 05:25:03,843 log_hooks.py: 277: Rank: 0; [ep: 16] iter: 1600; lr: 0.02814; loss: 2.69737; btime(ms): 1349; eta: 3:04:25; peak_mem(M): 2605;
INFO 2022-05-16 05:25:33,723 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:25:33,728 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 523
INFO 2022-05-16 05:25:33,729 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  466.77 ms  466.87 ms
             forward:    6.69 ms   35.01 ms
        loss_compute:    1.14 ms    1.21 ms
     loss_all_reduce:    0.17 ms    0.18 ms
       meters_update:   13.96 ms   13.99 ms
            backward:    1.89 ms    1.95 ms
      optimizer_step:    0.76 ms    0.79 ms
    train_step_total:  522.76 ms  522.81 ms
INFO 2022-05-16 05:25:33,729 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 33.662}, 'top_5': {'flatten': 62.516000000000005}}
INFO 2022-05-16 05:25:33,730 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:25:33,733 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:25:33,734 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 33, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:26:25,692 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:26:25,693 state_update_hooks.py: 115: Starting phase 33 [test]
INFO 2022-05-16 05:26:37,074 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:26:37,080 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 569
INFO 2022-05-16 05:26:37,081 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 30.620000000000005}, 'top_5': {'flatten': 58.46}}
INFO 2022-05-16 05:26:37,082 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:26:37,086 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:26:37,087 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 34, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:27:29,208 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:27:29,209 state_update_hooks.py: 115: Starting phase 34 [train]
INFO 2022-05-16 05:28:10,212 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:28:10,217 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 418
INFO 2022-05-16 05:28:10,218 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  368.92 ms  368.98 ms
             forward:    8.30 ms   32.84 ms
        loss_compute:    1.02 ms    1.03 ms
     loss_all_reduce:    0.13 ms    0.14 ms
       meters_update:   10.01 ms   10.03 ms
            backward:    2.01 ms    2.02 ms
      optimizer_step:    0.66 ms    0.67 ms
    train_step_total:  418.15 ms  418.16 ms
INFO 2022-05-16 05:28:10,219 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 34.08}, 'top_5': {'flatten': 62.922}}
INFO 2022-05-16 05:28:10,219 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:28:10,223 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:28:10,224 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 35, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:29:00,103 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:29:00,104 state_update_hooks.py: 115: Starting phase 35 [test]
INFO 2022-05-16 05:29:10,851 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:29:10,858 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 537
INFO 2022-05-16 05:29:10,859 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 30.380000000000003}, 'top_5': {'flatten': 59.06}}
INFO 2022-05-16 05:29:10,860 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:29:10,865 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:29:10,866 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 36, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:29:56,832 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:29:56,833 state_update_hooks.py: 115: Starting phase 36 [train]
INFO 2022-05-16 05:30:12,045 log_hooks.py: 277: Rank: 0; [ep: 18] iter: 1800; lr: 0.02766; loss: 2.56225; btime(ms): 1342; eta: 2:58:57; peak_mem(M): 2605;
INFO 2022-05-16 05:30:42,199 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:30:42,205 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 462
INFO 2022-05-16 05:30:42,206 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  410.35 ms  410.43 ms
             forward:    7.28 ms   32.96 ms
        loss_compute:    1.27 ms    1.26 ms
     loss_all_reduce:    0.16 ms    0.17 ms
       meters_update:   12.90 ms   12.94 ms
            backward:    1.84 ms    1.86 ms
      optimizer_step:    0.72 ms    0.73 ms
    train_step_total:  462.67 ms  462.68 ms
INFO 2022-05-16 05:30:42,207 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 34.168}, 'top_5': {'flatten': 63.198}}
INFO 2022-05-16 05:30:42,207 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:30:42,216 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:30:42,217 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 37, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:31:34,857 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:31:34,858 state_update_hooks.py: 115: Starting phase 37 [test]
INFO 2022-05-16 05:31:45,782 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:31:45,788 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 546
INFO 2022-05-16 05:31:45,789 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 30.570000000000004}, 'top_5': {'flatten': 58.89}}
INFO 2022-05-16 05:31:45,789 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:31:45,795 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:31:45,795 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 38, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:32:27,036 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:32:27,036 state_update_hooks.py: 115: Starting phase 38 [train]
INFO 2022-05-16 05:33:04,646 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:33:04,651 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 383
INFO 2022-05-16 05:33:04,651 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  341.74 ms  341.77 ms
             forward:    5.49 ms   31.56 ms
        loss_compute:    0.58 ms    0.58 ms
     loss_all_reduce:    0.11 ms    0.11 ms
       meters_update:    6.06 ms    6.08 ms
            backward:    1.24 ms    1.23 ms
      optimizer_step:    0.58 ms    0.59 ms
    train_step_total:  383.59 ms  383.59 ms
INFO 2022-05-16 05:33:04,652 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 34.58}, 'top_5': {'flatten': 63.656}}
INFO 2022-05-16 05:33:04,652 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:33:04,656 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:33:04,656 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 39, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:33:41,654 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:33:41,656 state_update_hooks.py: 115: Starting phase 39 [test]
INFO 2022-05-16 05:33:47,882 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:33:47,888 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 311
INFO 2022-05-16 05:33:47,888 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 30.520000000000003}, 'top_5': {'flatten': 58.74}}
INFO 2022-05-16 05:33:47,889 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:33:47,893 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:33:47,894 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 40, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:34:24,367 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:34:24,369 state_update_hooks.py: 115: Starting phase 40 [train]
INFO 2022-05-16 05:34:39,563 log_hooks.py: 277: Rank: 0; [ep: 20] iter: 2000; lr: 0.02714; loss: 2.57742; btime(ms): 1319; eta: 2:51:31; peak_mem(M): 2605;
INFO 2022-05-16 05:35:01,510 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:35:01,514 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 379
INFO 2022-05-16 05:35:01,515 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  336.41 ms  336.44 ms
             forward:    5.56 ms   31.56 ms
        loss_compute:    0.58 ms    0.58 ms
     loss_all_reduce:    0.11 ms    0.11 ms
       meters_update:    6.60 ms    6.62 ms
            backward:    1.22 ms    1.23 ms
      optimizer_step:    0.56 ms    0.56 ms
    train_step_total:  378.81 ms  378.81 ms
INFO 2022-05-16 05:35:01,516 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 34.833999999999996}, 'top_5': {'flatten': 63.598}}
INFO 2022-05-16 05:35:01,516 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:35:01,520 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:35:01,521 log_hooks.py: 425: [phase: 20] Saving checkpoint to ./checkpoints/LP_sgd/2/
INFO 2022-05-16 05:35:01,670 checkpoint.py: 131: Saved checkpoint: ./checkpoints/LP_sgd/2//model_phase20.torch
INFO 2022-05-16 05:35:01,670 checkpoint.py: 140: Creating symlink...
INFO 2022-05-16 05:35:01,674 checkpoint.py: 144: Created symlink: ./checkpoints/LP_sgd/2//checkpoint.torch
INFO 2022-05-16 05:35:01,675 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 41, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:35:36,826 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:35:36,827 state_update_hooks.py: 115: Starting phase 41 [test]
INFO 2022-05-16 05:35:42,772 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:35:42,776 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 297
INFO 2022-05-16 05:35:42,777 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 31.72}, 'top_5': {'flatten': 58.879999999999995}}
INFO 2022-05-16 05:35:42,777 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:35:42,780 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:35:42,781 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 42, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:36:15,464 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:36:15,464 state_update_hooks.py: 115: Starting phase 42 [train]
INFO 2022-05-16 05:36:50,637 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:36:50,642 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 358
INFO 2022-05-16 05:36:50,643 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  315.35 ms  315.38 ms
             forward:    5.63 ms   31.56 ms
        loss_compute:    0.60 ms    0.60 ms
     loss_all_reduce:    0.11 ms    0.12 ms
       meters_update:    7.45 ms    7.47 ms
            backward:    1.21 ms    1.22 ms
      optimizer_step:    0.62 ms    0.63 ms
    train_step_total:  358.70 ms  358.71 ms
INFO 2022-05-16 05:36:50,644 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 35.321999999999996}, 'top_5': {'flatten': 63.902}}
INFO 2022-05-16 05:36:50,644 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:36:50,648 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:36:50,649 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 43, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:37:26,532 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:37:26,533 state_update_hooks.py: 115: Starting phase 43 [test]
INFO 2022-05-16 05:37:33,375 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:37:33,380 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 342
INFO 2022-05-16 05:37:33,380 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 31.3}, 'top_5': {'flatten': 58.95}}
INFO 2022-05-16 05:37:33,381 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:37:33,384 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:37:33,384 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 44, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:38:11,376 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:38:11,377 state_update_hooks.py: 115: Starting phase 44 [train]
INFO 2022-05-16 05:38:28,152 log_hooks.py: 277: Rank: 0; [ep: 22] iter: 2200; lr: 0.02656; loss: 2.53248; btime(ms): 1286; eta: 2:42:54; peak_mem(M): 2605;
INFO 2022-05-16 05:38:47,324 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:38:47,328 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 366
INFO 2022-05-16 05:38:47,329 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  324.67 ms  324.70 ms
             forward:    5.20 ms   31.25 ms
        loss_compute:    0.53 ms    0.53 ms
     loss_all_reduce:    0.11 ms    0.11 ms
       meters_update:    6.70 ms    6.72 ms
            backward:    1.19 ms    1.20 ms
      optimizer_step:    0.57 ms    0.58 ms
    train_step_total:  366.62 ms  366.62 ms
INFO 2022-05-16 05:38:47,329 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 35.315999999999995}, 'top_5': {'flatten': 64.172}}
INFO 2022-05-16 05:38:47,330 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:38:47,332 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:38:47,333 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 45, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:39:20,855 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:39:20,856 state_update_hooks.py: 115: Starting phase 45 [test]
INFO 2022-05-16 05:39:27,227 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:39:27,232 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 318
INFO 2022-05-16 05:39:27,233 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 30.490000000000002}, 'top_5': {'flatten': 59.35}}
INFO 2022-05-16 05:39:27,233 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:39:27,236 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:39:27,237 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 46, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:39:59,770 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:39:59,771 state_update_hooks.py: 115: Starting phase 46 [train]
INFO 2022-05-16 05:40:32,274 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:40:32,278 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 331
INFO 2022-05-16 05:40:32,279 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  287.57 ms  287.61 ms
             forward:    5.76 ms   31.58 ms
        loss_compute:    0.62 ms    0.62 ms
     loss_all_reduce:    0.12 ms    0.13 ms
       meters_update:    8.02 ms    8.04 ms
            backward:    1.09 ms    1.10 ms
      optimizer_step:    0.59 ms    0.60 ms
    train_step_total:  331.48 ms  331.48 ms
INFO 2022-05-16 05:40:32,280 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 35.404}, 'top_5': {'flatten': 64.3}}
INFO 2022-05-16 05:40:32,280 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:40:32,283 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:40:32,283 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 47, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:41:03,829 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:41:03,830 state_update_hooks.py: 115: Starting phase 47 [test]
INFO 2022-05-16 05:41:09,438 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:41:09,443 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 280
INFO 2022-05-16 05:41:09,444 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 30.95}, 'top_5': {'flatten': 59.050000000000004}}
INFO 2022-05-16 05:41:09,444 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:41:09,446 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:41:09,447 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 48, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:41:41,620 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:41:41,620 state_update_hooks.py: 115: Starting phase 48 [train]
INFO 2022-05-16 05:41:57,388 log_hooks.py: 277: Rank: 0; [ep: 24] iter: 2400; lr: 0.02593; loss: 2.51253; btime(ms): 1251; eta: 2:34:21; peak_mem(M): 2605;
INFO 2022-05-16 05:42:13,117 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:42:13,121 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 321
INFO 2022-05-16 05:42:13,122 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  277.29 ms  277.32 ms
             forward:    5.55 ms   31.58 ms
        loss_compute:    0.58 ms    0.58 ms
     loss_all_reduce:    0.11 ms    0.11 ms
       meters_update:    8.01 ms    8.03 ms
            backward:    1.13 ms    1.14 ms
      optimizer_step:    0.59 ms    0.60 ms
    train_step_total:  321.21 ms  321.21 ms
INFO 2022-05-16 05:42:13,123 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 35.794}, 'top_5': {'flatten': 64.714}}
INFO 2022-05-16 05:42:13,123 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:42:13,126 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:42:13,126 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 49, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:42:43,842 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:42:43,843 state_update_hooks.py: 115: Starting phase 49 [test]
INFO 2022-05-16 05:42:49,512 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:42:49,517 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 283
INFO 2022-05-16 05:42:49,517 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 31.39}, 'top_5': {'flatten': 59.67}}
INFO 2022-05-16 05:42:49,518 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:42:49,521 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:42:49,521 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 50, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:43:19,681 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:43:19,682 state_update_hooks.py: 115: Starting phase 50 [train]
INFO 2022-05-16 05:43:50,916 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:43:50,921 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 318
INFO 2022-05-16 05:43:50,921 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  274.69 ms  274.72 ms
             forward:    5.68 ms   31.54 ms
        loss_compute:    0.62 ms    0.62 ms
     loss_all_reduce:    0.13 ms    0.13 ms
       meters_update:    7.77 ms    7.79 ms
            backward:    1.19 ms    1.20 ms
      optimizer_step:    0.60 ms    0.61 ms
    train_step_total:  318.52 ms  318.53 ms
INFO 2022-05-16 05:43:50,922 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 36.065999999999995}, 'top_5': {'flatten': 64.778}}
INFO 2022-05-16 05:43:50,922 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:43:50,925 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:43:50,926 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 51, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:44:20,753 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:44:20,754 state_update_hooks.py: 115: Starting phase 51 [test]
INFO 2022-05-16 05:44:26,198 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:44:26,202 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 272
INFO 2022-05-16 05:44:26,203 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 31.180000000000003}, 'top_5': {'flatten': 59.53000000000001}}
INFO 2022-05-16 05:44:26,203 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:44:26,206 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:44:26,206 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 52, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:44:56,605 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:44:56,606 state_update_hooks.py: 115: Starting phase 52 [train]
INFO 2022-05-16 05:45:14,176 log_hooks.py: 277: Rank: 0; [ep: 26] iter: 2600; lr: 0.02527; loss: 2.60356; btime(ms): 1218; eta: 2:26:12; peak_mem(M): 2605;
INFO 2022-05-16 05:45:28,432 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:45:28,436 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 324
INFO 2022-05-16 05:45:28,436 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  280.61 ms  280.64 ms
             forward:    5.90 ms   31.66 ms
        loss_compute:    0.61 ms    0.61 ms
     loss_all_reduce:    0.12 ms    0.12 ms
       meters_update:    7.75 ms    7.76 ms
            backward:    1.25 ms    1.26 ms
      optimizer_step:    0.67 ms    0.68 ms
    train_step_total:  324.56 ms  324.56 ms
INFO 2022-05-16 05:45:28,437 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 36.094}, 'top_5': {'flatten': 65.074}}
INFO 2022-05-16 05:45:28,437 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:45:28,440 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:45:28,440 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 53, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:45:59,454 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:45:59,455 state_update_hooks.py: 115: Starting phase 53 [test]
INFO 2022-05-16 05:46:04,986 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:46:04,990 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 276
INFO 2022-05-16 05:46:04,991 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 31.35}, 'top_5': {'flatten': 58.95}}
INFO 2022-05-16 05:46:04,992 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:46:04,994 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:46:04,995 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 54, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:46:37,697 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:46:37,699 state_update_hooks.py: 115: Starting phase 54 [train]
INFO 2022-05-16 05:47:11,725 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:47:11,730 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 347
INFO 2022-05-16 05:47:11,731 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  301.20 ms  301.24 ms
             forward:    6.49 ms   31.91 ms
        loss_compute:    0.74 ms    0.75 ms
     loss_all_reduce:    0.12 ms    0.12 ms
       meters_update:    8.62 ms    8.63 ms
            backward:    1.64 ms    1.65 ms
      optimizer_step:    0.65 ms    0.66 ms
    train_step_total:  346.99 ms  346.99 ms
INFO 2022-05-16 05:47:11,732 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 36.384}, 'top_5': {'flatten': 65.106}}
INFO 2022-05-16 05:47:11,732 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:47:11,735 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:47:11,736 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 55, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:47:46,391 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:47:46,392 state_update_hooks.py: 115: Starting phase 55 [test]
INFO 2022-05-16 05:47:53,088 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:47:53,096 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 335
INFO 2022-05-16 05:47:53,097 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 31.230000000000004}, 'top_5': {'flatten': 59.56}}
INFO 2022-05-16 05:47:53,097 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:47:53,100 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:47:53,100 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 56, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:48:29,799 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:48:29,799 state_update_hooks.py: 115: Starting phase 56 [train]
INFO 2022-05-16 05:48:51,310 log_hooks.py: 277: Rank: 0; [ep: 28] iter: 2800; lr: 0.02456; loss: 2.61663; btime(ms): 1195; eta: 2:19:31; peak_mem(M): 2605;
INFO 2022-05-16 05:49:14,789 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:49:14,798 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 459
INFO 2022-05-16 05:49:14,800 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  399.66 ms  399.75 ms
             forward:    8.83 ms   35.69 ms
        loss_compute:    1.14 ms    1.24 ms
     loss_all_reduce:    0.16 ms    0.14 ms
       meters_update:   15.39 ms   15.41 ms
            backward:    2.24 ms    2.33 ms
      optimizer_step:    0.75 ms    0.81 ms
    train_step_total:  458.78 ms  458.79 ms
INFO 2022-05-16 05:49:14,801 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 36.472}, 'top_5': {'flatten': 65.578}}
INFO 2022-05-16 05:49:14,802 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:49:14,808 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:49:14,809 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 57, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:50:05,736 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:50:05,737 state_update_hooks.py: 115: Starting phase 57 [test]
INFO 2022-05-16 05:50:16,105 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:50:16,113 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 518
INFO 2022-05-16 05:50:16,114 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 31.0}, 'top_5': {'flatten': 59.419999999999995}}
INFO 2022-05-16 05:50:16,115 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:50:16,118 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:50:16,118 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 58, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:50:55,333 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:50:55,334 state_update_hooks.py: 115: Starting phase 58 [train]
INFO 2022-05-16 05:51:51,176 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:51:51,188 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 569
INFO 2022-05-16 05:51:51,192 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  503.09 ms  503.17 ms
             forward:    9.63 ms   35.07 ms
        loss_compute:    1.38 ms    1.44 ms
     loss_all_reduce:    0.16 ms    0.19 ms
       meters_update:   21.89 ms   21.91 ms
            backward:    2.61 ms    2.73 ms
      optimizer_step:    0.98 ms    0.97 ms
    train_step_total:  569.53 ms  569.55 ms
INFO 2022-05-16 05:51:51,193 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 36.752}, 'top_5': {'flatten': 65.61}}
INFO 2022-05-16 05:51:51,196 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:51:51,198 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:51:51,199 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 59, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:52:38,098 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:52:38,099 state_update_hooks.py: 115: Starting phase 59 [test]
INFO 2022-05-16 05:52:48,264 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:52:48,271 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 508
INFO 2022-05-16 05:52:48,275 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 31.16}, 'top_5': {'flatten': 59.35}}
INFO 2022-05-16 05:52:48,276 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:52:48,278 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:52:48,279 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 60, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:53:23,074 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:53:23,075 state_update_hooks.py: 115: Starting phase 60 [train]
INFO 2022-05-16 05:53:51,987 log_hooks.py: 277: Rank: 0; [ep: 30] iter: 3000; lr: 0.02382; loss: 2.48968; btime(ms): 1199; eta: 2:15:57; peak_mem(M): 2605;
INFO 2022-05-16 05:54:13,412 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:54:13,420 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 513
INFO 2022-05-16 05:54:13,421 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  449.48 ms  449.59 ms
             forward:    7.25 ms   36.91 ms
        loss_compute:    1.14 ms    1.31 ms
     loss_all_reduce:    0.14 ms    0.13 ms
       meters_update:   19.94 ms   19.96 ms
            backward:    1.73 ms    1.88 ms
      optimizer_step:    0.79 ms    0.81 ms
    train_step_total:  513.33 ms  513.38 ms
INFO 2022-05-16 05:54:13,422 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 36.894}, 'top_5': {'flatten': 65.812}}
INFO 2022-05-16 05:54:13,423 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:54:13,431 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:54:13,432 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 61, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:55:02,406 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:55:02,407 state_update_hooks.py: 115: Starting phase 61 [test]
INFO 2022-05-16 05:55:11,455 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:55:11,460 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 452
INFO 2022-05-16 05:55:11,461 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 31.25}, 'top_5': {'flatten': 59.660000000000004}}
INFO 2022-05-16 05:55:11,461 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:55:11,464 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:55:11,464 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 62, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:55:46,369 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:55:46,370 state_update_hooks.py: 115: Starting phase 62 [train]
INFO 2022-05-16 05:56:45,779 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:56:45,788 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 606
INFO 2022-05-16 05:56:45,793 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  538.33 ms  538.32 ms
             forward:    6.64 ms   36.81 ms
        loss_compute:    1.12 ms    1.34 ms
     loss_all_reduce:    0.14 ms    0.15 ms
       meters_update:   24.00 ms   24.01 ms
            backward:    1.92 ms    2.16 ms
      optimizer_step:    0.82 ms    0.80 ms
    train_step_total:  605.99 ms  606.00 ms
INFO 2022-05-16 05:56:45,797 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 36.902}, 'top_5': {'flatten': 65.95400000000001}}
INFO 2022-05-16 05:56:45,798 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:56:45,801 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:56:45,801 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 63, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:57:31,224 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:57:31,225 state_update_hooks.py: 115: Starting phase 63 [test]
INFO 2022-05-16 05:57:37,668 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:57:37,675 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 322
INFO 2022-05-16 05:57:37,676 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 31.35}, 'top_5': {'flatten': 59.330000000000005}}
INFO 2022-05-16 05:57:37,676 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:57:37,679 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:57:37,680 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 64, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:58:16,966 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:58:16,967 state_update_hooks.py: 115: Starting phase 64 [train]
INFO 2022-05-16 05:58:56,928 log_hooks.py: 277: Rank: 0; [ep: 32] iter: 3200; lr: 0.02304; loss: 2.49208; btime(ms): 1204; eta: 2:12:27; peak_mem(M): 2605;
INFO 2022-05-16 05:59:15,664 trainer_main.py: 214: Meters synced
INFO 2022-05-16 05:59:15,670 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 599
INFO 2022-05-16 05:59:15,670 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  532.03 ms  532.12 ms
             forward:    6.45 ms   36.88 ms
        loss_compute:    0.99 ms    1.17 ms
     loss_all_reduce:    0.12 ms    0.11 ms
       meters_update:   23.28 ms   23.29 ms
            backward:    1.46 ms    1.65 ms
      optimizer_step:    0.93 ms    0.96 ms
    train_step_total:  598.74 ms  598.78 ms
INFO 2022-05-16 05:59:15,671 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 37.11}, 'top_5': {'flatten': 66.21000000000001}}
INFO 2022-05-16 05:59:15,675 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:59:15,681 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 05:59:15,682 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 65, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 05:59:55,950 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 05:59:55,950 state_update_hooks.py: 115: Starting phase 65 [test]
INFO 2022-05-16 06:00:02,864 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:00:02,869 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 345
INFO 2022-05-16 06:00:02,870 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 31.419999999999998}, 'top_5': {'flatten': 59.39}}
INFO 2022-05-16 06:00:02,870 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:00:02,873 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:00:02,874 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 66, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:00:45,697 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:00:45,697 state_update_hooks.py: 115: Starting phase 66 [train]
INFO 2022-05-16 06:01:45,427 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:01:45,434 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 609
INFO 2022-05-16 06:01:45,435 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  543.08 ms  543.11 ms
             forward:    6.28 ms   36.61 ms
        loss_compute:    1.19 ms    1.40 ms
     loss_all_reduce:    0.12 ms    0.11 ms
       meters_update:   22.72 ms   22.76 ms
            backward:    1.52 ms    1.79 ms
      optimizer_step:    0.97 ms    0.90 ms
    train_step_total:  609.26 ms  609.28 ms
INFO 2022-05-16 06:01:45,436 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 37.296}, 'top_5': {'flatten': 66.216}}
INFO 2022-05-16 06:01:45,437 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:01:45,440 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:01:45,441 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 67, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:02:22,618 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:02:22,619 state_update_hooks.py: 115: Starting phase 67 [test]
INFO 2022-05-16 06:02:28,781 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:02:28,786 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 308
INFO 2022-05-16 06:02:28,787 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 31.569999999999997}, 'top_5': {'flatten': 59.230000000000004}}
INFO 2022-05-16 06:02:28,788 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:02:28,790 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:02:28,791 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 68, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:03:15,198 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:03:15,199 state_update_hooks.py: 115: Starting phase 68 [train]
INFO 2022-05-16 06:03:57,771 log_hooks.py: 277: Rank: 0; [ep: 34] iter: 3400; lr: 0.02223; loss: 2.51982; btime(ms): 1207; eta: 2:08:45; peak_mem(M): 2605;
INFO 2022-05-16 06:04:11,001 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:04:11,007 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 569
INFO 2022-05-16 06:04:11,008 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  500.12 ms  500.20 ms
             forward:    6.78 ms   39.69 ms
        loss_compute:    1.36 ms    1.62 ms
     loss_all_reduce:    0.13 ms    0.13 ms
       meters_update:   22.32 ms   22.38 ms
            backward:    1.82 ms    2.07 ms
      optimizer_step:    0.76 ms    0.65 ms
    train_step_total:  569.16 ms  569.16 ms
INFO 2022-05-16 06:04:11,009 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 37.634}, 'top_5': {'flatten': 66.304}}
INFO 2022-05-16 06:04:11,009 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:04:11,012 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:04:11,012 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 69, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:04:46,242 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:04:46,243 state_update_hooks.py: 115: Starting phase 69 [test]
INFO 2022-05-16 06:04:56,402 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:04:56,409 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 508
INFO 2022-05-16 06:04:56,410 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 31.44}, 'top_5': {'flatten': 59.519999999999996}}
INFO 2022-05-16 06:04:56,411 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:04:56,415 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:04:56,416 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 70, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:05:45,392 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:05:45,393 state_update_hooks.py: 115: Starting phase 70 [train]
INFO 2022-05-16 06:06:32,401 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:06:32,405 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 479
INFO 2022-05-16 06:06:32,406 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  420.26 ms  420.33 ms
             forward:    6.98 ms   34.16 ms
        loss_compute:    0.91 ms    1.03 ms
     loss_all_reduce:    0.13 ms    0.12 ms
       meters_update:   18.39 ms   18.42 ms
            backward:    1.97 ms    2.05 ms
      optimizer_step:    0.85 ms    0.80 ms
    train_step_total:  479.43 ms  479.44 ms
INFO 2022-05-16 06:06:32,407 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 37.702000000000005}, 'top_5': {'flatten': 66.616}}
INFO 2022-05-16 06:06:32,407 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:06:32,410 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:06:32,410 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 71, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:07:11,057 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:07:11,058 state_update_hooks.py: 115: Starting phase 71 [test]
INFO 2022-05-16 06:07:21,105 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:07:21,111 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 502
INFO 2022-05-16 06:07:21,111 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 31.509999999999998}, 'top_5': {'flatten': 59.72}}
INFO 2022-05-16 06:07:21,112 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:07:21,114 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:07:21,115 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 72, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:08:08,328 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:08:08,328 state_update_hooks.py: 115: Starting phase 72 [train]
INFO 2022-05-16 06:08:40,062 log_hooks.py: 277: Rank: 0; [ep: 36] iter: 3600; lr: 0.02139; loss: 2.39471; btime(ms): 1205; eta: 2:04:32; peak_mem(M): 2605;
INFO 2022-05-16 06:08:48,611 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:08:48,616 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 411
INFO 2022-05-16 06:08:48,617 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  358.26 ms  358.32 ms
             forward:    7.60 ms   32.83 ms
        loss_compute:    1.00 ms    1.00 ms
     loss_all_reduce:    0.12 ms    0.13 ms
       meters_update:   13.74 ms   13.76 ms
            backward:    1.85 ms    1.86 ms
      optimizer_step:    0.66 ms    0.69 ms
    train_step_total:  410.82 ms  410.83 ms
INFO 2022-05-16 06:08:48,618 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 37.897999999999996}, 'top_5': {'flatten': 66.664}}
INFO 2022-05-16 06:08:48,618 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:08:48,621 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:08:48,622 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 73, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:09:32,194 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:09:32,195 state_update_hooks.py: 115: Starting phase 73 [test]
INFO 2022-05-16 06:09:42,111 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:09:42,119 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 496
INFO 2022-05-16 06:09:42,120 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 31.59}, 'top_5': {'flatten': 59.97}}
INFO 2022-05-16 06:09:42,121 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:09:42,124 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:09:42,124 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 74, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:10:27,981 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:10:27,982 state_update_hooks.py: 115: Starting phase 74 [train]
INFO 2022-05-16 06:11:03,141 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:11:03,146 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 358
INFO 2022-05-16 06:11:03,146 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  308.78 ms  308.83 ms
             forward:    7.99 ms   32.53 ms
        loss_compute:    1.38 ms    1.41 ms
     loss_all_reduce:    0.15 ms    0.16 ms
       meters_update:   10.53 ms   10.55 ms
            backward:    2.11 ms    2.14 ms
      optimizer_step:    0.72 ms    0.73 ms
    train_step_total:  358.52 ms  358.53 ms
INFO 2022-05-16 06:11:03,147 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 37.794}, 'top_5': {'flatten': 66.9}}
INFO 2022-05-16 06:11:03,147 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:11:03,150 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:11:03,151 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 75, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:11:49,705 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:11:49,706 state_update_hooks.py: 115: Starting phase 75 [test]
INFO 2022-05-16 06:11:59,904 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:11:59,914 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 510
INFO 2022-05-16 06:11:59,915 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 31.7}, 'top_5': {'flatten': 59.69}}
INFO 2022-05-16 06:11:59,918 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:11:59,921 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:11:59,922 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 76, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:12:42,977 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:12:42,979 state_update_hooks.py: 115: Starting phase 76 [train]
INFO 2022-05-16 06:13:10,860 log_hooks.py: 277: Rank: 0; [ep: 38] iter: 3800; lr: 0.02052; loss: 2.61276; btime(ms): 1201; eta: 2:00:07; peak_mem(M): 2605;
INFO 2022-05-16 06:13:22,473 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:13:22,478 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 403
INFO 2022-05-16 06:13:22,479 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  350.43 ms  350.47 ms
             forward:    7.48 ms   33.49 ms
        loss_compute:    0.92 ms    0.96 ms
     loss_all_reduce:    0.13 ms    0.13 ms
       meters_update:   12.80 ms   12.83 ms
            backward:    1.82 ms    1.87 ms
      optimizer_step:    0.70 ms    0.71 ms
    train_step_total:  402.77 ms  402.78 ms
INFO 2022-05-16 06:13:22,480 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 38.024}, 'top_5': {'flatten': 67.0}}
INFO 2022-05-16 06:13:22,480 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:13:22,483 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:13:22,484 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 77, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:14:09,584 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:14:09,585 state_update_hooks.py: 115: Starting phase 77 [test]
INFO 2022-05-16 06:14:19,405 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:14:19,410 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 491
INFO 2022-05-16 06:14:19,411 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 31.580000000000002}, 'top_5': {'flatten': 59.870000000000005}}
INFO 2022-05-16 06:14:19,412 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:14:19,415 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:14:19,416 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 78, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:14:58,147 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:14:58,149 state_update_hooks.py: 115: Starting phase 78 [train]
INFO 2022-05-16 06:15:33,634 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:15:33,637 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 362
INFO 2022-05-16 06:15:33,638 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  315.01 ms  315.05 ms
             forward:    6.64 ms   32.05 ms
        loss_compute:    0.74 ms    0.74 ms
     loss_all_reduce:    0.12 ms    0.13 ms
       meters_update:    9.91 ms    9.92 ms
            backward:    1.42 ms    1.44 ms
      optimizer_step:    0.63 ms    0.62 ms
    train_step_total:  361.88 ms  361.88 ms
INFO 2022-05-16 06:15:33,638 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 38.322}, 'top_5': {'flatten': 66.97}}
INFO 2022-05-16 06:15:33,639 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:15:33,641 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:15:33,642 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 79, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:16:04,295 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:16:04,296 state_update_hooks.py: 115: Starting phase 79 [test]
INFO 2022-05-16 06:16:10,556 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:16:10,559 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 313
INFO 2022-05-16 06:16:10,560 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.0}, 'top_5': {'flatten': 59.61}}
INFO 2022-05-16 06:16:10,560 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:16:10,563 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:16:10,563 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 80, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:16:43,706 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:16:43,707 state_update_hooks.py: 115: Starting phase 80 [train]
INFO 2022-05-16 06:17:17,632 log_hooks.py: 277: Rank: 0; [ep: 40] iter: 4000; lr: 0.01964; loss: 2.6841; btime(ms): 1192; eta: 1:55:17; peak_mem(M): 2605;
INFO 2022-05-16 06:17:22,941 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:17:22,946 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 400
INFO 2022-05-16 06:17:22,946 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  348.65 ms  348.70 ms
             forward:    8.32 ms   32.88 ms
        loss_compute:    0.98 ms    0.97 ms
     loss_all_reduce:    0.13 ms    0.14 ms
       meters_update:   12.01 ms   12.04 ms
            backward:    2.13 ms    2.14 ms
      optimizer_step:    0.79 ms    0.79 ms
    train_step_total:  400.09 ms  400.09 ms
INFO 2022-05-16 06:17:22,947 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 38.308}, 'top_5': {'flatten': 67.2}}
INFO 2022-05-16 06:17:22,947 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:17:22,950 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:17:22,950 log_hooks.py: 425: [phase: 40] Saving checkpoint to ./checkpoints/LP_sgd/2/
INFO 2022-05-16 06:17:23,099 checkpoint.py: 131: Saved checkpoint: ./checkpoints/LP_sgd/2//model_phase40.torch
INFO 2022-05-16 06:17:23,099 checkpoint.py: 140: Creating symlink...
INFO 2022-05-16 06:17:23,102 checkpoint.py: 144: Created symlink: ./checkpoints/LP_sgd/2//checkpoint.torch
INFO 2022-05-16 06:17:23,102 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 81, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:17:59,876 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:17:59,877 state_update_hooks.py: 115: Starting phase 81 [test]
INFO 2022-05-16 06:18:05,658 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:18:05,662 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 289
INFO 2022-05-16 06:18:05,663 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 31.65}, 'top_5': {'flatten': 59.77}}
INFO 2022-05-16 06:18:05,663 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:18:05,666 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:18:05,667 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 82, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:18:41,177 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:18:41,177 state_update_hooks.py: 115: Starting phase 82 [train]
INFO 2022-05-16 06:19:40,357 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:19:40,362 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 603
INFO 2022-05-16 06:19:40,363 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  535.15 ms  535.20 ms
             forward:    6.32 ms   37.42 ms
        loss_compute:    1.25 ms    1.47 ms
     loss_all_reduce:    0.15 ms    0.20 ms
       meters_update:   23.85 ms   23.87 ms
            backward:    1.47 ms    1.68 ms
      optimizer_step:    0.87 ms    0.86 ms
    train_step_total:  603.64 ms  603.68 ms
INFO 2022-05-16 06:19:40,364 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 38.408}, 'top_5': {'flatten': 67.5}}
INFO 2022-05-16 06:19:40,365 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:19:40,383 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:19:40,383 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 83, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:20:23,295 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:20:23,295 state_update_hooks.py: 115: Starting phase 83 [test]
INFO 2022-05-16 06:20:29,472 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:20:29,477 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 309
INFO 2022-05-16 06:20:29,477 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 31.5}, 'top_5': {'flatten': 59.5}}
INFO 2022-05-16 06:20:29,478 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:20:29,481 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:20:29,481 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 84, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:21:06,782 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:21:06,784 state_update_hooks.py: 115: Starting phase 84 [train]
INFO 2022-05-16 06:21:38,256 log_hooks.py: 277: Rank: 0; [ep: 42] iter: 4200; lr: 0.01873; loss: 2.39538; btime(ms): 1187; eta: 1:50:50; peak_mem(M): 2605;
INFO 2022-05-16 06:21:45,122 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:21:45,127 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 391
INFO 2022-05-16 06:21:45,128 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  339.06 ms  339.12 ms
             forward:    7.50 ms   32.37 ms
        loss_compute:    0.88 ms    0.89 ms
     loss_all_reduce:    0.13 ms    0.13 ms
       meters_update:   13.49 ms   13.51 ms
            backward:    1.92 ms    1.97 ms
      optimizer_step:    0.68 ms    0.69 ms
    train_step_total:  390.95 ms  390.95 ms
INFO 2022-05-16 06:21:45,132 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 38.550000000000004}, 'top_5': {'flatten': 67.54}}
INFO 2022-05-16 06:21:45,133 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:21:45,137 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:21:45,138 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 85, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:22:31,856 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:22:31,856 state_update_hooks.py: 115: Starting phase 85 [test]
INFO 2022-05-16 06:22:41,645 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:22:41,654 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 489
INFO 2022-05-16 06:22:41,654 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 31.900000000000002}, 'top_5': {'flatten': 60.099999999999994}}
INFO 2022-05-16 06:22:41,655 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:22:41,657 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:22:41,658 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 86, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:23:20,909 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:23:20,910 state_update_hooks.py: 115: Starting phase 86 [train]
INFO 2022-05-16 06:23:55,971 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:23:55,975 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 357
INFO 2022-05-16 06:23:55,975 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  310.18 ms  310.22 ms
             forward:    6.54 ms   32.12 ms
        loss_compute:    0.71 ms    0.71 ms
     loss_all_reduce:    0.12 ms    0.12 ms
       meters_update:   10.39 ms   10.41 ms
            backward:    1.43 ms    1.44 ms
      optimizer_step:    0.63 ms    0.64 ms
    train_step_total:  357.55 ms  357.55 ms
INFO 2022-05-16 06:23:55,976 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 38.672000000000004}, 'top_5': {'flatten': 67.46}}
INFO 2022-05-16 06:23:55,976 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:23:55,979 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:23:55,979 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 87, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:24:27,481 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:24:27,482 state_update_hooks.py: 115: Starting phase 87 [test]
INFO 2022-05-16 06:24:33,489 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:24:33,494 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 300
INFO 2022-05-16 06:24:33,495 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.190000000000005}, 'top_5': {'flatten': 59.88}}
INFO 2022-05-16 06:24:33,495 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:24:33,498 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:24:33,498 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 88, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:25:07,803 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:25:07,804 state_update_hooks.py: 115: Starting phase 88 [train]
INFO 2022-05-16 06:25:40,450 log_hooks.py: 277: Rank: 0; [ep: 44] iter: 4400; lr: 0.01781; loss: 2.41727; btime(ms): 1179; eta: 1:46:09; peak_mem(M): 2605;
INFO 2022-05-16 06:25:45,063 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:25:45,070 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 380
INFO 2022-05-16 06:25:45,071 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  328.00 ms  328.07 ms
             forward:    8.19 ms   33.64 ms
        loss_compute:    1.02 ms    1.08 ms
     loss_all_reduce:    0.16 ms    0.16 ms
       meters_update:   11.82 ms   11.84 ms
            backward:    2.04 ms    2.09 ms
      optimizer_step:    0.68 ms    0.67 ms
    train_step_total:  379.94 ms  379.97 ms
INFO 2022-05-16 06:25:45,072 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 38.82}, 'top_5': {'flatten': 67.67800000000001}}
INFO 2022-05-16 06:25:45,073 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:25:45,079 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:25:45,080 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 89, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:26:32,081 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:26:32,082 state_update_hooks.py: 115: Starting phase 89 [test]
INFO 2022-05-16 06:26:42,164 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:26:42,169 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 504
INFO 2022-05-16 06:26:42,170 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.06}, 'top_5': {'flatten': 60.14000000000001}}
INFO 2022-05-16 06:26:42,171 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:26:42,175 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:26:42,176 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 90, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:27:20,972 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:27:20,973 state_update_hooks.py: 115: Starting phase 90 [train]
INFO 2022-05-16 06:28:04,044 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:28:04,052 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 439
INFO 2022-05-16 06:28:04,053 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  380.89 ms  380.92 ms
             forward:    7.78 ms   34.34 ms
        loss_compute:    1.04 ms    1.08 ms
     loss_all_reduce:    0.13 ms    0.13 ms
       meters_update:   17.49 ms   17.52 ms
            backward:    2.00 ms    2.05 ms
      optimizer_step:    0.74 ms    0.75 ms
    train_step_total:  439.25 ms  439.24 ms
INFO 2022-05-16 06:28:04,054 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 39.062000000000005}, 'top_5': {'flatten': 67.62}}
INFO 2022-05-16 06:28:04,055 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:28:04,060 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:28:04,061 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 91, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:28:51,100 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:28:51,101 state_update_hooks.py: 115: Starting phase 91 [test]
INFO 2022-05-16 06:29:01,385 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:29:01,390 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 514
INFO 2022-05-16 06:29:01,391 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 31.830000000000002}, 'top_5': {'flatten': 59.919999999999995}}
INFO 2022-05-16 06:29:01,392 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:29:01,398 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:29:01,399 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 92, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:29:39,789 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:29:39,789 state_update_hooks.py: 115: Starting phase 92 [train]
INFO 2022-05-16 06:30:30,347 log_hooks.py: 277: Rank: 0; [ep: 46] iter: 4600; lr: 0.01688; loss: 2.35148; btime(ms): 1180; eta: 1:42:19; peak_mem(M): 2605;
INFO 2022-05-16 06:30:32,603 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:30:32,613 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 539
INFO 2022-05-16 06:30:32,614 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  474.68 ms  474.72 ms
             forward:    7.14 ms   37.05 ms
        loss_compute:    1.03 ms    1.24 ms
     loss_all_reduce:    0.21 ms    0.17 ms
       meters_update:   20.22 ms   20.24 ms
            backward:    1.74 ms    1.87 ms
      optimizer_step:    0.73 ms    0.78 ms
    train_step_total:  538.69 ms  538.73 ms
INFO 2022-05-16 06:30:32,615 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 39.254}, 'top_5': {'flatten': 68.0}}
INFO 2022-05-16 06:30:32,616 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:30:32,618 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:30:32,622 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 93, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:31:20,240 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:31:20,241 state_update_hooks.py: 115: Starting phase 93 [test]
INFO 2022-05-16 06:31:28,154 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:31:28,155 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 395
INFO 2022-05-16 06:31:28,156 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 31.919999999999998}, 'top_5': {'flatten': 60.12}}
INFO 2022-05-16 06:31:28,156 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:31:28,159 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:31:28,159 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 94, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:32:02,199 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:32:02,200 state_update_hooks.py: 115: Starting phase 94 [train]
INFO 2022-05-16 06:33:01,742 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:33:01,747 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 607
INFO 2022-05-16 06:33:01,748 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  540.79 ms  540.85 ms
             forward:    6.63 ms   37.14 ms
        loss_compute:    1.08 ms    1.26 ms
     loss_all_reduce:    0.13 ms    0.15 ms
       meters_update:   21.96 ms   22.01 ms
            backward:    1.94 ms    2.14 ms
      optimizer_step:    0.95 ms    0.89 ms
    train_step_total:  607.35 ms  607.36 ms
INFO 2022-05-16 06:33:01,749 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 39.194}, 'top_5': {'flatten': 68.05}}
INFO 2022-05-16 06:33:01,749 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:33:01,752 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:33:01,753 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 95, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:33:44,508 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:33:44,509 state_update_hooks.py: 115: Starting phase 95 [test]
INFO 2022-05-16 06:33:50,679 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:33:50,683 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 308
INFO 2022-05-16 06:33:50,684 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.2}, 'top_5': {'flatten': 60.099999999999994}}
INFO 2022-05-16 06:33:50,685 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:33:50,687 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:33:50,687 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 96, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:34:30,020 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:34:30,021 state_update_hooks.py: 115: Starting phase 96 [train]
INFO 2022-05-16 06:35:29,055 log_hooks.py: 277: Rank: 0; [ep: 48] iter: 4800; lr: 0.01594; loss: 2.37815; btime(ms): 1183; eta: 1:38:36; peak_mem(M): 2605;
INFO 2022-05-16 06:35:29,129 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:35:29,136 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 603
INFO 2022-05-16 06:35:29,137 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  536.21 ms  536.28 ms
             forward:    6.42 ms   36.15 ms
        loss_compute:    1.06 ms    1.21 ms
     loss_all_reduce:    0.13 ms    0.12 ms
       meters_update:   24.25 ms   24.30 ms
            backward:    1.41 ms    1.57 ms
      optimizer_step:    0.83 ms    0.81 ms
    train_step_total:  602.91 ms  602.90 ms
INFO 2022-05-16 06:35:29,139 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 39.012}, 'top_5': {'flatten': 68.096}}
INFO 2022-05-16 06:35:29,139 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:35:29,145 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:35:29,146 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 97, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:36:08,188 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:36:08,190 state_update_hooks.py: 115: Starting phase 97 [test]
INFO 2022-05-16 06:36:14,503 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:36:14,508 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 315
INFO 2022-05-16 06:36:14,509 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 31.6}, 'top_5': {'flatten': 60.050000000000004}}
INFO 2022-05-16 06:36:14,509 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:36:14,512 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:36:14,512 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 98, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:36:57,735 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:36:57,735 state_update_hooks.py: 115: Starting phase 98 [train]
INFO 2022-05-16 06:37:58,602 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:37:58,612 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 621
INFO 2022-05-16 06:37:58,612 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  551.56 ms  551.63 ms
             forward:    6.40 ms   38.46 ms
        loss_compute:    1.07 ms    1.32 ms
     loss_all_reduce:    0.18 ms    0.16 ms
       meters_update:   23.86 ms   23.90 ms
            backward:    1.57 ms    1.77 ms
      optimizer_step:    0.77 ms    0.77 ms
    train_step_total:  620.61 ms  620.86 ms
INFO 2022-05-16 06:37:58,613 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 39.467999999999996}, 'top_5': {'flatten': 68.052}}
INFO 2022-05-16 06:37:58,614 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:37:58,617 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:37:58,617 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 99, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:38:32,434 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:38:32,435 state_update_hooks.py: 115: Starting phase 99 [test]
INFO 2022-05-16 06:38:38,916 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:38:38,921 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 324
INFO 2022-05-16 06:38:38,923 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 31.97}, 'top_5': {'flatten': 60.12}}
INFO 2022-05-16 06:38:38,923 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:38:38,929 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:38:38,932 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 100, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:39:25,681 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:39:25,681 state_update_hooks.py: 115: Starting phase 100 [train]
INFO 2022-05-16 06:40:20,495 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:40:20,500 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 559
INFO 2022-05-16 06:40:20,501 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  491.80 ms  491.90 ms
             forward:    6.72 ms   38.08 ms
        loss_compute:    1.24 ms    1.40 ms
     loss_all_reduce:    0.16 ms    0.14 ms
       meters_update:   20.95 ms   21.03 ms
            backward:    2.25 ms    2.42 ms
      optimizer_step:    0.85 ms    0.94 ms
    train_step_total:  559.08 ms  559.09 ms
INFO 2022-05-16 06:40:20,502 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 39.418}, 'top_5': {'flatten': 68.362}}
INFO 2022-05-16 06:40:20,502 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:40:20,505 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:40:20,506 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 101, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:40:57,366 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:40:57,367 state_update_hooks.py: 115: Starting phase 101 [test]
INFO 2022-05-16 06:41:07,580 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:41:07,588 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 511
INFO 2022-05-16 06:41:07,589 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.09}, 'top_5': {'flatten': 59.95}}
INFO 2022-05-16 06:41:07,589 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:41:07,592 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:41:07,592 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 102, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:41:56,465 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:41:56,465 state_update_hooks.py: 115: Starting phase 102 [train]
INFO 2022-05-16 06:41:56,829 log_hooks.py: 277: Rank: 0; [ep: 51] iter: 5000; lr: 0.01453; loss: 2.36725; btime(ms): 1196; eta: 1:35:43; peak_mem(M): 2605;
INFO 2022-05-16 06:42:42,120 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:42:42,125 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 465
INFO 2022-05-16 06:42:42,125 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  406.93 ms  406.93 ms
             forward:    6.80 ms   35.61 ms
        loss_compute:    1.18 ms    1.30 ms
     loss_all_reduce:    0.14 ms    0.14 ms
       meters_update:   16.59 ms   16.62 ms
            backward:    1.62 ms    1.73 ms
      optimizer_step:    0.73 ms    0.78 ms
    train_step_total:  465.63 ms  465.64 ms
INFO 2022-05-16 06:42:42,126 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 39.702}, 'top_5': {'flatten': 68.352}}
INFO 2022-05-16 06:42:42,126 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:42:42,129 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:42:42,129 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 103, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:43:22,202 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:43:22,203 state_update_hooks.py: 115: Starting phase 103 [test]
INFO 2022-05-16 06:43:32,943 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:43:32,949 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 537
INFO 2022-05-16 06:43:32,953 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.26}, 'top_5': {'flatten': 60.029999999999994}}
INFO 2022-05-16 06:43:32,954 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:43:32,966 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:43:32,967 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 104, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:44:21,773 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:44:21,773 state_update_hooks.py: 115: Starting phase 104 [train]
INFO 2022-05-16 06:45:00,147 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:45:00,152 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 391
INFO 2022-05-16 06:45:00,152 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  341.34 ms  341.40 ms
             forward:    7.64 ms   33.07 ms
        loss_compute:    0.93 ms    0.95 ms
     loss_all_reduce:    0.13 ms    0.13 ms
       meters_update:   10.89 ms   10.90 ms
            backward:    1.87 ms    1.89 ms
      optimizer_step:    0.79 ms    0.80 ms
    train_step_total:  391.34 ms  391.35 ms
INFO 2022-05-16 06:45:00,153 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 39.738}, 'top_5': {'flatten': 68.47800000000001}}
INFO 2022-05-16 06:45:00,153 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:45:00,156 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:45:00,156 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 105, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:45:43,104 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:45:43,105 state_update_hooks.py: 115: Starting phase 105 [test]
INFO 2022-05-16 06:45:53,280 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:45:53,285 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 509
INFO 2022-05-16 06:45:53,286 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.34}, 'top_5': {'flatten': 60.17}}
INFO 2022-05-16 06:45:53,287 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:45:53,289 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:45:53,290 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 106, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:46:37,730 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:46:37,730 state_update_hooks.py: 115: Starting phase 106 [train]
INFO 2022-05-16 06:46:39,413 log_hooks.py: 277: Rank: 0; [ep: 53] iter: 5200; lr: 0.01359; loss: 2.41531; btime(ms): 1195; eta: 1:31:41; peak_mem(M): 2605;
INFO 2022-05-16 06:47:15,335 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:47:15,340 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 383
INFO 2022-05-16 06:47:15,340 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  334.45 ms  334.49 ms
             forward:    7.57 ms   32.55 ms
        loss_compute:    1.07 ms    1.05 ms
     loss_all_reduce:    0.14 ms    0.14 ms
       meters_update:   10.41 ms   10.42 ms
            backward:    1.86 ms    1.88 ms
      optimizer_step:    0.61 ms    0.62 ms
    train_step_total:  383.48 ms  383.48 ms
INFO 2022-05-16 06:47:15,341 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 39.89}, 'top_5': {'flatten': 68.432}}
INFO 2022-05-16 06:47:15,341 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:47:15,344 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:47:15,345 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 107, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:48:05,271 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:48:05,272 state_update_hooks.py: 115: Starting phase 107 [test]
INFO 2022-05-16 06:48:15,628 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:48:15,634 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 518
INFO 2022-05-16 06:48:15,638 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.48}, 'top_5': {'flatten': 60.35}}
INFO 2022-05-16 06:48:15,638 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:48:15,641 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:48:15,641 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 108, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:48:57,003 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:48:57,004 state_update_hooks.py: 115: Starting phase 108 [train]
INFO 2022-05-16 06:49:39,002 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:49:39,007 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 428
INFO 2022-05-16 06:49:39,008 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  371.18 ms  371.24 ms
             forward:    7.80 ms   35.23 ms
        loss_compute:    1.08 ms    1.18 ms
     loss_all_reduce:    0.22 ms    0.20 ms
       meters_update:   14.83 ms   14.85 ms
            backward:    1.95 ms    2.01 ms
      optimizer_step:    0.76 ms    0.79 ms
    train_step_total:  428.31 ms  428.32 ms
INFO 2022-05-16 06:49:39,012 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 39.972}, 'top_5': {'flatten': 68.642}}
INFO 2022-05-16 06:49:39,013 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:49:39,019 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:49:39,022 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 109, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:50:26,761 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:50:26,762 state_update_hooks.py: 115: Starting phase 109 [test]
INFO 2022-05-16 06:50:36,806 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:50:36,811 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 502
INFO 2022-05-16 06:50:36,812 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.33}, 'top_5': {'flatten': 59.99}}
INFO 2022-05-16 06:50:36,813 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:50:36,816 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:50:36,817 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 110, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:51:13,844 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:51:13,845 state_update_hooks.py: 115: Starting phase 110 [train]
INFO 2022-05-16 06:51:16,879 log_hooks.py: 277: Rank: 0; [ep: 55] iter: 5400; lr: 0.01265; loss: 2.38021; btime(ms): 1194; eta: 1:27:35; peak_mem(M): 2605;
INFO 2022-05-16 06:52:02,543 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:52:02,549 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 496
INFO 2022-05-16 06:52:02,552 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  434.60 ms  434.65 ms
             forward:    7.10 ms   35.92 ms
        loss_compute:    0.97 ms    1.12 ms
     loss_all_reduce:    0.13 ms    0.12 ms
       meters_update:   19.80 ms   19.82 ms
            backward:    1.72 ms    1.84 ms
      optimizer_step:    0.79 ms    0.79 ms
    train_step_total:  496.69 ms  496.71 ms
INFO 2022-05-16 06:52:02,554 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 39.946}, 'top_5': {'flatten': 68.596}}
INFO 2022-05-16 06:52:02,554 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:52:02,557 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:52:02,557 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 111, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:52:49,847 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:52:49,848 state_update_hooks.py: 115: Starting phase 111 [test]
INFO 2022-05-16 06:53:00,154 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:53:00,160 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 515
INFO 2022-05-16 06:53:00,164 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.09}, 'top_5': {'flatten': 60.550000000000004}}
INFO 2022-05-16 06:53:00,165 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:53:00,168 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:53:00,170 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 112, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:53:33,794 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:53:33,795 state_update_hooks.py: 115: Starting phase 112 [train]
INFO 2022-05-16 06:54:30,623 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:54:30,640 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 580
INFO 2022-05-16 06:54:30,641 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  513.27 ms  513.26 ms
             forward:    6.56 ms   37.20 ms
        loss_compute:    1.08 ms    1.30 ms
     loss_all_reduce:    0.18 ms    0.17 ms
       meters_update:   22.59 ms   22.70 ms
            backward:    1.56 ms    1.74 ms
      optimizer_step:    0.73 ms    0.76 ms
    train_step_total:  579.66 ms  579.62 ms
INFO 2022-05-16 06:54:30,642 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 40.138}, 'top_5': {'flatten': 68.882}}
INFO 2022-05-16 06:54:30,643 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:54:30,655 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:54:30,656 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 113, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:55:14,730 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:55:14,731 state_update_hooks.py: 115: Starting phase 113 [test]
INFO 2022-05-16 06:55:20,514 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:55:20,519 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 289
INFO 2022-05-16 06:55:20,520 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.31}, 'top_5': {'flatten': 59.9}}
INFO 2022-05-16 06:55:20,521 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:55:20,523 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:55:20,524 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 114, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:55:57,005 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:55:57,006 state_update_hooks.py: 115: Starting phase 114 [train]
INFO 2022-05-16 06:56:04,749 log_hooks.py: 277: Rank: 0; [ep: 57] iter: 5600; lr: 0.01173; loss: 2.40354; btime(ms): 1194; eta: 1:23:37; peak_mem(M): 2605;
INFO 2022-05-16 06:56:54,576 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:56:54,582 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 587
INFO 2022-05-16 06:56:54,583 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  518.91 ms  518.92 ms
             forward:    6.21 ms   37.33 ms
        loss_compute:    1.15 ms    1.38 ms
     loss_all_reduce:    0.13 ms    0.12 ms
       meters_update:   24.17 ms   24.23 ms
            backward:    1.67 ms    1.86 ms
      optimizer_step:    0.89 ms    0.90 ms
    train_step_total:  587.23 ms  587.22 ms
INFO 2022-05-16 06:56:54,584 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 40.02}, 'top_5': {'flatten': 68.754}}
INFO 2022-05-16 06:56:54,584 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:56:54,587 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:56:54,590 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 115, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:57:34,813 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:57:34,814 state_update_hooks.py: 115: Starting phase 115 [test]
INFO 2022-05-16 06:57:40,754 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:57:40,758 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 297
INFO 2022-05-16 06:57:40,759 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.51}, 'top_5': {'flatten': 60.49}}
INFO 2022-05-16 06:57:40,760 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:57:40,762 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:57:40,763 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 116, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:58:21,155 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:58:21,157 state_update_hooks.py: 115: Starting phase 116 [train]
INFO 2022-05-16 06:59:20,001 trainer_main.py: 214: Meters synced
INFO 2022-05-16 06:59:20,007 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 600
INFO 2022-05-16 06:59:20,008 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  534.29 ms  534.41 ms
             forward:    6.27 ms   36.54 ms
        loss_compute:    0.95 ms    1.08 ms
     loss_all_reduce:    0.13 ms    0.12 ms
       meters_update:   23.02 ms   23.06 ms
            backward:    1.63 ms    1.77 ms
      optimizer_step:    0.81 ms    0.82 ms
    train_step_total:  600.21 ms  600.26 ms
INFO 2022-05-16 06:59:20,009 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 40.300000000000004}, 'top_5': {'flatten': 69.024}}
INFO 2022-05-16 06:59:20,009 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:59:20,012 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 06:59:20,013 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 117, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 06:59:56,829 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 06:59:56,830 state_update_hooks.py: 115: Starting phase 117 [test]
INFO 2022-05-16 07:00:03,069 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:00:03,075 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 312
INFO 2022-05-16 07:00:03,076 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.11}, 'top_5': {'flatten': 60.35}}
INFO 2022-05-16 07:00:03,076 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:00:03,079 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:00:03,079 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 118, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:00:48,320 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:00:48,320 state_update_hooks.py: 115: Starting phase 118 [train]
INFO 2022-05-16 07:00:58,527 log_hooks.py: 277: Rank: 0; [ep: 59] iter: 5800; lr: 0.01082; loss: 2.41699; btime(ms): 1195; eta: 1:19:42; peak_mem(M): 2605;
INFO 2022-05-16 07:01:45,627 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:01:45,631 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 584
INFO 2022-05-16 07:01:45,632 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  515.90 ms  515.97 ms
             forward:    6.34 ms   38.00 ms
        loss_compute:    1.10 ms    1.40 ms
     loss_all_reduce:    0.13 ms    0.17 ms
       meters_update:   23.45 ms   23.49 ms
            backward:    1.61 ms    1.73 ms
      optimizer_step:    0.85 ms    0.91 ms
    train_step_total:  584.53 ms  584.53 ms
INFO 2022-05-16 07:01:45,633 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 40.45}, 'top_5': {'flatten': 68.998}}
INFO 2022-05-16 07:01:45,633 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:01:45,635 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:01:45,636 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 119, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:02:20,986 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:02:20,987 state_update_hooks.py: 115: Starting phase 119 [test]
INFO 2022-05-16 07:02:31,113 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:02:31,119 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 506
INFO 2022-05-16 07:02:31,120 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.24}, 'top_5': {'flatten': 60.129999999999995}}
INFO 2022-05-16 07:02:31,124 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:02:31,130 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:02:31,130 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 120, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:03:19,352 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:03:19,353 state_update_hooks.py: 115: Starting phase 120 [train]
INFO 2022-05-16 07:04:08,166 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:04:08,172 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 498
INFO 2022-05-16 07:04:08,173 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  434.75 ms  434.80 ms
             forward:    7.31 ms   36.33 ms
        loss_compute:    1.05 ms    1.25 ms
     loss_all_reduce:    0.13 ms    0.15 ms
       meters_update:   18.60 ms   18.63 ms
            backward:    1.96 ms    2.06 ms
      optimizer_step:    0.81 ms    0.88 ms
    train_step_total:  497.87 ms  497.89 ms
INFO 2022-05-16 07:04:08,174 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 40.6}, 'top_5': {'flatten': 69.284}}
INFO 2022-05-16 07:04:08,174 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:04:08,177 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:04:08,177 log_hooks.py: 425: [phase: 60] Saving checkpoint to ./checkpoints/LP_sgd/2/
INFO 2022-05-16 07:04:08,368 checkpoint.py: 131: Saved checkpoint: ./checkpoints/LP_sgd/2//model_phase60.torch
INFO 2022-05-16 07:04:08,369 checkpoint.py: 140: Creating symlink...
INFO 2022-05-16 07:04:08,371 checkpoint.py: 144: Created symlink: ./checkpoints/LP_sgd/2//checkpoint.torch
INFO 2022-05-16 07:04:08,372 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 121, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:04:47,432 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:04:47,432 state_update_hooks.py: 115: Starting phase 121 [test]
INFO 2022-05-16 07:04:57,796 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:04:57,803 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 518
INFO 2022-05-16 07:04:57,804 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.66}, 'top_5': {'flatten': 60.209999999999994}}
INFO 2022-05-16 07:04:57,805 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:04:57,807 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:04:57,808 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 122, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:05:46,542 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:05:46,543 state_update_hooks.py: 115: Starting phase 122 [train]
INFO 2022-05-16 07:05:59,002 log_hooks.py: 277: Rank: 0; [ep: 61] iter: 6000; lr: 0.00992; loss: 2.52189; btime(ms): 1197; eta: 1:15:50; peak_mem(M): 2605;
INFO 2022-05-16 07:06:27,049 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:06:27,054 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 413
INFO 2022-05-16 07:06:27,054 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  357.50 ms  357.58 ms
             forward:    7.73 ms   34.14 ms
        loss_compute:    1.06 ms    1.14 ms
     loss_all_reduce:    0.13 ms    0.13 ms
       meters_update:   15.17 ms   15.19 ms
            backward:    1.94 ms    1.99 ms
      optimizer_step:    0.70 ms    0.69 ms
    train_step_total:  413.08 ms  413.07 ms
INFO 2022-05-16 07:06:27,055 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 40.348}, 'top_5': {'flatten': 69.296}}
INFO 2022-05-16 07:06:27,056 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:06:27,058 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:06:27,059 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 123, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:07:10,017 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:07:10,018 state_update_hooks.py: 115: Starting phase 123 [test]
INFO 2022-05-16 07:07:19,875 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:07:19,881 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 493
INFO 2022-05-16 07:07:19,882 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.22}, 'top_5': {'flatten': 60.22}}
INFO 2022-05-16 07:07:19,882 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:07:19,888 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:07:19,888 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 124, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:08:04,541 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:08:04,542 state_update_hooks.py: 115: Starting phase 124 [train]
INFO 2022-05-16 07:08:38,241 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:08:38,246 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 343
INFO 2022-05-16 07:08:38,247 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  290.55 ms  290.61 ms
             forward:    9.49 ms   34.31 ms
        loss_compute:    1.02 ms    1.02 ms
     loss_all_reduce:    0.13 ms    0.13 ms
       meters_update:   12.41 ms   12.43 ms
            backward:    2.01 ms    2.03 ms
      optimizer_step:    0.72 ms    0.72 ms
    train_step_total:  343.63 ms  343.63 ms
INFO 2022-05-16 07:08:38,247 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 40.458}, 'top_5': {'flatten': 69.106}}
INFO 2022-05-16 07:08:38,248 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:08:38,250 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:08:38,251 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 125, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:09:22,431 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:09:22,432 state_update_hooks.py: 115: Starting phase 125 [test]
INFO 2022-05-16 07:09:32,391 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:09:32,400 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 498
INFO 2022-05-16 07:09:32,401 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.31}, 'top_5': {'flatten': 60.440000000000005}}
INFO 2022-05-16 07:09:32,401 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:09:32,408 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:09:32,409 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 126, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:10:15,204 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:10:15,205 state_update_hooks.py: 115: Starting phase 126 [train]
INFO 2022-05-16 07:10:24,230 log_hooks.py: 277: Rank: 0; [ep: 63] iter: 6200; lr: 0.00904; loss: 2.3354; btime(ms): 1194; eta: 1:11:40; peak_mem(M): 2605;
INFO 2022-05-16 07:10:52,336 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:10:52,342 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 378
INFO 2022-05-16 07:10:52,343 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  327.45 ms  327.49 ms
             forward:    7.88 ms   32.48 ms
        loss_compute:    1.09 ms    1.07 ms
     loss_all_reduce:    0.16 ms    0.16 ms
       meters_update:   11.52 ms   11.53 ms
            backward:    2.11 ms    2.12 ms
      optimizer_step:    0.75 ms    0.77 ms
    train_step_total:  378.64 ms  378.64 ms
INFO 2022-05-16 07:10:52,344 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 40.776}, 'top_5': {'flatten': 69.342}}
INFO 2022-05-16 07:10:52,344 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:10:52,351 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:10:52,352 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 127, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:11:39,863 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:11:39,864 state_update_hooks.py: 115: Starting phase 127 [test]
INFO 2022-05-16 07:11:49,797 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:11:49,805 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 497
INFO 2022-05-16 07:11:49,806 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.12}, 'top_5': {'flatten': 60.25}}
INFO 2022-05-16 07:11:49,806 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:11:49,809 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:11:49,810 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 128, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:12:29,550 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:12:29,551 state_update_hooks.py: 115: Starting phase 128 [train]
INFO 2022-05-16 07:13:13,646 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:13:13,652 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 450
INFO 2022-05-16 07:13:13,653 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  393.46 ms  393.51 ms
             forward:    8.33 ms   34.00 ms
        loss_compute:    0.91 ms    0.93 ms
     loss_all_reduce:    0.13 ms    0.14 ms
       meters_update:   15.74 ms   15.77 ms
            backward:    1.89 ms    1.94 ms
      optimizer_step:    0.76 ms    0.76 ms
    train_step_total:  449.70 ms  449.70 ms
INFO 2022-05-16 07:13:13,657 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 40.838}, 'top_5': {'flatten': 69.426}}
INFO 2022-05-16 07:13:13,657 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:13:13,661 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:13:13,662 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 129, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:14:00,606 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:14:00,607 state_update_hooks.py: 115: Starting phase 129 [test]
INFO 2022-05-16 07:14:10,776 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:14:10,782 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 508
INFO 2022-05-16 07:14:10,784 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.16}, 'top_5': {'flatten': 60.529999999999994}}
INFO 2022-05-16 07:14:10,784 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:14:10,792 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:14:10,793 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 130, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:14:46,569 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:14:46,570 state_update_hooks.py: 115: Starting phase 130 [train]
INFO 2022-05-16 07:14:56,906 log_hooks.py: 277: Rank: 0; [ep: 65] iter: 6400; lr: 0.00819; loss: 2.52829; btime(ms): 1192; eta: 1:07:35; peak_mem(M): 2605;
INFO 2022-05-16 07:15:36,348 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:15:36,356 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 508
INFO 2022-05-16 07:15:36,356 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  444.46 ms  444.64 ms
             forward:    7.44 ms   36.33 ms
        loss_compute:    1.08 ms    1.24 ms
     loss_all_reduce:    0.12 ms    0.12 ms
       meters_update:   19.52 ms   19.54 ms
            backward:    1.83 ms    1.93 ms
      optimizer_step:    0.82 ms    0.80 ms
    train_step_total:  507.71 ms  507.74 ms
INFO 2022-05-16 07:15:36,357 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 40.838}, 'top_5': {'flatten': 69.408}}
INFO 2022-05-16 07:15:36,358 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:15:36,360 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:15:36,361 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 131, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:16:23,438 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:16:23,440 state_update_hooks.py: 115: Starting phase 131 [test]
INFO 2022-05-16 07:16:32,332 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:16:32,337 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 444
INFO 2022-05-16 07:16:32,337 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.48}, 'top_5': {'flatten': 60.27}}
INFO 2022-05-16 07:16:32,338 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:16:32,340 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:16:32,341 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 132, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:17:07,204 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:17:07,204 state_update_hooks.py: 115: Starting phase 132 [train]
INFO 2022-05-16 07:18:06,485 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:18:06,490 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 604
INFO 2022-05-16 07:18:06,491 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  533.43 ms  533.51 ms
             forward:    6.76 ms   38.70 ms
        loss_compute:    1.37 ms    1.82 ms
     loss_all_reduce:    0.15 ms    0.12 ms
       meters_update:   24.46 ms   24.35 ms
            backward:    1.90 ms    2.10 ms
      optimizer_step:    1.01 ms    0.94 ms
    train_step_total:  604.64 ms  604.69 ms
INFO 2022-05-16 07:18:06,493 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 40.928}, 'top_5': {'flatten': 69.604}}
INFO 2022-05-16 07:18:06,495 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:18:06,502 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:18:06,503 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 133, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:18:50,708 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:18:50,709 state_update_hooks.py: 115: Starting phase 133 [test]
INFO 2022-05-16 07:18:57,118 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:18:57,123 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 320
INFO 2022-05-16 07:18:57,124 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.58}, 'top_5': {'flatten': 60.23}}
INFO 2022-05-16 07:18:57,124 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:18:57,127 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:18:57,127 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 134, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:19:36,675 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:19:36,675 state_update_hooks.py: 115: Starting phase 134 [train]
INFO 2022-05-16 07:19:56,832 log_hooks.py: 277: Rank: 0; [ep: 67] iter: 6600; lr: 0.00736; loss: 2.25877; btime(ms): 1194; eta: 1:03:42; peak_mem(M): 2605;
INFO 2022-05-16 07:20:36,451 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:20:36,457 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 610
INFO 2022-05-16 07:20:36,461 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  542.89 ms  542.98 ms
             forward:    6.33 ms   36.60 ms
        loss_compute:    1.28 ms    1.45 ms
     loss_all_reduce:    0.14 ms    0.21 ms
       meters_update:   23.19 ms   23.21 ms
            backward:    1.54 ms    1.77 ms
      optimizer_step:    0.89 ms    0.78 ms
    train_step_total:  609.64 ms  609.66 ms
INFO 2022-05-16 07:20:36,462 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 41.026}, 'top_5': {'flatten': 69.616}}
INFO 2022-05-16 07:20:36,463 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:20:36,468 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:20:36,469 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 135, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:21:15,061 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:21:15,062 state_update_hooks.py: 115: Starting phase 135 [test]
INFO 2022-05-16 07:21:20,747 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:21:20,752 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 284
INFO 2022-05-16 07:21:20,753 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.5}, 'top_5': {'flatten': 60.01}}
INFO 2022-05-16 07:21:20,753 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:21:20,755 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:21:20,756 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 136, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:22:03,769 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:22:03,770 state_update_hooks.py: 115: Starting phase 136 [train]
INFO 2022-05-16 07:23:02,344 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:23:02,350 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 597
INFO 2022-05-16 07:23:02,351 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  530.94 ms  530.98 ms
             forward:    6.66 ms   38.00 ms
        loss_compute:    1.09 ms    1.33 ms
     loss_all_reduce:    0.16 ms    0.14 ms
       meters_update:   21.85 ms   21.89 ms
            backward:    1.44 ms    1.60 ms
      optimizer_step:    0.93 ms    1.01 ms
    train_step_total:  597.43 ms  597.47 ms
INFO 2022-05-16 07:23:02,352 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 41.028}, 'top_5': {'flatten': 69.646}}
INFO 2022-05-16 07:23:02,353 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:23:02,358 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:23:02,359 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 137, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:23:37,278 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:23:37,279 state_update_hooks.py: 115: Starting phase 137 [test]
INFO 2022-05-16 07:23:43,907 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:23:43,913 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 331
INFO 2022-05-16 07:23:43,913 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.57}, 'top_5': {'flatten': 60.36}}
INFO 2022-05-16 07:23:43,914 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:23:43,918 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:23:43,918 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 138, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:24:33,157 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:24:33,158 state_update_hooks.py: 115: Starting phase 138 [train]
INFO 2022-05-16 07:24:55,533 log_hooks.py: 277: Rank: 0; [ep: 69] iter: 6800; lr: 0.00657; loss: 2.37735; btime(ms): 1195; eta: 0:59:47; peak_mem(M): 2605;
INFO 2022-05-16 07:25:26,233 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:25:26,238 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 541
INFO 2022-05-16 07:25:26,238 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  477.13 ms  477.23 ms
             forward:    6.78 ms   36.73 ms
        loss_compute:    1.11 ms    1.35 ms
     loss_all_reduce:    0.15 ms    0.19 ms
       meters_update:   20.67 ms   20.69 ms
            backward:    1.62 ms    1.78 ms
      optimizer_step:    0.75 ms    0.78 ms
    train_step_total:  541.35 ms  541.38 ms
INFO 2022-05-16 07:25:26,239 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 41.184}, 'top_5': {'flatten': 69.658}}
INFO 2022-05-16 07:25:26,240 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:25:26,243 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:25:26,243 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 139, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:26:03,707 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:26:03,708 state_update_hooks.py: 115: Starting phase 139 [test]
INFO 2022-05-16 07:26:14,487 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:26:14,492 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 539
INFO 2022-05-16 07:26:14,493 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.46}, 'top_5': {'flatten': 60.29}}
INFO 2022-05-16 07:26:14,494 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:26:14,497 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:26:14,498 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 140, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:27:03,166 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:27:03,167 state_update_hooks.py: 115: Starting phase 140 [train]
INFO 2022-05-16 07:27:47,757 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:27:47,762 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 455
INFO 2022-05-16 07:27:47,762 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  398.52 ms  398.59 ms
             forward:    6.81 ms   34.36 ms
        loss_compute:    1.24 ms    1.31 ms
     loss_all_reduce:    0.16 ms    0.16 ms
       meters_update:   15.68 ms   15.69 ms
            backward:    1.68 ms    1.74 ms
      optimizer_step:    0.70 ms    0.74 ms
    train_step_total:  454.77 ms  454.79 ms
INFO 2022-05-16 07:27:47,763 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 41.174}, 'top_5': {'flatten': 69.77600000000001}}
INFO 2022-05-16 07:27:47,763 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:27:47,766 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:27:47,766 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 141, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:28:29,445 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:28:29,446 state_update_hooks.py: 115: Starting phase 141 [test]
INFO 2022-05-16 07:28:39,534 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:28:39,542 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 504
INFO 2022-05-16 07:28:39,544 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.64}, 'top_5': {'flatten': 60.589999999999996}}
INFO 2022-05-16 07:28:39,547 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:28:39,554 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:28:39,554 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 142, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:29:28,190 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:29:28,191 state_update_hooks.py: 115: Starting phase 142 [train]
INFO 2022-05-16 07:29:44,900 log_hooks.py: 277: Rank: 0; [ep: 71] iter: 7000; lr: 0.00581; loss: 2.36903; btime(ms): 1196; eta: 0:55:49; peak_mem(M): 2605;
INFO 2022-05-16 07:30:06,582 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:30:06,590 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 391
INFO 2022-05-16 07:30:06,591 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  340.64 ms  340.72 ms
             forward:    8.49 ms   33.41 ms
        loss_compute:    1.05 ms    1.07 ms
     loss_all_reduce:    0.13 ms    0.14 ms
       meters_update:   11.08 ms   11.09 ms
            backward:    1.96 ms    1.98 ms
      optimizer_step:    0.68 ms    0.70 ms
    train_step_total:  391.49 ms  391.50 ms
INFO 2022-05-16 07:30:06,592 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 41.227999999999994}, 'top_5': {'flatten': 69.816}}
INFO 2022-05-16 07:30:06,593 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:30:06,596 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:30:06,596 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 143, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:30:51,412 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:30:51,413 state_update_hooks.py: 115: Starting phase 143 [test]
INFO 2022-05-16 07:31:01,353 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:31:01,362 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 497
INFO 2022-05-16 07:31:01,364 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.36}, 'top_5': {'flatten': 60.4}}
INFO 2022-05-16 07:31:01,365 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:31:01,370 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:31:01,371 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 144, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:31:46,406 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:31:46,407 state_update_hooks.py: 115: Starting phase 144 [train]
INFO 2022-05-16 07:32:23,406 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:32:23,412 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 377
INFO 2022-05-16 07:32:23,413 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  328.32 ms  328.37 ms
             forward:    7.87 ms   32.76 ms
        loss_compute:    0.91 ms    0.91 ms
     loss_all_reduce:    0.13 ms    0.13 ms
       meters_update:   10.31 ms   10.32 ms
            backward:    1.92 ms    1.93 ms
      optimizer_step:    0.63 ms    0.63 ms
    train_step_total:  377.30 ms  377.30 ms
INFO 2022-05-16 07:32:23,414 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 41.246}, 'top_5': {'flatten': 69.768}}
INFO 2022-05-16 07:32:23,415 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:32:23,421 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:32:23,422 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 145, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:33:10,129 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:33:10,130 state_update_hooks.py: 115: Starting phase 145 [test]
INFO 2022-05-16 07:33:20,362 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:33:20,374 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 512
INFO 2022-05-16 07:33:20,375 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.45}, 'top_5': {'flatten': 60.5}}
INFO 2022-05-16 07:33:20,375 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:33:20,378 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:33:20,379 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 146, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:34:01,778 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:34:01,783 state_update_hooks.py: 115: Starting phase 146 [train]
INFO 2022-05-16 07:34:19,085 log_hooks.py: 277: Rank: 0; [ep: 73] iter: 7200; lr: 0.00508; loss: 2.3586; btime(ms): 1194; eta: 0:51:46; peak_mem(M): 2605;
INFO 2022-05-16 07:34:44,423 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:34:44,428 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 435
INFO 2022-05-16 07:34:44,429 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  380.04 ms  380.04 ms
             forward:    7.60 ms   34.70 ms
        loss_compute:    1.00 ms    1.07 ms
     loss_all_reduce:    0.12 ms    0.15 ms
       meters_update:   13.57 ms   13.59 ms
            backward:    1.75 ms    1.83 ms
      optimizer_step:    0.97 ms    0.99 ms
    train_step_total:  434.85 ms  434.86 ms
INFO 2022-05-16 07:34:44,430 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 41.342}, 'top_5': {'flatten': 69.894}}
INFO 2022-05-16 07:34:44,431 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:34:44,434 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:34:44,435 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 147, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:35:32,905 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:35:32,905 state_update_hooks.py: 115: Starting phase 147 [test]
INFO 2022-05-16 07:35:43,344 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:35:43,356 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 522
INFO 2022-05-16 07:35:43,357 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.76}, 'top_5': {'flatten': 60.58}}
INFO 2022-05-16 07:35:43,358 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:35:43,367 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:35:43,368 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 148, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:36:21,958 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:36:21,959 state_update_hooks.py: 115: Starting phase 148 [train]
INFO 2022-05-16 07:37:13,371 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:37:13,379 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 524
INFO 2022-05-16 07:37:13,380 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  461.07 ms  461.22 ms
             forward:    7.14 ms   37.25 ms
        loss_compute:    0.97 ms    1.17 ms
     loss_all_reduce:    0.13 ms    0.12 ms
       meters_update:   19.75 ms   19.78 ms
            backward:    1.61 ms    1.82 ms
      optimizer_step:    0.76 ms    0.71 ms
    train_step_total:  524.38 ms  524.39 ms
INFO 2022-05-16 07:37:13,381 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 41.396}, 'top_5': {'flatten': 70.036}}
INFO 2022-05-16 07:37:13,381 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:37:13,385 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:37:13,385 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 149, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:38:02,123 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:38:02,124 state_update_hooks.py: 115: Starting phase 149 [test]
INFO 2022-05-16 07:38:11,012 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:38:11,017 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 444
INFO 2022-05-16 07:38:11,018 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.66}, 'top_5': {'flatten': 60.589999999999996}}
INFO 2022-05-16 07:38:11,019 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:38:11,021 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:38:11,022 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 150, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:38:46,483 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:38:46,483 state_update_hooks.py: 115: Starting phase 150 [train]
INFO 2022-05-16 07:39:16,846 log_hooks.py: 277: Rank: 0; [ep: 75] iter: 7400; lr: 0.00439; loss: 2.2773; btime(ms): 1195; eta: 0:47:50; peak_mem(M): 2605;
INFO 2022-05-16 07:39:45,344 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:39:45,352 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 600
INFO 2022-05-16 07:39:45,353 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  531.53 ms  531.60 ms
             forward:    6.78 ms   37.55 ms
        loss_compute:    0.97 ms    1.15 ms
     loss_all_reduce:    0.13 ms    0.12 ms
       meters_update:   24.45 ms   24.48 ms
            backward:    1.56 ms    1.73 ms
      optimizer_step:    0.94 ms    0.91 ms
    train_step_total:  600.40 ms  600.35 ms
INFO 2022-05-16 07:39:45,353 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 41.402}, 'top_5': {'flatten': 70.094}}
INFO 2022-05-16 07:39:45,354 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:39:45,360 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:39:45,361 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 151, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:40:29,836 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:40:29,836 state_update_hooks.py: 115: Starting phase 151 [test]
INFO 2022-05-16 07:40:36,364 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:40:36,369 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 326
INFO 2022-05-16 07:40:36,370 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.62}, 'top_5': {'flatten': 60.519999999999996}}
INFO 2022-05-16 07:40:36,370 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:40:36,373 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:40:36,374 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 152, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:41:16,546 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:41:16,547 state_update_hooks.py: 115: Starting phase 152 [train]
INFO 2022-05-16 07:42:15,976 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:42:15,982 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 606
INFO 2022-05-16 07:42:15,983 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  538.56 ms  538.64 ms
             forward:    6.30 ms   37.88 ms
        loss_compute:    0.88 ms    1.14 ms
     loss_all_reduce:    0.12 ms    0.12 ms
       meters_update:   23.77 ms   23.78 ms
            backward:    1.33 ms    1.56 ms
      optimizer_step:    0.88 ms    0.87 ms
    train_step_total:  606.22 ms  606.25 ms
INFO 2022-05-16 07:42:15,984 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 41.592}, 'top_5': {'flatten': 70.04}}
INFO 2022-05-16 07:42:15,984 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:42:15,989 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:42:15,989 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 153, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:42:54,887 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:42:54,889 state_update_hooks.py: 115: Starting phase 153 [test]
INFO 2022-05-16 07:43:01,650 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:43:01,654 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 338
INFO 2022-05-16 07:43:01,655 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.85}, 'top_5': {'flatten': 60.58}}
INFO 2022-05-16 07:43:01,655 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:43:01,659 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:43:01,659 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 154, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:43:47,281 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:43:47,281 state_update_hooks.py: 115: Starting phase 154 [train]
INFO 2022-05-16 07:44:20,205 log_hooks.py: 277: Rank: 0; [ep: 77] iter: 7600; lr: 0.00375; loss: 2.27084; btime(ms): 1197; eta: 0:43:54; peak_mem(M): 2605;
INFO 2022-05-16 07:44:46,375 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:44:46,381 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 603
INFO 2022-05-16 07:44:46,381 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  535.17 ms  535.25 ms
             forward:    6.31 ms   36.85 ms
        loss_compute:    0.94 ms    1.11 ms
     loss_all_reduce:    0.13 ms    0.12 ms
       meters_update:   24.55 ms   24.57 ms
            backward:    1.46 ms    1.62 ms
      optimizer_step:    0.86 ms    0.85 ms
    train_step_total:  602.78 ms  602.79 ms
INFO 2022-05-16 07:44:46,382 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 41.54}, 'top_5': {'flatten': 69.932}}
INFO 2022-05-16 07:44:46,382 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:44:46,387 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:44:46,387 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 155, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:45:21,432 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:45:21,433 state_update_hooks.py: 115: Starting phase 155 [test]
INFO 2022-05-16 07:45:27,993 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:45:27,999 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 328
INFO 2022-05-16 07:45:28,000 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.910000000000004}, 'top_5': {'flatten': 60.57}}
INFO 2022-05-16 07:45:28,001 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:45:28,005 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:45:28,005 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 156, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:46:17,166 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:46:17,166 state_update_hooks.py: 115: Starting phase 156 [train]
INFO 2022-05-16 07:47:12,204 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:47:12,208 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 561
INFO 2022-05-16 07:47:12,209 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  498.32 ms  498.38 ms
             forward:    6.58 ms   36.53 ms
        loss_compute:    1.05 ms    1.21 ms
     loss_all_reduce:    0.17 ms    0.21 ms
       meters_update:   20.28 ms   20.30 ms
            backward:    1.51 ms    1.68 ms
      optimizer_step:    0.81 ms    0.84 ms
    train_step_total:  561.37 ms  561.37 ms
INFO 2022-05-16 07:47:12,210 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 41.632000000000005}, 'top_5': {'flatten': 70.174}}
INFO 2022-05-16 07:47:12,210 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:47:12,213 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:47:12,214 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 157, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:47:49,839 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:47:49,840 state_update_hooks.py: 115: Starting phase 157 [test]
INFO 2022-05-16 07:48:00,271 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:48:00,276 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 521
INFO 2022-05-16 07:48:00,279 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.68}, 'top_5': {'flatten': 60.81999999999999}}
INFO 2022-05-16 07:48:00,280 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:48:00,282 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:48:00,286 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 158, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:48:49,567 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:48:49,568 state_update_hooks.py: 115: Starting phase 158 [train]
INFO 2022-05-16 07:49:19,997 log_hooks.py: 277: Rank: 0; [ep: 79] iter: 7800; lr: 0.00315; loss: 2.24591; btime(ms): 1199; eta: 0:39:58; peak_mem(M): 2605;
INFO 2022-05-16 07:49:34,848 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:49:34,852 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 462
INFO 2022-05-16 07:49:34,853 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  403.02 ms  403.07 ms
             forward:    7.01 ms   35.44 ms
        loss_compute:    0.98 ms    1.11 ms
     loss_all_reduce:    0.13 ms    0.13 ms
       meters_update:   16.79 ms   16.85 ms
            backward:    1.98 ms    2.06 ms
      optimizer_step:    0.74 ms    0.78 ms
    train_step_total:  461.80 ms  461.82 ms
INFO 2022-05-16 07:49:34,854 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 41.833999999999996}, 'top_5': {'flatten': 70.128}}
INFO 2022-05-16 07:49:34,854 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:49:34,857 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:49:34,857 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 159, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:50:17,024 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:50:17,026 state_update_hooks.py: 115: Starting phase 159 [test]
INFO 2022-05-16 07:50:27,329 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:50:27,334 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 515
INFO 2022-05-16 07:50:27,335 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.769999999999996}, 'top_5': {'flatten': 60.58}}
INFO 2022-05-16 07:50:27,336 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:50:27,340 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:50:27,341 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 160, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:51:15,742 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:51:15,742 state_update_hooks.py: 115: Starting phase 160 [train]
INFO 2022-05-16 07:51:52,732 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:51:52,740 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 377
INFO 2022-05-16 07:51:52,741 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  324.44 ms  324.49 ms
             forward:    8.02 ms   33.68 ms
        loss_compute:    1.29 ms    1.28 ms
     loss_all_reduce:    0.17 ms    0.17 ms
       meters_update:   11.94 ms   11.96 ms
            backward:    2.09 ms    2.13 ms
      optimizer_step:    0.69 ms    0.73 ms
    train_step_total:  377.19 ms  377.20 ms
INFO 2022-05-16 07:51:52,742 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 41.638}, 'top_5': {'flatten': 70.274}}
INFO 2022-05-16 07:51:52,742 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:51:52,745 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:51:52,745 log_hooks.py: 425: [phase: 80] Saving checkpoint to ./checkpoints/LP_sgd/2/
INFO 2022-05-16 07:51:52,895 checkpoint.py: 131: Saved checkpoint: ./checkpoints/LP_sgd/2//model_phase80.torch
INFO 2022-05-16 07:51:52,895 checkpoint.py: 140: Creating symlink...
INFO 2022-05-16 07:51:52,897 checkpoint.py: 144: Created symlink: ./checkpoints/LP_sgd/2//checkpoint.torch
INFO 2022-05-16 07:51:52,898 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 161, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:52:36,314 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:52:36,315 state_update_hooks.py: 115: Starting phase 161 [test]
INFO 2022-05-16 07:52:46,523 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:52:46,529 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 510
INFO 2022-05-16 07:52:46,530 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.800000000000004}, 'top_5': {'flatten': 60.56}}
INFO 2022-05-16 07:52:46,531 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:52:46,534 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:52:46,534 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 162, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:53:32,403 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:53:32,404 state_update_hooks.py: 115: Starting phase 162 [train]
INFO 2022-05-16 07:53:56,021 log_hooks.py: 277: Rank: 0; [ep: 81] iter: 8000; lr: 0.00259; loss: 2.44661; btime(ms): 1197; eta: 0:35:56; peak_mem(M): 2605;
INFO 2022-05-16 07:54:09,852 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:54:09,857 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 382
INFO 2022-05-16 07:54:09,858 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  331.14 ms  331.18 ms
             forward:    8.52 ms   33.06 ms
        loss_compute:    1.02 ms    1.02 ms
     loss_all_reduce:    0.13 ms    0.14 ms
       meters_update:   10.82 ms   10.84 ms
            backward:    2.48 ms    2.50 ms
      optimizer_step:    0.70 ms    0.71 ms
    train_step_total:  381.87 ms  381.87 ms
INFO 2022-05-16 07:54:09,859 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 41.698}, 'top_5': {'flatten': 70.25200000000001}}
INFO 2022-05-16 07:54:09,861 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:54:09,865 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:54:09,865 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 163, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:54:57,160 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:54:57,161 state_update_hooks.py: 115: Starting phase 163 [test]
INFO 2022-05-16 07:55:07,554 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:55:07,560 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 519
INFO 2022-05-16 07:55:07,565 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.93}, 'top_5': {'flatten': 60.51}}
INFO 2022-05-16 07:55:07,566 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:55:07,568 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:55:07,569 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 164, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:55:48,735 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:55:48,736 state_update_hooks.py: 115: Starting phase 164 [train]
INFO 2022-05-16 07:56:30,362 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:56:30,368 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 424
INFO 2022-05-16 07:56:30,369 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  372.80 ms  372.83 ms
             forward:    7.59 ms   32.63 ms
        loss_compute:    0.87 ms    0.89 ms
     loss_all_reduce:    0.17 ms    0.19 ms
       meters_update:   13.27 ms   13.28 ms
            backward:    1.77 ms    1.79 ms
      optimizer_step:    0.72 ms    0.72 ms
    train_step_total:  424.52 ms  424.52 ms
INFO 2022-05-16 07:56:30,370 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 41.82}, 'top_5': {'flatten': 70.272}}
INFO 2022-05-16 07:56:30,371 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:56:30,376 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:56:30,376 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 165, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:57:18,728 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:57:18,728 state_update_hooks.py: 115: Starting phase 165 [test]
INFO 2022-05-16 07:57:28,688 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:57:28,693 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 498
INFO 2022-05-16 07:57:28,694 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.79}, 'top_5': {'flatten': 60.58}}
INFO 2022-05-16 07:57:28,695 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:57:28,697 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:57:28,698 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 166, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:58:05,572 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:58:05,573 state_update_hooks.py: 115: Starting phase 166 [train]
INFO 2022-05-16 07:58:34,931 log_hooks.py: 277: Rank: 0; [ep: 83] iter: 8200; lr: 0.00209; loss: 2.26536; btime(ms): 1196; eta: 0:31:55; peak_mem(M): 2605;
INFO 2022-05-16 07:58:53,523 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:58:53,528 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 489
INFO 2022-05-16 07:58:53,529 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  427.62 ms  427.68 ms
             forward:    7.05 ms   35.93 ms
        loss_compute:    1.01 ms    1.17 ms
     loss_all_reduce:    0.21 ms    0.22 ms
       meters_update:   18.61 ms   18.64 ms
            backward:    1.85 ms    2.01 ms
      optimizer_step:    0.83 ms    0.82 ms
    train_step_total:  489.06 ms  489.06 ms
INFO 2022-05-16 07:58:53,530 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 41.79}, 'top_5': {'flatten': 70.292}}
INFO 2022-05-16 07:58:53,531 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:58:53,534 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:58:53,534 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 167, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 07:59:42,104 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 07:59:42,105 state_update_hooks.py: 115: Starting phase 167 [test]
INFO 2022-05-16 07:59:52,404 trainer_main.py: 214: Meters synced
INFO 2022-05-16 07:59:52,413 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 515
INFO 2022-05-16 07:59:52,415 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.75}, 'top_5': {'flatten': 60.589999999999996}}
INFO 2022-05-16 07:59:52,416 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:59:52,421 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 07:59:52,422 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 168, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 08:00:27,480 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 08:00:27,481 state_update_hooks.py: 115: Starting phase 168 [train]
INFO 2022-05-16 08:01:25,916 trainer_main.py: 214: Meters synced
INFO 2022-05-16 08:01:25,923 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 596
INFO 2022-05-16 08:01:25,924 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  528.90 ms  528.92 ms
             forward:    6.93 ms   36.32 ms
        loss_compute:    1.28 ms    1.42 ms
     loss_all_reduce:    0.13 ms    0.15 ms
       meters_update:   23.15 ms   23.21 ms
            backward:    1.91 ms    2.08 ms
      optimizer_step:    1.02 ms    1.01 ms
    train_step_total:  596.05 ms  596.07 ms
INFO 2022-05-16 08:01:25,925 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 41.932}, 'top_5': {'flatten': 70.288}}
INFO 2022-05-16 08:01:25,926 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:01:25,929 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:01:25,930 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 169, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 08:02:12,047 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 08:02:12,048 state_update_hooks.py: 115: Starting phase 169 [test]
INFO 2022-05-16 08:02:18,249 trainer_main.py: 214: Meters synced
INFO 2022-05-16 08:02:18,254 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 310
INFO 2022-05-16 08:02:18,254 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.72}, 'top_5': {'flatten': 60.64000000000001}}
INFO 2022-05-16 08:02:18,255 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:02:18,258 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:02:18,259 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 170, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 08:02:56,544 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 08:02:56,545 state_update_hooks.py: 115: Starting phase 170 [train]
INFO 2022-05-16 08:03:39,405 log_hooks.py: 277: Rank: 0; [ep: 85] iter: 8400; lr: 0.00163; loss: 2.20472; btime(ms): 1198; eta: 0:27:58; peak_mem(M): 2605;
INFO 2022-05-16 08:03:55,496 trainer_main.py: 214: Meters synced
INFO 2022-05-16 08:03:55,508 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 601
INFO 2022-05-16 08:03:55,512 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  530.08 ms  530.15 ms
             forward:    6.42 ms   38.44 ms
        loss_compute:    1.33 ms    1.54 ms
     loss_all_reduce:    0.14 ms    0.15 ms
       meters_update:   25.37 ms   25.43 ms
            backward:    1.55 ms    1.80 ms
      optimizer_step:    0.90 ms    0.85 ms
    train_step_total:  601.32 ms  601.34 ms
INFO 2022-05-16 08:03:55,513 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 41.886}, 'top_5': {'flatten': 70.292}}
INFO 2022-05-16 08:03:55,514 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:03:55,519 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:03:55,520 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 171, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 08:04:35,912 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 08:04:35,913 state_update_hooks.py: 115: Starting phase 171 [test]
INFO 2022-05-16 08:04:42,150 trainer_main.py: 214: Meters synced
INFO 2022-05-16 08:04:42,155 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 312
INFO 2022-05-16 08:04:42,156 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.97}, 'top_5': {'flatten': 60.83}}
INFO 2022-05-16 08:04:42,156 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:04:42,159 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:04:42,159 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 172, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 08:05:25,230 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 08:05:25,230 state_update_hooks.py: 115: Starting phase 172 [train]
INFO 2022-05-16 08:06:25,318 trainer_main.py: 214: Meters synced
INFO 2022-05-16 08:06:25,328 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 613
INFO 2022-05-16 08:06:25,329 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  546.44 ms  546.46 ms
             forward:    6.48 ms   36.74 ms
        loss_compute:    1.03 ms    1.19 ms
     loss_all_reduce:    0.14 ms    0.12 ms
       meters_update:   23.50 ms   23.52 ms
            backward:    1.44 ms    1.56 ms
      optimizer_step:    0.89 ms    0.98 ms
    train_step_total:  612.93 ms  612.89 ms
INFO 2022-05-16 08:06:25,330 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 42.034}, 'top_5': {'flatten': 70.282}}
INFO 2022-05-16 08:06:25,330 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:06:25,336 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:06:25,337 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 173, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 08:07:02,344 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 08:07:02,347 state_update_hooks.py: 115: Starting phase 173 [test]
INFO 2022-05-16 08:07:08,645 trainer_main.py: 214: Meters synced
INFO 2022-05-16 08:07:08,651 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 315
INFO 2022-05-16 08:07:08,651 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.910000000000004}, 'top_5': {'flatten': 60.72}}
INFO 2022-05-16 08:07:08,652 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:07:08,655 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:07:08,655 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 174, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 08:07:57,441 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 08:07:57,442 state_update_hooks.py: 115: Starting phase 174 [train]
INFO 2022-05-16 08:08:43,121 log_hooks.py: 277: Rank: 0; [ep: 87] iter: 8600; lr: 0.00123; loss: 2.25465; btime(ms): 1200; eta: 0:24:00; peak_mem(M): 2605;
INFO 2022-05-16 08:08:53,629 trainer_main.py: 214: Meters synced
INFO 2022-05-16 08:08:53,634 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 573
INFO 2022-05-16 08:08:53,635 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  505.50 ms  505.60 ms
             forward:    6.37 ms   37.55 ms
        loss_compute:    1.09 ms    1.33 ms
     loss_all_reduce:    0.28 ms    0.26 ms
       meters_update:   22.63 ms   22.64 ms
            backward:    2.02 ms    2.16 ms
      optimizer_step:    0.84 ms    0.88 ms
    train_step_total:  573.11 ms  573.13 ms
INFO 2022-05-16 08:08:53,635 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 42.082}, 'top_5': {'flatten': 70.324}}
INFO 2022-05-16 08:08:53,636 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:08:53,638 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:08:53,639 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 175, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 08:09:31,869 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 08:09:31,869 state_update_hooks.py: 115: Starting phase 175 [test]
INFO 2022-05-16 08:09:42,626 trainer_main.py: 214: Meters synced
INFO 2022-05-16 08:09:42,633 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 538
INFO 2022-05-16 08:09:42,634 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.92}, 'top_5': {'flatten': 60.73}}
INFO 2022-05-16 08:09:42,635 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:09:42,640 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:09:42,640 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 176, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 08:10:31,898 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 08:10:31,898 state_update_hooks.py: 115: Starting phase 176 [train]
INFO 2022-05-16 08:11:19,198 trainer_main.py: 214: Meters synced
INFO 2022-05-16 08:11:19,202 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 482
INFO 2022-05-16 08:11:19,203 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  421.57 ms  421.65 ms
             forward:    7.32 ms   36.39 ms
        loss_compute:    1.12 ms    1.27 ms
     loss_all_reduce:    0.19 ms    0.18 ms
       meters_update:   17.64 ms   17.65 ms
            backward:    1.75 ms    1.90 ms
      optimizer_step:    0.72 ms    0.72 ms
    train_step_total:  482.42 ms  482.42 ms
INFO 2022-05-16 08:11:19,204 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 42.002}, 'top_5': {'flatten': 70.37}}
INFO 2022-05-16 08:11:19,204 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:11:19,208 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:11:19,208 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 177, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 08:11:59,936 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 08:11:59,937 state_update_hooks.py: 115: Starting phase 177 [test]
INFO 2022-05-16 08:12:10,009 trainer_main.py: 214: Meters synced
INFO 2022-05-16 08:12:10,015 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 503
INFO 2022-05-16 08:12:10,016 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 33.040000000000006}, 'top_5': {'flatten': 60.660000000000004}}
INFO 2022-05-16 08:12:10,017 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:12:10,020 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:12:10,024 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 178, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 08:12:58,567 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 08:12:58,568 state_update_hooks.py: 115: Starting phase 178 [train]
INFO 2022-05-16 08:13:31,230 log_hooks.py: 277: Rank: 0; [ep: 89] iter: 8800; lr: 0.00089; loss: 2.23305; btime(ms): 1200; eta: 0:20:00; peak_mem(M): 2605;
INFO 2022-05-16 08:13:38,359 trainer_main.py: 214: Meters synced
INFO 2022-05-16 08:13:38,363 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 406
INFO 2022-05-16 08:13:38,364 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  353.93 ms  354.00 ms
             forward:    7.30 ms   33.42 ms
        loss_compute:    0.91 ms    0.96 ms
     loss_all_reduce:    0.12 ms    0.12 ms
       meters_update:   11.36 ms   11.38 ms
            backward:    2.07 ms    2.13 ms
      optimizer_step:    0.68 ms    0.66 ms
    train_step_total:  405.78 ms  405.79 ms
INFO 2022-05-16 08:13:38,365 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 42.05}, 'top_5': {'flatten': 70.474}}
INFO 2022-05-16 08:13:38,365 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:13:38,368 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:13:38,368 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 179, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 08:14:24,953 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 08:14:24,954 state_update_hooks.py: 115: Starting phase 179 [test]
INFO 2022-05-16 08:14:35,694 trainer_main.py: 214: Meters synced
INFO 2022-05-16 08:14:35,703 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 537
INFO 2022-05-16 08:14:35,704 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.84}, 'top_5': {'flatten': 60.699999999999996}}
INFO 2022-05-16 08:14:35,705 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:14:35,711 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:14:35,712 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 180, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 08:15:22,232 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 08:15:22,233 state_update_hooks.py: 115: Starting phase 180 [train]
INFO 2022-05-16 08:16:00,666 trainer_main.py: 214: Meters synced
INFO 2022-05-16 08:16:00,674 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 392
INFO 2022-05-16 08:16:00,675 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  341.63 ms  341.68 ms
             forward:    8.23 ms   32.86 ms
        loss_compute:    0.95 ms    0.95 ms
     loss_all_reduce:    0.14 ms    0.15 ms
       meters_update:   10.84 ms   10.86 ms
            backward:    2.01 ms    2.03 ms
      optimizer_step:    1.00 ms    1.01 ms
    train_step_total:  391.90 ms  391.90 ms
INFO 2022-05-16 08:16:00,677 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 41.916}, 'top_5': {'flatten': 70.41199999999999}}
INFO 2022-05-16 08:16:00,680 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:16:00,686 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:16:00,687 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 181, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 08:16:49,743 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 08:16:49,744 state_update_hooks.py: 115: Starting phase 181 [test]
INFO 2022-05-16 08:17:00,412 trainer_main.py: 214: Meters synced
INFO 2022-05-16 08:17:00,419 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 533
INFO 2022-05-16 08:17:00,420 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.9}, 'top_5': {'flatten': 60.68}}
INFO 2022-05-16 08:17:00,420 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:17:00,431 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:17:00,432 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 182, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 08:17:42,458 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 08:17:42,459 state_update_hooks.py: 115: Starting phase 182 [train]
INFO 2022-05-16 08:18:18,167 log_hooks.py: 277: Rank: 0; [ep: 91] iter: 9000; lr: 0.0006; loss: 2.25962; btime(ms): 1200; eta: 0:16:00; peak_mem(M): 2605;
INFO 2022-05-16 08:18:27,673 trainer_main.py: 214: Meters synced
INFO 2022-05-16 08:18:27,683 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 461
INFO 2022-05-16 08:18:27,684 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  403.08 ms  403.16 ms
             forward:    7.78 ms   34.58 ms
        loss_compute:    1.27 ms    1.20 ms
     loss_all_reduce:    0.13 ms    0.12 ms
       meters_update:   15.26 ms   15.28 ms
            backward:    1.81 ms    1.89 ms
      optimizer_step:    0.72 ms    0.73 ms
    train_step_total:  461.12 ms  461.10 ms
INFO 2022-05-16 08:18:27,686 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 42.07}, 'top_5': {'flatten': 70.356}}
INFO 2022-05-16 08:18:27,690 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:18:27,696 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:18:27,697 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 183, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 08:19:15,322 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 08:19:15,323 state_update_hooks.py: 115: Starting phase 183 [test]
INFO 2022-05-16 08:19:25,190 trainer_main.py: 214: Meters synced
INFO 2022-05-16 08:19:25,197 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 493
INFO 2022-05-16 08:19:25,198 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 33.019999999999996}, 'top_5': {'flatten': 60.709999999999994}}
INFO 2022-05-16 08:19:25,198 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:19:25,201 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:19:25,201 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 184, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 08:20:01,188 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 08:20:01,190 state_update_hooks.py: 115: Starting phase 184 [train]
INFO 2022-05-16 08:20:52,471 trainer_main.py: 214: Meters synced
INFO 2022-05-16 08:20:52,476 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 523
INFO 2022-05-16 08:20:52,477 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  462.20 ms  462.24 ms
             forward:    6.40 ms   35.77 ms
        loss_compute:    0.95 ms    1.11 ms
     loss_all_reduce:    0.13 ms    0.12 ms
       meters_update:   18.59 ms   18.63 ms
            backward:    1.57 ms    1.69 ms
      optimizer_step:    0.83 ms    0.75 ms
    train_step_total:  523.03 ms  523.06 ms
INFO 2022-05-16 08:20:52,478 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 42.04}, 'top_5': {'flatten': 70.41199999999999}}
INFO 2022-05-16 08:20:52,479 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:20:52,489 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:20:52,489 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 185, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 08:21:39,192 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 08:21:39,193 state_update_hooks.py: 115: Starting phase 185 [test]
INFO 2022-05-16 08:21:45,391 trainer_main.py: 214: Meters synced
INFO 2022-05-16 08:21:45,395 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 310
INFO 2022-05-16 08:21:45,396 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.93}, 'top_5': {'flatten': 60.72}}
INFO 2022-05-16 08:21:45,396 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:21:45,399 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:21:45,400 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 186, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 08:22:22,245 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 08:22:22,246 state_update_hooks.py: 115: Starting phase 186 [train]
INFO 2022-05-16 08:23:12,989 log_hooks.py: 277: Rank: 0; [ep: 93] iter: 9200; lr: 0.00036; loss: 2.23449; btime(ms): 1200; eta: 0:12:00; peak_mem(M): 2605;
INFO 2022-05-16 08:23:19,389 trainer_main.py: 214: Meters synced
INFO 2022-05-16 08:23:19,395 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 583
INFO 2022-05-16 08:23:19,397 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  517.58 ms  517.72 ms
             forward:    6.04 ms   37.07 ms
        loss_compute:    0.95 ms    1.13 ms
     loss_all_reduce:    0.13 ms    0.11 ms
       meters_update:   21.97 ms   21.99 ms
            backward:    1.25 ms    1.40 ms
      optimizer_step:    0.88 ms    0.94 ms
    train_step_total:  582.88 ms  582.91 ms
INFO 2022-05-16 08:23:19,398 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 42.064}, 'top_5': {'flatten': 70.474}}
INFO 2022-05-16 08:23:19,398 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:23:19,401 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:23:19,402 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 187, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 08:23:59,719 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 08:23:59,720 state_update_hooks.py: 115: Starting phase 187 [test]
INFO 2022-05-16 08:24:05,427 trainer_main.py: 214: Meters synced
INFO 2022-05-16 08:24:05,432 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 285
INFO 2022-05-16 08:24:05,432 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.82}, 'top_5': {'flatten': 60.83}}
INFO 2022-05-16 08:24:05,433 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:24:05,435 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:24:05,436 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 188, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 08:24:46,120 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 08:24:46,121 state_update_hooks.py: 115: Starting phase 188 [train]
INFO 2022-05-16 08:25:42,454 trainer_main.py: 214: Meters synced
INFO 2022-05-16 08:25:42,462 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 574
INFO 2022-05-16 08:25:42,463 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  508.47 ms  508.51 ms
             forward:    5.90 ms   37.66 ms
        loss_compute:    1.26 ms    1.52 ms
     loss_all_reduce:    0.13 ms    0.13 ms
       meters_update:   21.28 ms   21.30 ms
            backward:    1.76 ms    2.01 ms
      optimizer_step:    0.78 ms    0.71 ms
    train_step_total:  574.62 ms  574.60 ms
INFO 2022-05-16 08:25:42,467 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 42.120000000000005}, 'top_5': {'flatten': 70.52000000000001}}
INFO 2022-05-16 08:25:42,470 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:25:42,476 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:25:42,476 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 189, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 08:26:20,684 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 08:26:20,685 state_update_hooks.py: 115: Starting phase 189 [test]
INFO 2022-05-16 08:26:26,611 trainer_main.py: 214: Meters synced
INFO 2022-05-16 08:26:26,619 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 296
INFO 2022-05-16 08:26:26,620 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.910000000000004}, 'top_5': {'flatten': 60.69}}
INFO 2022-05-16 08:26:26,620 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:26:26,623 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:26:26,623 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 190, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 08:27:12,114 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 08:27:12,115 state_update_hooks.py: 115: Starting phase 190 [train]
INFO 2022-05-16 08:28:04,560 log_hooks.py: 277: Rank: 0; [ep: 95] iter: 9400; lr: 0.00018; loss: 2.32312; btime(ms): 1201; eta: 0:08:00; peak_mem(M): 2605;
INFO 2022-05-16 08:28:07,506 trainer_main.py: 214: Meters synced
INFO 2022-05-16 08:28:07,510 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 565
INFO 2022-05-16 08:28:07,510 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  497.51 ms  497.56 ms
             forward:    5.93 ms   39.32 ms
        loss_compute:    1.30 ms    1.59 ms
     loss_all_reduce:    0.17 ms    0.17 ms
       meters_update:   20.94 ms   20.97 ms
            backward:    1.45 ms    1.59 ms
      optimizer_step:    0.80 ms    0.84 ms
    train_step_total:  565.01 ms  564.97 ms
INFO 2022-05-16 08:28:07,511 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 42.126000000000005}, 'top_5': {'flatten': 70.464}}
INFO 2022-05-16 08:28:07,512 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:28:07,514 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:28:07,515 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 191, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 08:28:42,227 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 08:28:42,228 state_update_hooks.py: 115: Starting phase 191 [test]
INFO 2022-05-16 08:28:51,712 trainer_main.py: 214: Meters synced
INFO 2022-05-16 08:28:51,718 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 474
INFO 2022-05-16 08:28:51,719 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.95}, 'top_5': {'flatten': 60.760000000000005}}
INFO 2022-05-16 08:28:51,720 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:28:51,723 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:28:51,724 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 192, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 08:29:39,979 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 08:29:39,979 state_update_hooks.py: 115: Starting phase 192 [train]
INFO 2022-05-16 08:30:26,319 trainer_main.py: 214: Meters synced
INFO 2022-05-16 08:30:26,323 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 472
INFO 2022-05-16 08:30:26,324 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  414.17 ms  414.20 ms
             forward:    7.04 ms   36.93 ms
        loss_compute:    1.08 ms    1.21 ms
     loss_all_reduce:    0.13 ms    0.11 ms
       meters_update:   14.95 ms   14.98 ms
            backward:    1.96 ms    2.10 ms
      optimizer_step:    0.71 ms    0.70 ms
    train_step_total:  472.59 ms  472.58 ms
INFO 2022-05-16 08:30:26,325 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 42.026}, 'top_5': {'flatten': 70.64}}
INFO 2022-05-16 08:30:26,325 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:30:26,328 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:30:26,329 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 193, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 08:31:05,354 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 08:31:05,355 state_update_hooks.py: 115: Starting phase 193 [test]
INFO 2022-05-16 08:31:14,668 trainer_main.py: 214: Meters synced
INFO 2022-05-16 08:31:14,675 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 465
INFO 2022-05-16 08:31:14,676 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.96}, 'top_5': {'flatten': 60.67}}
INFO 2022-05-16 08:31:14,676 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:31:14,685 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:31:14,686 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 194, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 08:32:02,081 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 08:32:02,082 state_update_hooks.py: 115: Starting phase 194 [train]
INFO 2022-05-16 08:32:40,416 log_hooks.py: 277: Rank: 0; [ep: 97] iter: 9600; lr: 7e-05; loss: 2.26173; btime(ms): 1199; eta: 0:03:59; peak_mem(M): 2605;
INFO 2022-05-16 08:32:41,687 trainer_main.py: 214: Meters synced
INFO 2022-05-16 08:32:41,692 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 404
INFO 2022-05-16 08:32:41,693 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  353.83 ms  353.91 ms
             forward:    7.22 ms   33.17 ms
        loss_compute:    1.07 ms    1.10 ms
     loss_all_reduce:    0.12 ms    0.12 ms
       meters_update:   11.02 ms   11.04 ms
            backward:    1.74 ms    1.78 ms
      optimizer_step:    0.66 ms    0.68 ms
    train_step_total:  403.88 ms  403.90 ms
INFO 2022-05-16 08:32:41,693 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 42.232}, 'top_5': {'flatten': 70.572}}
INFO 2022-05-16 08:32:41,694 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:32:41,697 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:32:41,697 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 195, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 08:33:25,750 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 08:33:25,751 state_update_hooks.py: 115: Starting phase 195 [test]
INFO 2022-05-16 08:33:35,539 trainer_main.py: 214: Meters synced
INFO 2022-05-16 08:33:35,547 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 489
INFO 2022-05-16 08:33:35,548 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.85}, 'top_5': {'flatten': 60.709999999999994}}
INFO 2022-05-16 08:33:35,549 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:33:35,551 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:33:35,552 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 196, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 08:34:20,340 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 08:34:20,341 state_update_hooks.py: 115: Starting phase 196 [train]
INFO 2022-05-16 08:34:54,514 trainer_main.py: 214: Meters synced
INFO 2022-05-16 08:34:54,519 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 348
INFO 2022-05-16 08:34:54,520 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  299.35 ms  299.40 ms
             forward:    7.75 ms   32.78 ms
        loss_compute:    1.00 ms    0.99 ms
     loss_all_reduce:    0.13 ms    0.14 ms
       meters_update:   10.17 ms   10.19 ms
            backward:    1.91 ms    1.91 ms
      optimizer_step:    0.66 ms    0.67 ms
    train_step_total:  348.45 ms  348.46 ms
INFO 2022-05-16 08:34:54,521 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 42.178}, 'top_5': {'flatten': 70.44}}
INFO 2022-05-16 08:34:54,521 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:34:54,524 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:34:54,524 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 197, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 08:35:40,565 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 08:35:40,566 state_update_hooks.py: 115: Starting phase 197 [test]
INFO 2022-05-16 08:35:50,512 trainer_main.py: 214: Meters synced
INFO 2022-05-16 08:35:50,517 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 497
INFO 2022-05-16 08:35:50,518 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.910000000000004}, 'top_5': {'flatten': 60.84}}
INFO 2022-05-16 08:35:50,519 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:35:50,521 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:35:50,522 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 198, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 08:36:30,646 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 08:36:30,648 state_update_hooks.py: 115: Starting phase 198 [train]
INFO 2022-05-16 08:37:10,348 trainer_main.py: 214: Meters synced
INFO 2022-05-16 08:37:10,354 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 405
INFO 2022-05-16 08:37:10,355 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  352.19 ms  352.21 ms
             forward:    7.25 ms   33.91 ms
        loss_compute:    0.89 ms    0.93 ms
     loss_all_reduce:    0.15 ms    0.13 ms
       meters_update:   12.84 ms   12.87 ms
            backward:    1.69 ms    1.77 ms
      optimizer_step:    0.68 ms    0.70 ms
    train_step_total:  404.88 ms  404.89 ms
INFO 2022-05-16 08:37:10,356 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'flatten': 42.178}, 'top_5': {'flatten': 70.506}}
INFO 2022-05-16 08:37:10,356 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:37:10,359 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:37:10,360 log_hooks.py: 425: [phase: 99] Saving checkpoint to ./checkpoints/LP_sgd/2/
INFO 2022-05-16 08:37:10,553 checkpoint.py: 131: Saved checkpoint: ./checkpoints/LP_sgd/2//model_final_checkpoint_phase99.torch
INFO 2022-05-16 08:37:10,553 checkpoint.py: 140: Creating symlink...
INFO 2022-05-16 08:37:10,556 checkpoint.py: 144: Created symlink: ./checkpoints/LP_sgd/2//checkpoint.torch
INFO 2022-05-16 08:37:10,557 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 199, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-16 08:37:58,240 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-16 08:37:58,241 state_update_hooks.py: 115: Starting phase 199 [test]
INFO 2022-05-16 08:38:07,973 trainer_main.py: 214: Meters synced
INFO 2022-05-16 08:38:07,978 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 486
INFO 2022-05-16 08:38:07,979 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'flatten': 32.93}, 'top_5': {'flatten': 60.79}}
INFO 2022-05-16 08:38:07,982 io.py:  63: Saving data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:38:07,987 io.py:  89: Saved data to file: ./checkpoints/LP_sgd/2//metrics.json
INFO 2022-05-16 08:38:08,083 train.py: 131: All Done!
INFO 2022-05-16 08:38:08,085 logger.py:  73: Shutting down loggers...
INFO 2022-05-16 08:38:08,091 distributed_launcher.py: 168: All Done!
INFO 2022-05-16 08:38:08,091 logger.py:  73: Shutting down loggers...
