INFO 2022-05-15 13:29:27,245 train.py:  94: Env set for rank: 0, dist_rank: 0
INFO 2022-05-15 13:29:27,247 env.py:  50: ARCH:	x86_64
INFO 2022-05-15 13:29:27,248 env.py:  50: BASH_ENV:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/lmod/lmod/init/bash
INFO 2022-05-15 13:29:27,249 env.py:  50: BASH_FUNC_ml%%:	() {  eval $($LMOD_DIR/ml_cmd "$@")
}
INFO 2022-05-15 13:29:27,250 env.py:  50: BASH_FUNC_module%%:	() {  eval $($LMOD_CMD bash "$@") && eval $(${LMOD_SETTARG_CMD:-:} -s sh)
}
INFO 2022-05-15 13:29:27,251 env.py:  50: CMAKE_LIBRARY_PATH:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/gcc/7.4.0/lib64
INFO 2022-05-15 13:29:27,252 env.py:  50: CMAKE_PREFIX_PATH:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/gcc/7.4.0
INFO 2022-05-15 13:29:27,253 env.py:  50: COLUMNS:	202
INFO 2022-05-15 13:29:27,253 env.py:  50: CONDA_ACTIVATE:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/etc/profile.d/conda.sh
INFO 2022-05-15 13:29:27,254 env.py:  50: CONDA_DEFAULT_ENV:	vissl_env
INFO 2022-05-15 13:29:27,255 env.py:  50: CONDA_EXE:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/bin/conda
INFO 2022-05-15 13:29:27,255 env.py:  50: CONDA_PREFIX:	/home/mila/r/rajkuman/.conda/envs/vissl_env
INFO 2022-05-15 13:29:27,256 env.py:  50: CONDA_PROMPT_MODIFIER:	(vissl_env) 
INFO 2022-05-15 13:29:27,257 env.py:  50: CONDA_PYTHON_EXE:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/bin/python
INFO 2022-05-15 13:29:27,258 env.py:  50: CONDA_SHLVL:	1
INFO 2022-05-15 13:29:27,259 env.py:  50: CPATH:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/include
INFO 2022-05-15 13:29:27,260 env.py:  50: CSPYTHONPREFIXES:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3
INFO 2022-05-15 13:29:27,260 env.py:  50: CUDA_VISIBLE_DEVICES:	0
INFO 2022-05-15 13:29:27,261 env.py:  50: DBUS_SESSION_BUS_ADDRESS:	unix:path=/run/user/1471600619/bus
INFO 2022-05-15 13:29:27,262 env.py:  50: ENVIRONMENT:	BATCH
INFO 2022-05-15 13:29:27,263 env.py:  50: GPU_DEVICE_ORDINAL:	0
INFO 2022-05-15 13:29:27,264 env.py:  50: HOME:	/home/mila/r/rajkuman
INFO 2022-05-15 13:29:27,264 env.py:  50: HOSTNAME:	mila01
INFO 2022-05-15 13:29:27,265 env.py:  50: ID:	debian
INFO 2022-05-15 13:29:27,266 env.py:  50: JPY_API_TOKEN:	e960c7b7a4ec499483f421b612c72a5e
INFO 2022-05-15 13:29:27,267 env.py:  50: JUPYTERHUB_ACTIVITY_URL:	http://172.16.2.123:8081/hub/api/users/rajkuman/activity
INFO 2022-05-15 13:29:27,268 env.py:  50: JUPYTERHUB_API_TOKEN:	e960c7b7a4ec499483f421b612c72a5e
INFO 2022-05-15 13:29:27,269 env.py:  50: JUPYTERHUB_API_URL:	http://172.16.2.123:8081/hub/api
INFO 2022-05-15 13:29:27,269 env.py:  50: JUPYTERHUB_BASE_URL:	/
INFO 2022-05-15 13:29:27,270 env.py:  50: JUPYTERHUB_CLIENT_ID:	jupyterhub-user-rajkuman
INFO 2022-05-15 13:29:27,271 env.py:  50: JUPYTERHUB_HOST:	
INFO 2022-05-15 13:29:27,272 env.py:  50: JUPYTERHUB_OAUTH_CALLBACK_URL:	/user/rajkuman/oauth_callback
INFO 2022-05-15 13:29:27,273 env.py:  50: JUPYTERHUB_SERVER_NAME:	
INFO 2022-05-15 13:29:27,274 env.py:  50: JUPYTERHUB_SERVICE_PREFIX:	/user/rajkuman/
INFO 2022-05-15 13:29:27,275 env.py:  50: JUPYTERHUB_USER:	rajkuman
INFO 2022-05-15 13:29:27,275 env.py:  50: JUPYTER_SERVER_ROOT:	/home/mila/r/rajkuman
INFO 2022-05-15 13:29:27,276 env.py:  50: JUPYTER_SERVER_URL:	http://0.0.0.0:46141/user/rajkuman/
INFO 2022-05-15 13:29:27,277 env.py:  50: KERNEL_LAUNCH_TIMEOUT:	40
INFO 2022-05-15 13:29:27,278 env.py:  50: LANG:	en_US.UTF-8
INFO 2022-05-15 13:29:27,278 env.py:  50: LESSCLOSE:	/bin/lesspipe %s %s
INFO 2022-05-15 13:29:27,279 env.py:  50: LESSOPEN:	| /bin/lesspipe %s
INFO 2022-05-15 13:29:27,282 env.py:  50: LIBRARY_PATH:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib:/cvmfs/ai.mila.quebec/apps/x86_64/debian/gcc/7.4.0/lib64:/cvmfs/ai.mila.quebec/apps/x86_64/debian/gcc/7.4.0/lib
INFO 2022-05-15 13:29:27,283 env.py:  50: LINES:	50
INFO 2022-05-15 13:29:27,284 env.py:  50: LMOD_CMD:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/lmod/lmod/libexec/lmod
INFO 2022-05-15 13:29:27,285 env.py:  50: LMOD_DIR:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/lmod/lmod/libexec
INFO 2022-05-15 13:29:27,285 env.py:  50: LMOD_PACKAGE_PATH:	/cvmfs/config.mila.quebec/etc/lmod/
INFO 2022-05-15 13:29:27,286 env.py:  50: LMOD_PKG:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/lmod/lmod
INFO 2022-05-15 13:29:27,287 env.py:  50: LMOD_ROOT:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/lmod
INFO 2022-05-15 13:29:27,288 env.py:  50: LMOD_SETTARG_FULL_SUPPORT:	no
INFO 2022-05-15 13:29:27,289 env.py:  50: LMOD_SYSTEM_DEFAULT_MODULES:	Mila:gcc/7.4.0
INFO 2022-05-15 13:29:27,290 env.py:  50: LMOD_VERSION:	8.3.17
INFO 2022-05-15 13:29:27,291 env.py:  50: LMOD_sys:	Linux
INFO 2022-05-15 13:29:27,292 env.py:  50: LOADEDMODULES:	Mila:gcc/7.4.0:anaconda/3
INFO 2022-05-15 13:29:27,293 env.py:  50: LOCAL_RANK:	0
INFO 2022-05-15 13:29:27,293 env.py:  50: LOGNAME:	rajkuman
INFO 2022-05-15 13:29:27,294 env.py:  50: LS_COLORS:	rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:
INFO 2022-05-15 13:29:27,295 env.py:  50: MAIL:	/var/mail/rajkuman
INFO 2022-05-15 13:29:27,296 env.py:  50: MANPATH:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/share/man:/cvmfs/ai.mila.quebec/apps/x86_64/debian/gcc/7.4.0/share/man:/cvmfs/ai.mila.quebec/apps/x86_64/debian/lmod/lmod/share/man::
INFO 2022-05-15 13:29:27,297 env.py:  50: MODULEPATH:	/cvmfs/config.mila.quebec/modules/Core:/cvmfs/config.mila.quebec/modules/Compiler:/cvmfs/config.mila.quebec/modules/Environments:/cvmfs/config.mila.quebec/modules/Cuda:/cvmfs/config.mila.quebec/modules/Pytorch:/cvmfs/config.mila.quebec/modules/Tensorflow
INFO 2022-05-15 13:29:27,297 env.py:  50: MODULEPATH_ROOT:	/cvmfs/config.mila.quebec/modules
INFO 2022-05-15 13:29:27,298 env.py:  50: MODULERCFILE:	/cvmfs/config.mila.quebec/etc/lmod/modulerc.lua
INFO 2022-05-15 13:29:27,299 env.py:  50: MODULESHOME:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/lmod/lmod
INFO 2022-05-15 13:29:27,300 env.py:  50: OLDPWD:	/home/mila/r/rajkuman
INFO 2022-05-15 13:29:27,301 env.py:  50: PATH:	/home/mila/r/rajkuman/.conda/envs/vissl_env/bin:/home/mila/r/rajkuman/.local/bin:/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/condabin:/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/bin:/opt/slurm/bin:/sbin:/bin:/usr/sbin:/usr/bin
INFO 2022-05-15 13:29:27,302 env.py:  50: PKG_CONFIG_PATH:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/pkgconfig
INFO 2022-05-15 13:29:27,303 env.py:  50: PROCESSOR_ARCHITECTURE:	amd64
INFO 2022-05-15 13:29:27,303 env.py:  50: PWD:	/home/mila/r/rajkuman/mina/mphil-vissl
INFO 2022-05-15 13:29:27,304 env.py:  50: PYTHONNOUSERSITE:	True
INFO 2022-05-15 13:29:27,305 env.py:  50: PYTHONPATH:	/cvmfs/config.mila.quebec/etc/python.d/3.7
INFO 2022-05-15 13:29:27,306 env.py:  50: PYXTERM_DIMENSIONS:	80x25
INFO 2022-05-15 13:29:27,307 env.py:  50: RANK:	0
INFO 2022-05-15 13:29:27,308 env.py:  50: ROCR_VISIBLE_DEVICES:	0
INFO 2022-05-15 13:29:27,308 env.py:  50: SACCT_FORMAT:	User,JobID,Jobname,partition,state,time,start,end,elapsed,nnodes,ncpus,reqmem,alloctres,nodelist,workdir
INFO 2022-05-15 13:29:27,309 env.py:  50: SCRATCH:	/network/scratch/r/rajkuman
INFO 2022-05-15 13:29:27,310 env.py:  50: SHELL:	/bin/bash
INFO 2022-05-15 13:29:27,311 env.py:  50: SHLVL:	3
INFO 2022-05-15 13:29:27,312 env.py:  50: SINFO_FORMAT:	%18N %.6D %.11T %.4c %.8z %.6m %.8d %.6w %.22f %80E
INFO 2022-05-15 13:29:27,313 env.py:  50: SLURMD_NODENAME:	mila01
INFO 2022-05-15 13:29:27,314 env.py:  50: SLURM_CLUSTER_NAME:	mila
INFO 2022-05-15 13:29:27,314 env.py:  50: SLURM_CONF:	/etc/slurm/slurm.conf
INFO 2022-05-15 13:29:27,315 env.py:  50: SLURM_CPUS_ON_NODE:	4
INFO 2022-05-15 13:29:27,316 env.py:  50: SLURM_CPUS_PER_TASK:	4
INFO 2022-05-15 13:29:27,317 env.py:  50: SLURM_EXPORT_ENV:	PATH,LANG,USER,HOME,SHELL,JUPYTERHUB_API_TOKEN,JPY_API_TOKEN,JUPYTERHUB_CLIENT_ID,JUPYTERHUB_HOST,JUPYTERHUB_OAUTH_CALLBACK_URL,JUPYTERHUB_USER,JUPYTERHUB_SERVER_NAME,JUPYTERHUB_API_URL,JUPYTERHUB_ACTIVITY_URL,JUPYTERHUB_BASE_URL,JUPYTERHUB_SERVICE_PREFIX
INFO 2022-05-15 13:29:27,318 env.py:  50: SLURM_GET_USER_ENV:	1
INFO 2022-05-15 13:29:27,318 env.py:  50: SLURM_GTIDS:	0
INFO 2022-05-15 13:29:27,319 env.py:  50: SLURM_JOBID:	1827791
INFO 2022-05-15 13:29:27,320 env.py:  50: SLURM_JOB_ACCOUNT:	mila
INFO 2022-05-15 13:29:27,321 env.py:  50: SLURM_JOB_CPUS_PER_NODE:	4
INFO 2022-05-15 13:29:27,322 env.py:  50: SLURM_JOB_GID:	1471600619
INFO 2022-05-15 13:29:27,323 env.py:  50: SLURM_JOB_GPUS:	3
INFO 2022-05-15 13:29:27,324 env.py:  50: SLURM_JOB_ID:	1827791
INFO 2022-05-15 13:29:27,324 env.py:  50: SLURM_JOB_NAME:	jupyterhub-rajkuman
INFO 2022-05-15 13:29:27,325 env.py:  50: SLURM_JOB_NODELIST:	mila01
INFO 2022-05-15 13:29:27,326 env.py:  50: SLURM_JOB_NUM_NODES:	1
INFO 2022-05-15 13:29:27,327 env.py:  50: SLURM_JOB_PARTITION:	unkillable
INFO 2022-05-15 13:29:27,328 env.py:  50: SLURM_JOB_QOS:	normal
INFO 2022-05-15 13:29:27,329 env.py:  50: SLURM_JOB_UID:	1471600619
INFO 2022-05-15 13:29:27,330 env.py:  50: SLURM_JOB_USER:	rajkuman
INFO 2022-05-15 13:29:27,331 env.py:  50: SLURM_LOCALID:	0
INFO 2022-05-15 13:29:27,331 env.py:  50: SLURM_MEM_PER_NODE:	24000
INFO 2022-05-15 13:29:27,332 env.py:  50: SLURM_NNODES:	1
INFO 2022-05-15 13:29:27,333 env.py:  50: SLURM_NODEID:	0
INFO 2022-05-15 13:29:27,334 env.py:  50: SLURM_NODELIST:	mila01
INFO 2022-05-15 13:29:27,335 env.py:  50: SLURM_NODE_ALIASES:	(null)
INFO 2022-05-15 13:29:27,335 env.py:  50: SLURM_NPROCS:	1
INFO 2022-05-15 13:29:27,336 env.py:  50: SLURM_NTASKS:	1
INFO 2022-05-15 13:29:27,337 env.py:  50: SLURM_PRIO_PROCESS:	0
INFO 2022-05-15 13:29:27,337 env.py:  50: SLURM_PROCID:	0
INFO 2022-05-15 13:29:27,338 env.py:  50: SLURM_SUBMIT_DIR:	/var/lib/jupyterhub
INFO 2022-05-15 13:29:27,339 env.py:  50: SLURM_SUBMIT_HOST:	jupyter
INFO 2022-05-15 13:29:27,340 env.py:  50: SLURM_TASKS_PER_NODE:	1
INFO 2022-05-15 13:29:27,340 env.py:  50: SLURM_TASK_PID:	33829
INFO 2022-05-15 13:29:27,341 env.py:  50: SLURM_TMPDIR:	/Tmp/slurm.1827791.0
INFO 2022-05-15 13:29:27,342 env.py:  50: SLURM_TOPOLOGY_ADDR:	mila01
INFO 2022-05-15 13:29:27,343 env.py:  50: SLURM_TOPOLOGY_ADDR_PATTERN:	node
INFO 2022-05-15 13:29:27,344 env.py:  50: SLURM_WORKING_CLUSTER:	mila:slurm:6817:9216:109
INFO 2022-05-15 13:29:27,344 env.py:  50: SQUEUE_FORMAT:	%.8i %.8u %.12P %.14j %.3t %16S %.10M %.5D %.4C %.10b %.7m %N (%r) %k
INFO 2022-05-15 13:29:27,345 env.py:  50: S_COLORS:	auto
INFO 2022-05-15 13:29:27,346 env.py:  50: TERM:	xterm
INFO 2022-05-15 13:29:27,347 env.py:  50: TMPDIR:	/tmp
INFO 2022-05-15 13:29:27,347 env.py:  50: USER:	rajkuman
INFO 2022-05-15 13:29:27,348 env.py:  50: WORLD_SIZE:	1
INFO 2022-05-15 13:29:27,349 env.py:  50: XDG_SESSION_ID:	c181
INFO 2022-05-15 13:29:27,350 env.py:  50: _:	/home/mila/r/rajkuman/.conda/envs/vissl_env/bin/python
INFO 2022-05-15 13:29:27,351 env.py:  50: _CE_CONDA:	
INFO 2022-05-15 13:29:27,351 env.py:  50: _CE_M:	
INFO 2022-05-15 13:29:27,352 env.py:  50: _LMFILES_:	/cvmfs/config.mila.quebec/modules/Core/Mila.lua:/cvmfs/config.mila.quebec/modules/Core/gcc/7.4.0.lua:/cvmfs/config.mila.quebec/modules/Core/anaconda/3.lua
INFO 2022-05-15 13:29:27,353 env.py:  50: _ModuleTable001_:	X01vZHVsZVRhYmxlXz17WyJNVHZlcnNpb24iXT0zLFsiY19yZWJ1aWxkVGltZSJdPWZhbHNlLFsiY19zaG9ydFRpbWUiXT1mYWxzZSxkZXB0aFQ9e30sZmFtaWx5PXt9LG1UPXtNaWxhPXtbImZuIl09Ii9jdm1mcy9jb25maWcubWlsYS5xdWViZWMvbW9kdWxlcy9Db3JlL01pbGEubHVhIixbImZ1bGxOYW1lIl09Ik1pbGEiLFsibG9hZE9yZGVyIl09MSxwcm9wVD17bG1vZD17WyJzdGlja3kiXT0xLH0sfSxbInN0YWNrRGVwdGgiXT0wLFsic3RhdHVzIl09ImFjdGl2ZSIsWyJ1c2VyTmFtZSJdPSJNaWxhIix9LGFuYWNvbmRhPXtbImZuIl09Ii9jdm1mcy9jb25maWcubWlsYS5xdWViZWMvbW9kdWxlcy9Db3JlL2FuYWNvbmRhLzMubHVhIixbImZ1bGxOYW1lIl09ImFuYWNvbmRh
INFO 2022-05-15 13:29:27,354 env.py:  50: _ModuleTable002_:	LzMiLFsibG9hZE9yZGVyIl09Myxwcm9wVD17fSxbInN0YWNrRGVwdGgiXT0wLFsic3RhdHVzIl09ImFjdGl2ZSIsWyJ1c2VyTmFtZSJdPSJhbmFjb25kYS8zIix9LGdjYz17WyJmbiJdPSIvY3ZtZnMvY29uZmlnLm1pbGEucXVlYmVjL21vZHVsZXMvQ29yZS9nY2MvNy40LjAubHVhIixbImZ1bGxOYW1lIl09ImdjYy83LjQuMCIsWyJsb2FkT3JkZXIiXT0yLHByb3BUPXtsbW9kPXtbInN0aWNreSJdPTEsfSx9LFsic3RhY2tEZXB0aCJdPTAsWyJzdGF0dXMiXT0iYWN0aXZlIixbInVzZXJOYW1lIl09ImdjYy83LjQuMCIsfSx9LG1wYXRoQT17Ii9jdm1mcy9jb25maWcubWlsYS5xdWViZWMvbW9kdWxlcy9Db3JlIiwiL2N2bWZzL2NvbmZpZy5taWxhLnF1ZWJlYy9tb2R1bGVzL0Nv
INFO 2022-05-15 13:29:27,355 env.py:  50: _ModuleTable003_:	bXBpbGVyIiwiL2N2bWZzL2NvbmZpZy5taWxhLnF1ZWJlYy9tb2R1bGVzL0Vudmlyb25tZW50cyIsIi9jdm1mcy9jb25maWcubWlsYS5xdWViZWMvbW9kdWxlcy9DdWRhIiwiL2N2bWZzL2NvbmZpZy5taWxhLnF1ZWJlYy9tb2R1bGVzL1B5dG9yY2giLCIvY3ZtZnMvY29uZmlnLm1pbGEucXVlYmVjL21vZHVsZXMvVGVuc29yZmxvdyIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvY3ZtZnMvY29uZmlnLm1pbGEucXVlYmVjL21vZHVsZXMvQ29yZTovY3ZtZnMvY29uZmlnLm1pbGEucXVlYmVjL21vZHVsZXMvQ29tcGlsZXI6L2N2bWZzL2NvbmZpZy5taWxhLnF1ZWJlYy9tb2R1bGVzL0Vudmlyb25tZW50czovY3ZtZnMvY29uZmlnLm1pbGEucXVlYmVjL21vZHVsZXMvQ3VkYTovY3Zt
INFO 2022-05-15 13:29:27,356 env.py:  50: _ModuleTable004_:	ZnMvY29uZmlnLm1pbGEucXVlYmVjL21vZHVsZXMvUHl0b3JjaDovY3ZtZnMvY29uZmlnLm1pbGEucXVlYmVjL21vZHVsZXMvVGVuc29yZmxvdyIsfQ==
INFO 2022-05-15 13:29:27,356 env.py:  50: _ModuleTable_Sz_:	4
INFO 2022-05-15 13:29:27,357 env.py:  50: __Init_Default_Modules:	1
INFO 2022-05-15 13:29:27,358 env.py:  50: __LMOD_REF_COUNT_CMAKE_LIBRARY_PATH:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/gcc/7.4.0/lib64:1
INFO 2022-05-15 13:29:27,359 env.py:  50: __LMOD_REF_COUNT_CMAKE_PREFIX_PATH:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/gcc/7.4.0:1
INFO 2022-05-15 13:29:27,360 env.py:  50: __LMOD_REF_COUNT_CPATH:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/include:1
INFO 2022-05-15 13:29:27,360 env.py:  50: __LMOD_REF_COUNT_CSPYTHONPREFIXES:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3:1
INFO 2022-05-15 13:29:27,361 env.py:  50: __LMOD_REF_COUNT_LIBRARY_PATH:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib:1;/cvmfs/ai.mila.quebec/apps/x86_64/debian/gcc/7.4.0/lib64:1;/cvmfs/ai.mila.quebec/apps/x86_64/debian/gcc/7.4.0/lib:1
INFO 2022-05-15 13:29:27,362 env.py:  50: __LMOD_REF_COUNT_LOADEDMODULES:	Mila:1;gcc/7.4.0:1;anaconda/3:1
INFO 2022-05-15 13:29:27,363 env.py:  50: __LMOD_REF_COUNT_MANPATH:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/share/man:1;/cvmfs/ai.mila.quebec/apps/x86_64/debian/gcc/7.4.0/share/man:1;/cvmfs/ai.mila.quebec/apps/x86_64/debian/lmod/lmod/share/man:1
INFO 2022-05-15 13:29:27,364 env.py:  50: __LMOD_REF_COUNT_MODULEPATH:	/cvmfs/config.mila.quebec/modules/Core:1;/cvmfs/config.mila.quebec/modules/Compiler:1;/cvmfs/config.mila.quebec/modules/Environments:1;/cvmfs/config.mila.quebec/modules/Cuda:1;/cvmfs/config.mila.quebec/modules/Pytorch:1;/cvmfs/config.mila.quebec/modules/Tensorflow:1
INFO 2022-05-15 13:29:27,365 env.py:  50: __LMOD_REF_COUNT_PATH:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/bin:1;/opt/slurm/bin:1;/sbin:1;/bin:1;/usr/sbin:1;/usr/bin:1
INFO 2022-05-15 13:29:27,366 env.py:  50: __LMOD_REF_COUNT_PKG_CONFIG_PATH:	/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/pkgconfig:1
INFO 2022-05-15 13:29:27,367 env.py:  50: __LMOD_REF_COUNT_PYTHONPATH:	/cvmfs/config.mila.quebec/etc/python.d/3.7:1
INFO 2022-05-15 13:29:27,367 env.py:  50: __LMOD_REF_COUNT__LMFILES_:	/cvmfs/config.mila.quebec/modules/Core/Mila.lua:1;/cvmfs/config.mila.quebec/modules/Core/gcc/7.4.0.lua:1;/cvmfs/config.mila.quebec/modules/Core/anaconda/3.lua:1
INFO 2022-05-15 13:29:27,368 misc.py: 161: Set start method of multiprocessing to forkserver
INFO 2022-05-15 13:29:27,368 train.py: 105: Setting seed....
INFO 2022-05-15 13:29:27,369 misc.py: 173: MACHINE SEED: 0
INFO 2022-05-15 13:29:27,413 hydra_config.py: 132: Training with config:
INFO 2022-05-15 13:29:27,422 hydra_config.py: 141: {'CHECKPOINT': {'APPEND_DISTR_RUN_ID': False,
                'AUTO_RESUME': True,
                'BACKEND': 'disk',
                'CHECKPOINT_FREQUENCY': 20,
                'CHECKPOINT_ITER_FREQUENCY': -1,
                'DIR': './checkpoints/LP/2',
                'LATEST_CHECKPOINT_RESUME_FILE_NUM': 1,
                'OVERWRITE_EXISTING': False,
                'USE_SYMLINK_CHECKPOINT_FOR_RESUME': False},
 'CLUSTERFIT': {'CLUSTER_BACKEND': 'faiss',
                'DATA_LIMIT': -1,
                'DATA_LIMIT_SAMPLING': {'SEED': 0},
                'FEATURES': {'DATASET_NAME': '',
                             'DATA_PARTITION': 'TRAIN',
                             'DIMENSIONALITY_REDUCTION': 0,
                             'EXTRACT': False,
                             'LAYER_NAME': '',
                             'PATH': '.',
                             'TEST_PARTITION': 'TEST'},
                'NUM_CLUSTERS': 16000,
                'NUM_ITER': 50,
                'OUTPUT_DIR': '.'},
 'DATA': {'DDP_BUCKET_CAP_MB': 25,
          'ENABLE_ASYNC_GPU_COPY': True,
          'NUM_DATALOADER_WORKERS': 4,
          'PIN_MEMORY': True,
          'TEST': {'BASE_DATASET': 'generic_ssl',
                   'BATCHSIZE_PER_REPLICA': 512,
                   'COLLATE_FUNCTION': 'default_collate',
                   'COLLATE_FUNCTION_PARAMS': {},
                   'COPY_DESTINATION_DIR': '/tmp/cifar100/',
                   'COPY_TO_LOCAL_DISK': False,
                   'DATASET_NAMES': ['CIFAR100'],
                   'DATA_LIMIT': -1,
                   'DATA_LIMIT_SAMPLING': {'IS_BALANCED': False,
                                           'SEED': 0,
                                           'SKIP_NUM_SAMPLES': 0},
                   'DATA_PATHS': [],
                   'DATA_SOURCES': ['disk_folder'],
                   'DEFAULT_GRAY_IMG_SIZE': 224,
                   'DROP_LAST': False,
                   'ENABLE_QUEUE_DATASET': False,
                   'INPUT_KEY_NAMES': ['data'],
                   'LABEL_PATHS': [],
                   'LABEL_SOURCES': ['disk_folder'],
                   'LABEL_TYPE': 'standard',
                   'MMAP_MODE': True,
                   'NEW_IMG_PATH_PREFIX': '',
                   'RANDOM_SYNTHETIC_IMAGES': False,
                   'REMOVE_IMG_PATH_PREFIX': '',
                   'TARGET_KEY_NAMES': ['label'],
                   'TRANSFORMS': [{'name': 'ToTensor'},
                                  {'mean': [0.5074, 0.4867, 0.4411],
                                   'name': 'Normalize',
                                   'std': [0.2011, 0.1987, 0.202]}],
                   'USE_DEBUGGING_SAMPLER': False,
                   'USE_STATEFUL_DISTRIBUTED_SAMPLER': False},
          'TRAIN': {'BASE_DATASET': 'generic_ssl',
                    'BATCHSIZE_PER_REPLICA': 512,
                    'COLLATE_FUNCTION': 'default_collate',
                    'COLLATE_FUNCTION_PARAMS': {},
                    'COPY_DESTINATION_DIR': '/tmp/cifar100/',
                    'COPY_TO_LOCAL_DISK': False,
                    'DATASET_NAMES': ['CIFAR100'],
                    'DATA_LIMIT': -1,
                    'DATA_LIMIT_SAMPLING': {'IS_BALANCED': False,
                                            'SEED': 0,
                                            'SKIP_NUM_SAMPLES': 0},
                    'DATA_PATHS': [],
                    'DATA_SOURCES': ['disk_folder'],
                    'DEFAULT_GRAY_IMG_SIZE': 224,
                    'DROP_LAST': False,
                    'ENABLE_QUEUE_DATASET': False,
                    'INPUT_KEY_NAMES': ['data'],
                    'LABEL_PATHS': [],
                    'LABEL_SOURCES': ['disk_folder'],
                    'LABEL_TYPE': 'standard',
                    'MMAP_MODE': True,
                    'NEW_IMG_PATH_PREFIX': '',
                    'RANDOM_SYNTHETIC_IMAGES': False,
                    'REMOVE_IMG_PATH_PREFIX': '',
                    'TARGET_KEY_NAMES': ['label'],
                    'TRANSFORMS': [{'name': 'ToTensor'},
                                   {'mean': [0.5074, 0.4867, 0.4411],
                                    'name': 'Normalize',
                                    'std': [0.2011, 0.1987, 0.202]}],
                    'USE_DEBUGGING_SAMPLER': False,
                    'USE_STATEFUL_DISTRIBUTED_SAMPLER': False}},
 'DISTRIBUTED': {'BACKEND': 'nccl',
                 'BROADCAST_BUFFERS': True,
                 'INIT_METHOD': 'tcp',
                 'MANUAL_GRADIENT_REDUCTION': False,
                 'NCCL_DEBUG': False,
                 'NCCL_SOCKET_NTHREADS': '',
                 'NUM_NODES': 1,
                 'NUM_PROC_PER_NODE': 1,
                 'RUN_ID': 'auto'},
 'EXTRACT_FEATURES': {'CHUNK_THRESHOLD': 0, 'OUTPUT_DIR': ''},
 'HOOKS': {'CHECK_NAN': True,
           'LOG_GPU_STATS': True,
           'MEMORY_SUMMARY': {'DUMP_MEMORY_ON_EXCEPTION': False,
                              'LOG_ITERATION_NUM': 0,
                              'PRINT_MEMORY_SUMMARY': True},
           'MODEL_COMPLEXITY': {'COMPUTE_COMPLEXITY': False,
                                'INPUT_SHAPE': [3, 224, 224]},
           'PERF_STATS': {'MONITOR_PERF_STATS': True,
                          'PERF_STAT_FREQUENCY': -1,
                          'ROLLING_BTIME_FREQ': -1},
           'TENSORBOARD_SETUP': {'EXPERIMENT_LOG_DIR': 'tensorboard',
                                 'FLUSH_EVERY_N_MIN': 5,
                                 'LOG_DIR': '.',
                                 'LOG_PARAMS': True,
                                 'LOG_PARAMS_EVERY_N_ITERS': 310,
                                 'LOG_PARAMS_GRADIENTS': True,
                                 'USE_TENSORBOARD': False}},
 'IMG_RETRIEVAL': {'CROP_QUERY_ROI': False,
                   'DATASET_PATH': '',
                   'DEBUG_MODE': False,
                   'EVAL_BINARY_PATH': '',
                   'EVAL_DATASET_NAME': 'Paris',
                   'FEATS_PROCESSING_TYPE': '',
                   'GEM_POOL_POWER': 4.0,
                   'IMG_SCALINGS': [1],
                   'NORMALIZE_FEATURES': True,
                   'NUM_DATABASE_SAMPLES': -1,
                   'NUM_QUERY_SAMPLES': -1,
                   'NUM_TRAINING_SAMPLES': -1,
                   'N_PCA': 512,
                   'RESIZE_IMG': 1024,
                   'SAVE_FEATURES': False,
                   'SAVE_RETRIEVAL_RANKINGS_SCORES': True,
                   'SIMILARITY_MEASURE': 'cosine_similarity',
                   'SPATIAL_LEVELS': 3,
                   'TRAIN_DATASET_NAME': 'Oxford',
                   'TRAIN_PCA_WHITENING': True,
                   'USE_DISTRACTORS': False,
                   'WHITEN_IMG_LIST': ''},
 'LOG_FREQUENCY': 200,
 'LOSS': {'CrossEntropyLoss': {'ignore_index': -1},
          'barlow_twins_loss': {'embedding_dim': 8192,
                                'lambda_': 0.0051,
                                'scale_loss': 0.024},
          'bce_logits_multiple_output_single_target': {'normalize_output': False,
                                                       'reduction': 'none',
                                                       'world_size': 1},
          'cross_entropy_multiple_output_single_target': {'ignore_index': -1,
                                                          'normalize_output': False,
                                                          'reduction': 'mean',
                                                          'temperature': 1.0,
                                                          'weight': None},
          'deepclusterv2_loss': {'BATCHSIZE_PER_REPLICA': 256,
                                 'DROP_LAST': True,
                                 'kmeans_iters': 10,
                                 'memory_params': {'crops_for_mb': [0],
                                                   'embedding_dim': 128},
                                 'num_clusters': [3000, 3000, 3000],
                                 'num_crops': 2,
                                 'num_train_samples': -1,
                                 'temperature': 0.1},
          'dino_loss': {'crops_for_teacher': [0, 1],
                        'ema_center': 0.9,
                        'momentum': 0.996,
                        'normalize_last_layer': True,
                        'output_dim': 65536,
                        'student_temp': 0.1,
                        'teacher_temp_max': 0.07,
                        'teacher_temp_min': 0.04,
                        'teacher_temp_warmup_iters': 37500},
          'moco_loss': {'embedding_dim': 128,
                        'momentum': 0.999,
                        'queue_size': 65536,
                        'temperature': 0.2},
          'multicrop_simclr_info_nce_loss': {'buffer_params': {'effective_batch_size': 4096,
                                                               'embedding_dim': 128,
                                                               'world_size': 64},
                                             'num_crops': 2,
                                             'temperature': 0.1},
          'name': 'cross_entropy_multiple_output_single_target',
          'nce_loss_with_memory': {'loss_type': 'nce',
                                   'loss_weights': [1.0],
                                   'memory_params': {'embedding_dim': 128,
                                                     'memory_size': -1,
                                                     'momentum': 0.5,
                                                     'norm_init': True,
                                                     'update_mem_on_forward': True},
                                   'negative_sampling_params': {'num_negatives': 16000,
                                                                'type': 'random'},
                                   'norm_constant': -1,
                                   'norm_embedding': True,
                                   'num_train_samples': -1,
                                   'temperature': 0.07,
                                   'update_mem_with_emb_index': -100},
          'simclr_info_nce_loss': {'buffer_params': {'effective_batch_size': 4096,
                                                     'embedding_dim': 128,
                                                     'world_size': 64},
                                   'temperature': 0.1},
          'swav_loss': {'crops_for_assign': [0, 1],
                        'embedding_dim': 128,
                        'epsilon': 0.05,
                        'normalize_last_layer': True,
                        'num_crops': 2,
                        'num_iters': 3,
                        'num_prototypes': [3000],
                        'output_dir': '.',
                        'queue': {'local_queue_length': 0,
                                  'queue_length': 0,
                                  'start_iter': 0},
                        'temp_hard_assignment_iters': 0,
                        'temperature': 0.1,
                        'use_double_precision': False},
          'swav_momentum_loss': {'crops_for_assign': [0, 1],
                                 'embedding_dim': 128,
                                 'epsilon': 0.05,
                                 'momentum': 0.99,
                                 'momentum_eval_mode_iter_start': 0,
                                 'normalize_last_layer': True,
                                 'num_crops': 2,
                                 'num_iters': 3,
                                 'num_prototypes': [3000],
                                 'queue': {'local_queue_length': 0,
                                           'queue_length': 0,
                                           'start_iter': 0},
                                 'temperature': 0.1,
                                 'use_double_precision': False}},
 'MACHINE': {'DEVICE': 'gpu'},
 'METERS': {'accuracy_list_meter': {'meter_names': ['res5'],
                                    'num_meters': 1,
                                    'topk_values': [1, 5]},
            'enable_training_meter': True,
            'mean_ap_list_meter': {'max_cpu_capacity': -1,
                                   'meter_names': [],
                                   'num_classes': 9605,
                                   'num_meters': 1},
            'model_output_mask': False,
            'name': 'accuracy_list_meter',
            'names': ['accuracy_list_meter'],
            'precision_at_k_list_meter': {'meter_names': [],
                                          'num_meters': 1,
                                          'topk_values': [1]},
            'recall_at_k_list_meter': {'meter_names': [],
                                       'num_meters': 1,
                                       'topk_values': [1]}},
 'MODEL': {'ACTIVATION_CHECKPOINTING': {'NUM_ACTIVATION_CHECKPOINTING_SPLITS': 2,
                                        'USE_ACTIVATION_CHECKPOINTING': False},
           'AMP_PARAMS': {'AMP_ARGS': {'opt_level': 'O1'},
                          'AMP_TYPE': 'apex',
                          'USE_AMP': False},
           'BASE_MODEL_NAME': 'multi_input_output_model',
           'CUDA_CACHE': {'CLEAR_CUDA_CACHE': False, 'CLEAR_FREQ': 100},
           'FEATURE_EVAL_SETTINGS': {'EVAL_MODE_ON': True,
                                     'EVAL_TRUNK_AND_HEAD': False,
                                     'EXTRACT_TRUNK_FEATURES_ONLY': False,
                                     'FREEZE_TRUNK_AND_HEAD': False,
                                     'FREEZE_TRUNK_ONLY': True,
                                     'LINEAR_EVAL_FEAT_POOL_OPS_MAP': [['res5',
                                                                        ['AdaptiveAvgPool2d',
                                                                         [[1,
                                                                           1]]]]],
                                     'SHOULD_FLATTEN_FEATS': False},
           'FSDP_CONFIG': {'AUTO_WRAP_THRESHOLD': 0,
                           'bucket_cap_mb': 0,
                           'clear_autocast_cache': True,
                           'compute_dtype': torch.float32,
                           'flatten_parameters': True,
                           'fp32_reduce_scatter': False,
                           'mixed_precision': True,
                           'verbose': True},
           'GRAD_CLIP': {'MAX_NORM': 1, 'NORM_TYPE': 2, 'USE_GRAD_CLIP': False},
           'HEAD': {'BATCHNORM_EPS': 1e-05,
                    'BATCHNORM_MOMENTUM': 0.1,
                    'PARAMS': [['eval_mlp',
                                {'dims': [512, 100], 'in_channels': 512}]],
                    'PARAMS_MULTIPLIER': 1.0},
           'INPUT_TYPE': 'rgb',
           'MULTI_INPUT_HEAD_MAPPING': [],
           'NON_TRAINABLE_PARAMS': [],
           'SHARDED_DDP_SETUP': {'USE_SDP': False, 'reduce_buffer_size': -1},
           'SINGLE_PASS_EVERY_CROP': False,
           'SYNC_BN_CONFIG': {'CONVERT_BN_TO_SYNC_BN': False,
                              'GROUP_SIZE': -1,
                              'SYNC_BN_TYPE': 'pytorch'},
           'TEMP_FROZEN_PARAMS_ITER_MAP': [],
           'TRUNK': {'CONVIT': {'CLASS_TOKEN_IN_LOCAL_LAYERS': False,
                                'LOCALITY_DIM': 10,
                                'LOCALITY_STRENGTH': 1.0,
                                'N_GPSA_LAYERS': 10,
                                'USE_LOCAL_INIT': True},
                     'EFFICIENT_NETS': {},
                     'NAME': 'resnet',
                     'REGNET': {},
                     'RESNETS': {'BLOCK': 'BasicBlock',
                                 'CONV1_KERNEL': 3,
                                 'CONV1_PADDING': 1,
                                 'CONV1_STRIDE': 1,
                                 'DEPTH': 18,
                                 'GROUPNORM_GROUPS': 32,
                                 'GROUPS': 1,
                                 'LAYER4_STRIDE': 2,
                                 'MAXPOOL': False,
                                 'NORM': 'BatchNorm',
                                 'STANDARDIZE_CONVOLUTIONS': False,
                                 'WIDTH_MULTIPLIER': 1,
                                 'WIDTH_PER_GROUP': 64,
                                 'ZERO_INIT_RESIDUAL': False},
                     'VISION_TRANSFORMERS': {'ATTENTION_DROPOUT_RATE': 0,
                                             'CLASSIFIER': 'token',
                                             'DROPOUT_RATE': 0,
                                             'DROP_PATH_RATE': 0,
                                             'HIDDEN_DIM': 768,
                                             'IMAGE_SIZE': 224,
                                             'MLP_DIM': 3072,
                                             'NUM_HEADS': 12,
                                             'NUM_LAYERS': 12,
                                             'PATCH_SIZE': 16,
                                             'QKV_BIAS': False,
                                             'QK_SCALE': False,
                                             'name': None},
                     'XCIT': {'ATTENTION_DROPOUT_RATE': 0,
                              'DROPOUT_RATE': 0,
                              'DROP_PATH_RATE': 0.05,
                              'ETA': 1,
                              'HIDDEN_DIM': 384,
                              'IMAGE_SIZE': 224,
                              'NUM_HEADS': 8,
                              'NUM_LAYERS': 12,
                              'PATCH_SIZE': 16,
                              'QKV_BIAS': True,
                              'QK_SCALE': False,
                              'TOKENS_NORM': True,
                              'name': None}},
           'WEIGHTS_INIT': {'APPEND_PREFIX': '',
                            'PARAMS_FILE': './checkpoints/trained_swav/2/model_final_checkpoint_phase499.torch',
                            'REMOVE_PREFIX': '',
                            'SKIP_LAYERS': ['num_batches_tracked'],
                            'STATE_DICT_KEY_NAME': 'classy_state_dict'},
           '_MODEL_INIT_SEED': 0},
 'MONITORING': {'MONITOR_ACTIVATION_STATISTICS': 0},
 'MULTI_PROCESSING_METHOD': 'forkserver',
 'NEAREST_NEIGHBOR': {'L2_NORM_FEATS': False, 'SIGMA': 0.1, 'TOPK': 200},
 'OPTIMIZER': {'betas': [0.9, 0.999],
               'construct_single_param_group_only': False,
               'head_optimizer_params': {'use_different_lr': False,
                                         'use_different_wd': False,
                                         'weight_decay': 0.0},
               'larc_config': {'clip': False,
                               'eps': 1e-08,
                               'trust_coefficient': 0.001},
               'momentum': 0.0,
               'name': 'sgd',
               'nesterov': False,
               'non_regularized_parameters': [],
               'num_epochs': 100,
               'param_schedulers': {'lr': {'auto_lr_scaling': {'auto_scale': False,
                                                               'base_lr_batch_size': 256,
                                                               'base_value': 0.1,
                                                               'scaling_type': 'linear'},
                                           'end_value': 0.0,
                                           'interval_scaling': [],
                                           'lengths': [],
                                           'milestones': [60, 80],
                                           'name': 'multistep',
                                           'schedulers': [],
                                           'start_value': 0.1,
                                           'update_interval': 'epoch',
                                           'value': 0.1,
                                           'values': [0.3, 0.03, 0.003]},
                                    'lr_head': {'auto_lr_scaling': {'auto_scale': False,
                                                                    'base_lr_batch_size': 256,
                                                                    'base_value': 0.1,
                                                                    'scaling_type': 'linear'},
                                                'end_value': 0.0,
                                                'interval_scaling': [],
                                                'lengths': [],
                                                'milestones': [60, 80],
                                                'name': 'multistep',
                                                'schedulers': [],
                                                'start_value': 0.1,
                                                'update_interval': 'epoch',
                                                'value': 0.1,
                                                'values': [0.3, 0.03, 0.003]}},
               'regularize_bias': True,
               'regularize_bn': False,
               'use_larc': False,
               'use_zero': False,
               'weight_decay': 0.0},
 'PROFILING': {'MEMORY_PROFILING': {'TRACK_BY_LAYER_MEMORY': False},
               'NUM_ITERATIONS': 10,
               'OUTPUT_FOLDER': '.',
               'PROFILED_RANKS': [0, 1],
               'RUNTIME_PROFILING': {'LEGACY_PROFILER': False,
                                     'PROFILE_CPU': True,
                                     'PROFILE_GPU': True,
                                     'USE_PROFILER': False},
               'START_ITERATION': 0,
               'STOP_TRAINING_AFTER_PROFILING': False,
               'WARMUP_ITERATIONS': 0},
 'REPRODUCIBILITY': {'CUDDN_DETERMINISTIC': False},
 'SEED_VALUE': 0,
 'SLURM': {'ADDITIONAL_PARAMETERS': {},
           'COMMENT': 'vissl job',
           'CONSTRAINT': '',
           'LOG_FOLDER': '.',
           'MEM_GB': 250,
           'NAME': 'vissl',
           'NUM_CPU_PER_PROC': 8,
           'PARTITION': '',
           'PORT_ID': 40050,
           'TIME_HOURS': 72,
           'TIME_MINUTES': 0,
           'USE_SLURM': False},
 'SVM': {'cls_list': [],
         'costs': {'base': -1.0,
                   'costs_list': [0.1, 0.01],
                   'power_range': [4, 20]},
         'cross_val_folds': 3,
         'dual': True,
         'force_retrain': False,
         'loss': 'squared_hinge',
         'low_shot': {'dataset_name': 'voc',
                      'k_values': [1, 2, 4, 8, 16, 32, 64, 96],
                      'sample_inds': [1, 2, 3, 4, 5]},
         'max_iter': 2000,
         'normalize': True,
         'penalty': 'l2'},
 'TEST_EVERY_NUM_EPOCH': 1,
 'TEST_MODEL': True,
 'TEST_ONLY': False,
 'TRAINER': {'TASK_NAME': 'self_supervision_task',
             'TRAIN_STEP_NAME': 'standard_train_step'},
 'VERBOSE': True}
INFO 2022-05-15 13:29:37,278 train.py: 117: System config:
-------------------  ---------------------------------------------------------------------------------------------------------------
sys.platform         linux
Python               3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
numpy                1.19.5
Pillow               9.0.1
vissl                0.1.6 @/home/mila/r/rajkuman/mina/mphil-vissl/vissl
GPU available        True
GPU 0                Tesla V100-SXM2-16GB
CUDA_HOME            /usr/local/cuda
torchvision          0.9.1 @/home/mila/r/rajkuman/.conda/envs/vissl_env/lib/python3.8/site-packages/torchvision
hydra                1.0.7 @/home/mila/r/rajkuman/.conda/envs/vissl_env/lib/python3.8/site-packages/hydra_core-1.0.7-py3.8.egg/hydra
classy_vision        0.7.0.dev @/home/mila/r/rajkuman/.conda/envs/vissl_env/lib/python3.8/site-packages/classy_vision
tensorboard          2.9.0
apex                 0.1 @/home/mila/r/rajkuman/.conda/envs/vissl_env/lib/python3.8/site-packages/apex
PyTorch              1.8.1 @/home/mila/r/rajkuman/.conda/envs/vissl_env/lib/python3.8/site-packages/torch
PyTorch debug build  False
-------------------  ---------------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=10.2, CUDNN_VERSION=7.6.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

CPU info:
-------------------  -----------------------------------------
Architecture         x86_64
CPU op-mode(s)       32-bit, 64-bit
Byte Order           Little Endian
CPU(s)               80
On-line CPU(s) list  0-79
Thread(s) per core   2
Core(s) per socket   20
Socket(s)            2
NUMA node(s)         2
Vendor ID            GenuineIntel
CPU family           6
Model                79
Model name           Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz
Stepping             1
CPU MHz              2736.801
CPU max MHz          3600.0000
CPU min MHz          1200.0000
BogoMIPS             4390.19
Virtualization       VT-x
L1d cache            32K
L1i cache            32K
L2 cache             256K
L3 cache             51200K
NUMA node0 CPU(s)    0-19,40-59
NUMA node1 CPU(s)    20-39,60-79
-------------------  -----------------------------------------
INFO 2022-05-15 13:29:37,285 trainer_main.py: 112: Using Distributed init method: tcp://localhost:42111, world_size: 1, rank: 0
INFO 2022-05-15 13:29:37,291 distributed_c10d.py: 187: Added key: store_based_barrier_key:1 to store for rank: 0
INFO 2022-05-15 13:29:37,292 trainer_main.py: 130: | initialized host mila01 as rank 0 (0)
INFO 2022-05-15 13:29:45,076 train_task.py: 181: Not using Automatic Mixed Precision
INFO 2022-05-15 13:29:45,078 train_task.py: 455: Building model....
INFO 2022-05-15 13:29:45,080 feature_extractor.py:  27: Creating Feature extractor trunk...
INFO 2022-05-15 13:29:45,080 resnext.py:  66: ResNeXT trunk, supports activation checkpointing. Deactivated
INFO 2022-05-15 13:29:45,081 resnext.py:  93: Building model: ResNeXt18-1x64d-w1-BatchNorm2d
INFO 2022-05-15 13:29:45,539 feature_extractor.py:  50: Freezing model trunk...
INFO 2022-05-15 13:29:45,550 train_task.py: 472: config.MODEL.FEATURE_EVAL_SETTINGS.FREEZE_TRUNK_ONLY=True, will freeze trunk...
INFO 2022-05-15 13:29:45,551 base_ssl_model.py: 195: Freezing model trunk...
INFO 2022-05-15 13:29:45,559 train_task.py: 429: Initializing model from: ./checkpoints/trained_swav/2/model_final_checkpoint_phase499.torch
INFO 2022-05-15 13:29:45,564 util.py: 276: Attempting to load checkpoint from ./checkpoints/trained_swav/2/model_final_checkpoint_phase499.torch
INFO 2022-05-15 13:29:45,879 util.py: 281: Loaded checkpoint from ./checkpoints/trained_swav/2/model_final_checkpoint_phase499.torch
INFO 2022-05-15 13:29:45,880 util.py: 240: Broadcasting checkpoint loaded from ./checkpoints/trained_swav/2/model_final_checkpoint_phase499.torch
INFO 2022-05-15 13:29:50,580 train_task.py: 435: Checkpoint loaded: ./checkpoints/trained_swav/2/model_final_checkpoint_phase499.torch...
INFO 2022-05-15 13:29:50,582 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.conv1.weight                              of shape: torch.Size([64, 3, 3, 3]) from checkpoint
INFO 2022-05-15 13:29:50,582 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.bn1.weight                                of shape: torch.Size([64]) from checkpoint
INFO 2022-05-15 13:29:50,583 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.bn1.bias                                  of shape: torch.Size([64]) from checkpoint
INFO 2022-05-15 13:29:50,584 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.bn1.running_mean                          of shape: torch.Size([64]) from checkpoint
INFO 2022-05-15 13:29:50,585 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.bn1.running_var                           of shape: torch.Size([64]) from checkpoint
INFO 2022-05-15 13:29:50,585 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.bn1.num_batches_tracked
INFO 2022-05-15 13:29:50,590 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.0.conv1.weight                     of shape: torch.Size([64, 64, 3, 3]) from checkpoint
INFO 2022-05-15 13:29:50,592 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.0.bn1.weight                       of shape: torch.Size([64]) from checkpoint
INFO 2022-05-15 13:29:50,592 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.0.bn1.bias                         of shape: torch.Size([64]) from checkpoint
INFO 2022-05-15 13:29:50,593 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.0.bn1.running_mean                 of shape: torch.Size([64]) from checkpoint
INFO 2022-05-15 13:29:50,594 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.0.bn1.running_var                  of shape: torch.Size([64]) from checkpoint
INFO 2022-05-15 13:29:50,595 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer1.0.bn1.num_batches_tracked
INFO 2022-05-15 13:29:50,600 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.0.conv2.weight                     of shape: torch.Size([64, 64, 3, 3]) from checkpoint
INFO 2022-05-15 13:29:50,601 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.0.bn2.weight                       of shape: torch.Size([64]) from checkpoint
INFO 2022-05-15 13:29:50,602 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.0.bn2.bias                         of shape: torch.Size([64]) from checkpoint
INFO 2022-05-15 13:29:50,603 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.0.bn2.running_mean                 of shape: torch.Size([64]) from checkpoint
INFO 2022-05-15 13:29:50,605 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.0.bn2.running_var                  of shape: torch.Size([64]) from checkpoint
INFO 2022-05-15 13:29:50,606 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer1.0.bn2.num_batches_tracked
INFO 2022-05-15 13:29:50,611 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.1.conv1.weight                     of shape: torch.Size([64, 64, 3, 3]) from checkpoint
INFO 2022-05-15 13:29:50,612 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.1.bn1.weight                       of shape: torch.Size([64]) from checkpoint
INFO 2022-05-15 13:29:50,613 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.1.bn1.bias                         of shape: torch.Size([64]) from checkpoint
INFO 2022-05-15 13:29:50,614 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.1.bn1.running_mean                 of shape: torch.Size([64]) from checkpoint
INFO 2022-05-15 13:29:50,615 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.1.bn1.running_var                  of shape: torch.Size([64]) from checkpoint
INFO 2022-05-15 13:29:50,616 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer1.1.bn1.num_batches_tracked
INFO 2022-05-15 13:29:50,621 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.1.conv2.weight                     of shape: torch.Size([64, 64, 3, 3]) from checkpoint
INFO 2022-05-15 13:29:50,623 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.1.bn2.weight                       of shape: torch.Size([64]) from checkpoint
INFO 2022-05-15 13:29:50,626 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.1.bn2.bias                         of shape: torch.Size([64]) from checkpoint
INFO 2022-05-15 13:29:50,627 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.1.bn2.running_mean                 of shape: torch.Size([64]) from checkpoint
INFO 2022-05-15 13:29:50,628 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer1.1.bn2.running_var                  of shape: torch.Size([64]) from checkpoint
INFO 2022-05-15 13:29:50,629 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer1.1.bn2.num_batches_tracked
INFO 2022-05-15 13:29:50,632 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.0.conv1.weight                     of shape: torch.Size([128, 64, 3, 3]) from checkpoint
INFO 2022-05-15 13:29:50,633 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.0.bn1.weight                       of shape: torch.Size([128]) from checkpoint
INFO 2022-05-15 13:29:50,633 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.0.bn1.bias                         of shape: torch.Size([128]) from checkpoint
INFO 2022-05-15 13:29:50,634 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.0.bn1.running_mean                 of shape: torch.Size([128]) from checkpoint
INFO 2022-05-15 13:29:50,635 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.0.bn1.running_var                  of shape: torch.Size([128]) from checkpoint
INFO 2022-05-15 13:29:50,639 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer2.0.bn1.num_batches_tracked
INFO 2022-05-15 13:29:50,644 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.0.conv2.weight                     of shape: torch.Size([128, 128, 3, 3]) from checkpoint
INFO 2022-05-15 13:29:50,645 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.0.bn2.weight                       of shape: torch.Size([128]) from checkpoint
INFO 2022-05-15 13:29:50,646 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.0.bn2.bias                         of shape: torch.Size([128]) from checkpoint
INFO 2022-05-15 13:29:50,647 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.0.bn2.running_mean                 of shape: torch.Size([128]) from checkpoint
INFO 2022-05-15 13:29:50,648 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.0.bn2.running_var                  of shape: torch.Size([128]) from checkpoint
INFO 2022-05-15 13:29:50,648 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer2.0.bn2.num_batches_tracked
INFO 2022-05-15 13:29:50,649 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.0.downsample.0.weight              of shape: torch.Size([128, 64, 1, 1]) from checkpoint
INFO 2022-05-15 13:29:50,650 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.0.downsample.1.weight              of shape: torch.Size([128]) from checkpoint
INFO 2022-05-15 13:29:50,651 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.0.downsample.1.bias                of shape: torch.Size([128]) from checkpoint
INFO 2022-05-15 13:29:50,652 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.0.downsample.1.running_mean        of shape: torch.Size([128]) from checkpoint
INFO 2022-05-15 13:29:50,653 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.0.downsample.1.running_var         of shape: torch.Size([128]) from checkpoint
INFO 2022-05-15 13:29:50,654 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer2.0.downsample.1.num_batches_tracked
INFO 2022-05-15 13:29:50,655 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.1.conv1.weight                     of shape: torch.Size([128, 128, 3, 3]) from checkpoint
INFO 2022-05-15 13:29:50,657 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.1.bn1.weight                       of shape: torch.Size([128]) from checkpoint
INFO 2022-05-15 13:29:50,658 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.1.bn1.bias                         of shape: torch.Size([128]) from checkpoint
INFO 2022-05-15 13:29:50,658 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.1.bn1.running_mean                 of shape: torch.Size([128]) from checkpoint
INFO 2022-05-15 13:29:50,659 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.1.bn1.running_var                  of shape: torch.Size([128]) from checkpoint
INFO 2022-05-15 13:29:50,660 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer2.1.bn1.num_batches_tracked
INFO 2022-05-15 13:29:50,662 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.1.conv2.weight                     of shape: torch.Size([128, 128, 3, 3]) from checkpoint
INFO 2022-05-15 13:29:50,663 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.1.bn2.weight                       of shape: torch.Size([128]) from checkpoint
INFO 2022-05-15 13:29:50,666 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.1.bn2.bias                         of shape: torch.Size([128]) from checkpoint
INFO 2022-05-15 13:29:50,668 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.1.bn2.running_mean                 of shape: torch.Size([128]) from checkpoint
INFO 2022-05-15 13:29:50,669 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer2.1.bn2.running_var                  of shape: torch.Size([128]) from checkpoint
INFO 2022-05-15 13:29:50,669 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer2.1.bn2.num_batches_tracked
INFO 2022-05-15 13:29:50,675 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.0.conv1.weight                     of shape: torch.Size([256, 128, 3, 3]) from checkpoint
INFO 2022-05-15 13:29:50,676 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.0.bn1.weight                       of shape: torch.Size([256]) from checkpoint
INFO 2022-05-15 13:29:50,677 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.0.bn1.bias                         of shape: torch.Size([256]) from checkpoint
INFO 2022-05-15 13:29:50,678 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.0.bn1.running_mean                 of shape: torch.Size([256]) from checkpoint
INFO 2022-05-15 13:29:50,679 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.0.bn1.running_var                  of shape: torch.Size([256]) from checkpoint
INFO 2022-05-15 13:29:50,680 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer3.0.bn1.num_batches_tracked
INFO 2022-05-15 13:29:50,685 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.0.conv2.weight                     of shape: torch.Size([256, 256, 3, 3]) from checkpoint
INFO 2022-05-15 13:29:50,686 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.0.bn2.weight                       of shape: torch.Size([256]) from checkpoint
INFO 2022-05-15 13:29:50,688 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.0.bn2.bias                         of shape: torch.Size([256]) from checkpoint
INFO 2022-05-15 13:29:50,688 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.0.bn2.running_mean                 of shape: torch.Size([256]) from checkpoint
INFO 2022-05-15 13:29:50,689 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.0.bn2.running_var                  of shape: torch.Size([256]) from checkpoint
INFO 2022-05-15 13:29:50,690 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer3.0.bn2.num_batches_tracked
INFO 2022-05-15 13:29:50,691 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.0.downsample.0.weight              of shape: torch.Size([256, 128, 1, 1]) from checkpoint
INFO 2022-05-15 13:29:50,693 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.0.downsample.1.weight              of shape: torch.Size([256]) from checkpoint
INFO 2022-05-15 13:29:50,694 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.0.downsample.1.bias                of shape: torch.Size([256]) from checkpoint
INFO 2022-05-15 13:29:50,694 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.0.downsample.1.running_mean        of shape: torch.Size([256]) from checkpoint
INFO 2022-05-15 13:29:50,696 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.0.downsample.1.running_var         of shape: torch.Size([256]) from checkpoint
INFO 2022-05-15 13:29:50,697 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer3.0.downsample.1.num_batches_tracked
INFO 2022-05-15 13:29:50,700 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.1.conv1.weight                     of shape: torch.Size([256, 256, 3, 3]) from checkpoint
INFO 2022-05-15 13:29:50,704 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.1.bn1.weight                       of shape: torch.Size([256]) from checkpoint
INFO 2022-05-15 13:29:50,704 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.1.bn1.bias                         of shape: torch.Size([256]) from checkpoint
INFO 2022-05-15 13:29:50,705 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.1.bn1.running_mean                 of shape: torch.Size([256]) from checkpoint
INFO 2022-05-15 13:29:50,705 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.1.bn1.running_var                  of shape: torch.Size([256]) from checkpoint
INFO 2022-05-15 13:29:50,706 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer3.1.bn1.num_batches_tracked
INFO 2022-05-15 13:29:50,713 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.1.conv2.weight                     of shape: torch.Size([256, 256, 3, 3]) from checkpoint
INFO 2022-05-15 13:29:50,715 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.1.bn2.weight                       of shape: torch.Size([256]) from checkpoint
INFO 2022-05-15 13:29:50,716 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.1.bn2.bias                         of shape: torch.Size([256]) from checkpoint
INFO 2022-05-15 13:29:50,717 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.1.bn2.running_mean                 of shape: torch.Size([256]) from checkpoint
INFO 2022-05-15 13:29:50,718 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer3.1.bn2.running_var                  of shape: torch.Size([256]) from checkpoint
INFO 2022-05-15 13:29:50,719 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer3.1.bn2.num_batches_tracked
INFO 2022-05-15 13:29:50,725 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.0.conv1.weight                     of shape: torch.Size([512, 256, 3, 3]) from checkpoint
INFO 2022-05-15 13:29:50,726 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.0.bn1.weight                       of shape: torch.Size([512]) from checkpoint
INFO 2022-05-15 13:29:50,727 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.0.bn1.bias                         of shape: torch.Size([512]) from checkpoint
INFO 2022-05-15 13:29:50,728 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.0.bn1.running_mean                 of shape: torch.Size([512]) from checkpoint
INFO 2022-05-15 13:29:50,729 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.0.bn1.running_var                  of shape: torch.Size([512]) from checkpoint
INFO 2022-05-15 13:29:50,730 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer4.0.bn1.num_batches_tracked
INFO 2022-05-15 13:29:50,733 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.0.conv2.weight                     of shape: torch.Size([512, 512, 3, 3]) from checkpoint
INFO 2022-05-15 13:29:50,734 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.0.bn2.weight                       of shape: torch.Size([512]) from checkpoint
INFO 2022-05-15 13:29:50,735 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.0.bn2.bias                         of shape: torch.Size([512]) from checkpoint
INFO 2022-05-15 13:29:50,736 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.0.bn2.running_mean                 of shape: torch.Size([512]) from checkpoint
INFO 2022-05-15 13:29:50,737 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.0.bn2.running_var                  of shape: torch.Size([512]) from checkpoint
INFO 2022-05-15 13:29:50,738 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer4.0.bn2.num_batches_tracked
INFO 2022-05-15 13:29:50,744 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.0.downsample.0.weight              of shape: torch.Size([512, 256, 1, 1]) from checkpoint
INFO 2022-05-15 13:29:50,745 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.0.downsample.1.weight              of shape: torch.Size([512]) from checkpoint
INFO 2022-05-15 13:29:50,746 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.0.downsample.1.bias                of shape: torch.Size([512]) from checkpoint
INFO 2022-05-15 13:29:50,746 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.0.downsample.1.running_mean        of shape: torch.Size([512]) from checkpoint
INFO 2022-05-15 13:29:50,748 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.0.downsample.1.running_var         of shape: torch.Size([512]) from checkpoint
INFO 2022-05-15 13:29:50,748 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer4.0.downsample.1.num_batches_tracked
INFO 2022-05-15 13:29:50,753 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.1.conv1.weight                     of shape: torch.Size([512, 512, 3, 3]) from checkpoint
INFO 2022-05-15 13:29:50,754 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.1.bn1.weight                       of shape: torch.Size([512]) from checkpoint
INFO 2022-05-15 13:29:50,757 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.1.bn1.bias                         of shape: torch.Size([512]) from checkpoint
INFO 2022-05-15 13:29:50,758 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.1.bn1.running_mean                 of shape: torch.Size([512]) from checkpoint
INFO 2022-05-15 13:29:50,758 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.1.bn1.running_var                  of shape: torch.Size([512]) from checkpoint
INFO 2022-05-15 13:29:50,759 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer4.1.bn1.num_batches_tracked
INFO 2022-05-15 13:29:50,765 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.1.conv2.weight                     of shape: torch.Size([512, 512, 3, 3]) from checkpoint
INFO 2022-05-15 13:29:50,766 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.1.bn2.weight                       of shape: torch.Size([512]) from checkpoint
INFO 2022-05-15 13:29:50,767 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.1.bn2.bias                         of shape: torch.Size([512]) from checkpoint
INFO 2022-05-15 13:29:50,771 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.1.bn2.running_mean                 of shape: torch.Size([512]) from checkpoint
INFO 2022-05-15 13:29:50,772 checkpoint.py: 885: Loaded: trunk.base_model._feature_blocks.layer4.1.bn2.running_var                  of shape: torch.Size([512]) from checkpoint
INFO 2022-05-15 13:29:50,773 checkpoint.py: 851: Ignored layer:	trunk.base_model._feature_blocks.layer4.1.bn2.num_batches_tracked
INFO 2022-05-15 13:29:50,774 checkpoint.py: 894: Not found:		heads.0.channel_bn.weight, not initialized
INFO 2022-05-15 13:29:50,775 checkpoint.py: 894: Not found:		heads.0.channel_bn.bias, not initialized
INFO 2022-05-15 13:29:50,776 checkpoint.py: 894: Not found:		heads.0.channel_bn.running_mean, not initialized
INFO 2022-05-15 13:29:50,777 checkpoint.py: 894: Not found:		heads.0.channel_bn.running_var, not initialized
INFO 2022-05-15 13:29:50,778 checkpoint.py: 851: Ignored layer:	heads.0.channel_bn.num_batches_tracked
INFO 2022-05-15 13:29:50,779 checkpoint.py: 894: Not found:		heads.0.clf.clf.0.weight, not initialized
INFO 2022-05-15 13:29:50,780 checkpoint.py: 894: Not found:		heads.0.clf.clf.0.bias, not initialized
INFO 2022-05-15 13:29:50,780 checkpoint.py: 901: Extra layers not loaded from checkpoint: ['heads.0.projection_head.0.weight', 'heads.0.projection_head.0.bias', 'heads.0.projection_head.1.weight', 'heads.0.projection_head.1.bias', 'heads.0.projection_head.1.running_mean', 'heads.0.projection_head.1.running_var', 'heads.0.projection_head.1.num_batches_tracked', 'heads.0.projection_head.3.weight', 'heads.0.projection_head.3.bias', 'heads.0.prototypes0.weight']
INFO 2022-05-15 13:29:50,788 train_task.py: 656: Broadcast model BN buffers from primary on every forward pass
INFO 2022-05-15 13:29:50,789 classification_task.py: 387: Synchronized Batch Normalization is disabled
INFO 2022-05-15 13:29:50,917 optimizer_helper.py: 293: 
Trainable params: 4, 
Non-Trainable params: 0, 
Trunk Regularized Parameters: 0, 
Trunk Unregularized Parameters 0, 
Head Regularized Parameters: 2, 
Head Unregularized Parameters: 2 
Remaining Regularized Parameters: 0 
Remaining Unregularized Parameters: 0
INFO 2022-05-15 13:29:50,924 ssl_dataset.py: 156: Rank: 0 split: TEST Data files:
['data/CIFAR-100-dataset/test']
INFO 2022-05-15 13:29:50,925 ssl_dataset.py: 159: Rank: 0 split: TEST Label files:
['data/CIFAR-100-dataset/test']
INFO 2022-05-15 13:29:51,512 disk_dataset.py:  86: Loaded 10000 samples from folder data/CIFAR-100-dataset/test
INFO 2022-05-15 13:29:51,513 ssl_dataset.py: 156: Rank: 0 split: TRAIN Data files:
['data/CIFAR-100-dataset/train']
INFO 2022-05-15 13:29:51,514 ssl_dataset.py: 159: Rank: 0 split: TRAIN Label files:
['data/CIFAR-100-dataset/train']
INFO 2022-05-15 13:29:52,661 disk_dataset.py:  86: Loaded 50000 samples from folder data/CIFAR-100-dataset/train
INFO 2022-05-15 13:29:52,662 misc.py: 161: Set start method of multiprocessing to forkserver
INFO 2022-05-15 13:29:52,663 __init__.py: 126: Created the Distributed Sampler....
INFO 2022-05-15 13:29:52,663 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 0, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 13:29:52,664 __init__.py: 215: Wrapping the dataloader to async device copies
INFO 2022-05-15 13:29:52,665 misc.py: 161: Set start method of multiprocessing to forkserver
INFO 2022-05-15 13:29:52,668 __init__.py: 126: Created the Distributed Sampler....
INFO 2022-05-15 13:29:52,669 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 0, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 13:29:52,670 __init__.py: 215: Wrapping the dataloader to async device copies
INFO 2022-05-15 13:29:52,671 train_task.py: 384: Building loss...
INFO 2022-05-15 13:29:52,672 trainer_main.py: 268: Training 100 epochs
INFO 2022-05-15 13:29:52,673 trainer_main.py: 269: One epoch = 98 iterations.
INFO 2022-05-15 13:29:52,674 trainer_main.py: 270: Total 50000 samples in one epoch
INFO 2022-05-15 13:29:52,674 trainer_main.py: 276: Total 9800 iterations for training
INFO 2022-05-15 13:29:52,877 logger.py:  84: Sun May 15 13:29:52 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |
| N/A   34C    P0    68W / 300W |  11495MiB / 16160MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      6745      C   python                          10295MiB |
|    0   N/A  N/A      8678      C   python                           1197MiB |
+-----------------------------------------------------------------------------+

INFO 2022-05-15 13:29:52,880 trainer_main.py: 173: Model is:
 Classy <class 'vissl.models.base_ssl_model.BaseSSLMultiInputOutputModel'>:
BaseSSLMultiInputOutputModel(
  (_heads): ModuleDict()
  (trunk): FeatureExtractorModel(
    (base_model): ResNeXt(
      (_feature_blocks): ModuleDict(
        (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv1_relu): ReLU(inplace=True)
        (maxpool): Identity()
        (layer1): Sequential(
          (0): BasicBlock(
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (1): BasicBlock(
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (layer2): Sequential(
          (0): BasicBlock(
            (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (downsample): Sequential(
              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): BasicBlock(
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (layer3): Sequential(
          (0): BasicBlock(
            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (downsample): Sequential(
              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): BasicBlock(
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (layer4): Sequential(
          (0): BasicBlock(
            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(<SUPPORTED_L4_STRIDE.two: 2>, <SUPPORTED_L4_STRIDE.two: 2>), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (downsample): Sequential(
              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(<SUPPORTED_L4_STRIDE.two: 2>, <SUPPORTED_L4_STRIDE.two: 2>), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (1): BasicBlock(
            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
        (flatten): Flatten()
      )
    )
    (feature_pool_ops): ModuleList(
      (0): AdaptiveAvgPool2d(output_size=[1, 1])
    )
  )
  (heads): ModuleList(
    (0): LinearEvalMLP(
      (channel_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (clf): MLP(
        (clf): Sequential(
          (0): Linear(in_features=512, out_features=100, bias=True)
        )
      )
    )
  )
)
INFO 2022-05-15 13:29:52,880 trainer_main.py: 174: Loss is: CrossEntropyMultipleOutputSingleTargetLoss(
  (criterion): CrossEntropyMultipleOutputSingleTargetCriterion(
    (_losses): ModuleList()
  )
)
INFO 2022-05-15 13:29:52,881 trainer_main.py: 175: Starting training....
INFO 2022-05-15 13:29:52,882 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 0, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 13:31:25,063 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 13:31:25,065 log_hooks.py:  76: ========= Memory Summary at on_phase_start =======
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   50046 KB |   50046 KB |   50046 KB |     512 B  |
|       from large pool |   46464 KB |   46464 KB |   46464 KB |       0 B  |
|       from small pool |    3582 KB |    3582 KB |    3582 KB |     512 B  |
|---------------------------------------------------------------------------|
| Active memory         |   50046 KB |   50046 KB |   50046 KB |     512 B  |
|       from large pool |   46464 KB |   46464 KB |   46464 KB |       0 B  |
|       from small pool |    3582 KB |    3582 KB |    3582 KB |     512 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   88064 KB |   88064 KB |   88064 KB |       0 B  |
|       from large pool |   81920 KB |   81920 KB |   81920 KB |       0 B  |
|       from small pool |    6144 KB |    6144 KB |    6144 KB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   38018 KB |   38026 KB |   63796 KB |   25778 KB |
|       from large pool |   35456 KB |   35456 KB |   56192 KB |   20736 KB |
|       from small pool |    2562 KB |    2570 KB |    7604 KB |    5042 KB |
|---------------------------------------------------------------------------|
| Allocations           |     131    |     131    |     132    |       1    |
|       from large pool |       9    |       9    |       9    |       0    |
|       from small pool |     122    |     122    |     123    |       1    |
|---------------------------------------------------------------------------|
| Active allocs         |     131    |     131    |     132    |       1    |
|       from large pool |       9    |       9    |       9    |       0    |
|       from small pool |     122    |     122    |     123    |       1    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       7    |       7    |       7    |       0    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       3    |       3    |       3    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       7    |       7    |       8    |       1    |
|       from large pool |       4    |       4    |       4    |       0    |
|       from small pool |       3    |       3    |       4    |       1    |
|===========================================================================|


INFO 2022-05-15 13:31:25,065 state_update_hooks.py: 115: Starting phase 0 [train]
INFO 2022-05-15 13:31:26,874 log_hooks.py:  76: ========= Memory Summary at on_forward =======
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   58454 KB |    2278 MB |   13592 MB |   13535 MB |
|       from large pool |   52608 KB |    2275 MB |   13585 MB |   13533 MB |
|       from small pool |    5846 KB |       5 MB |       7 MB |       1 MB |
|---------------------------------------------------------------------------|
| Active memory         |   58454 KB |    2278 MB |   13592 MB |   13535 MB |
|       from large pool |   52608 KB |    2275 MB |   13585 MB |   13533 MB |
|       from small pool |    5846 KB |       5 MB |       7 MB |       1 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  331776 KB |    3450 MB |    6114 MB |    5790 MB |
|       from large pool |  323584 KB |    3444 MB |    6104 MB |    5788 MB |
|       from small pool |    8192 KB |       8 MB |      10 MB |       2 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   31658 KB |    2165 MB |    9057 MB |    9026 MB |
|       from large pool |   29312 KB |    2162 MB |    9042 MB |    9014 MB |
|       from small pool |    2346 KB |       4 MB |      14 MB |      12 MB |
|---------------------------------------------------------------------------|
| Allocations           |     140    |     140    |     213    |      73    |
|       from large pool |      10    |      15    |      70    |      60    |
|       from small pool |     130    |     130    |     143    |      13    |
|---------------------------------------------------------------------------|
| Active allocs         |     140    |     140    |     213    |      73    |
|       from large pool |      10    |      15    |      70    |      60    |
|       from small pool |     130    |     130    |     143    |      13    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      10    |      11    |      15    |       5    |
|       from large pool |       6    |       7    |      10    |       4    |
|       from small pool |       4    |       4    |       5    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |      11    |      43    |      35    |
|       from large pool |       4    |       8    |      33    |      29    |
|       from small pool |       4    |       5    |      10    |       6    |
|===========================================================================|


INFO 2022-05-15 13:31:26,963 log_hooks.py:  76: ========= Memory Summary at on_backward =======
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   56607 KB |    2278 MB |   13596 MB |   13540 MB |
|       from large pool |   52608 KB |    2275 MB |   13585 MB |   13533 MB |
|       from small pool |    3999 KB |       7 MB |      11 MB |       7 MB |
|---------------------------------------------------------------------------|
| Active memory         |   56607 KB |    2278 MB |   13596 MB |   13540 MB |
|       from large pool |   52608 KB |    2275 MB |   13585 MB |   13533 MB |
|       from small pool |    3999 KB |       7 MB |      11 MB |       7 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  333824 KB |    3450 MB |    6116 MB |    5790 MB |
|       from large pool |  323584 KB |    3444 MB |    6104 MB |    5788 MB |
|       from small pool |   10240 KB |      10 MB |      12 MB |       2 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   33504 KB |    2165 MB |    9067 MB |    9034 MB |
|       from large pool |   29312 KB |    2162 MB |    9042 MB |    9014 MB |
|       from small pool |    4192 KB |       5 MB |      24 MB |      20 MB |
|---------------------------------------------------------------------------|
| Allocations           |     142    |     148    |     240    |      98    |
|       from large pool |      10    |      15    |      70    |      60    |
|       from small pool |     132    |     138    |     170    |      38    |
|---------------------------------------------------------------------------|
| Active allocs         |     142    |     148    |     240    |      98    |
|       from large pool |      10    |      15    |      70    |      60    |
|       from small pool |     132    |     138    |     170    |      38    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      16    |       5    |
|       from large pool |       6    |       7    |      10    |       4    |
|       from small pool |       5    |       5    |       6    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      11    |      11    |      58    |      47    |
|       from large pool |       4    |       8    |      33    |      29    |
|       from small pool |       7    |       7    |      25    |      18    |
|===========================================================================|


INFO 2022-05-15 13:31:26,966 log_hooks.py:  76: ========= Memory Summary at on_update =======
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   56607 KB |    2278 MB |   13596 MB |   13540 MB |
|       from large pool |   52608 KB |    2275 MB |   13585 MB |   13533 MB |
|       from small pool |    3999 KB |       7 MB |      11 MB |       7 MB |
|---------------------------------------------------------------------------|
| Active memory         |   56607 KB |    2278 MB |   13596 MB |   13540 MB |
|       from large pool |   52608 KB |    2275 MB |   13585 MB |   13533 MB |
|       from small pool |    3999 KB |       7 MB |      11 MB |       7 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  333824 KB |    3450 MB |    6116 MB |    5790 MB |
|       from large pool |  323584 KB |    3444 MB |    6104 MB |    5788 MB |
|       from small pool |   10240 KB |      10 MB |      12 MB |       2 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   33504 KB |    2165 MB |    9067 MB |    9034 MB |
|       from large pool |   29312 KB |    2162 MB |    9042 MB |    9014 MB |
|       from small pool |    4192 KB |       5 MB |      24 MB |      20 MB |
|---------------------------------------------------------------------------|
| Allocations           |     142    |     148    |     240    |      98    |
|       from large pool |      10    |      15    |      70    |      60    |
|       from small pool |     132    |     138    |     170    |      38    |
|---------------------------------------------------------------------------|
| Active allocs         |     142    |     148    |     240    |      98    |
|       from large pool |      10    |      15    |      70    |      60    |
|       from small pool |     132    |     138    |     170    |      38    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      11    |      11    |      16    |       5    |
|       from large pool |       6    |       7    |      10    |       4    |
|       from small pool |       5    |       5    |       6    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      11    |      11    |      58    |      47    |
|       from large pool |       4    |       8    |      33    |      29    |
|       from small pool |       7    |       7    |      25    |      18    |
|===========================================================================|


INFO 2022-05-15 13:31:26,967 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 0; lr: 0.3; loss: 4.76401; btime(ms): 0; eta: 0:00:00; peak_mem(M): 2278;
INFO 2022-05-15 13:31:27,042 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 1; lr: 0.3; loss: 4.42725; btime(ms): 94296; eta: 10 days, 16:40:09; peak_mem(M): 2278; max_iterations: 9800;
INFO 2022-05-15 13:31:28,779 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 5; lr: 0.3; loss: 4.06431; btime(ms): 19211; eta: 2 days, 4:16:19; peak_mem(M): 2278;
INFO 2022-05-15 13:31:32,372 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 10; lr: 0.3; loss: 4.07076; btime(ms): 9965; eta: 1 day, 3:06:01; peak_mem(M): 2278;
INFO 2022-05-15 13:31:38,852 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 15; lr: 0.3; loss: 3.94189; btime(ms): 6880; eta: 18:42:06; peak_mem(M): 2278;
INFO 2022-05-15 13:31:42,005 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 20; lr: 0.3; loss: 3.97727; btime(ms): 5464; eta: 14:50:41; peak_mem(M): 2278;
INFO 2022-05-15 13:31:45,398 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 25; lr: 0.3; loss: 3.88432; btime(ms): 4507; eta: 12:14:21; peak_mem(M): 2278;
INFO 2022-05-15 13:31:48,600 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 30; lr: 0.3; loss: 3.79735; btime(ms): 3863; eta: 10:29:01; peak_mem(M): 2278;
INFO 2022-05-15 13:31:55,085 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 35; lr: 0.3; loss: 3.64212; btime(ms): 3408; eta: 9:14:40; peak_mem(M): 2278;
INFO 2022-05-15 13:31:58,569 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 40; lr: 0.3; loss: 3.65043; btime(ms): 3146; eta: 8:31:49; peak_mem(M): 2278;
INFO 2022-05-15 13:32:01,883 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 45; lr: 0.3; loss: 3.6332; btime(ms): 2870; eta: 7:46:42; peak_mem(M): 2278;
INFO 2022-05-15 13:32:05,399 logger.py:  84: Sun May 15 13:32:05 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |
| N/A   37C    P0    70W / 300W |  12525MiB / 16160MiB |     56%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      6745      C   python                          10295MiB |
|    0   N/A  N/A      8678      C   python                           2227MiB |
+-----------------------------------------------------------------------------+

INFO 2022-05-15 13:32:05,440 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 50; lr: 0.3; loss: 3.66986; btime(ms): 2650; eta: 7:10:44; peak_mem(M): 2278;
INFO 2022-05-15 13:32:11,466 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 55; lr: 0.3; loss: 3.54826; btime(ms): 2467; eta: 6:40:47; peak_mem(M): 2278;
INFO 2022-05-15 13:32:14,908 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 60; lr: 0.3; loss: 3.53863; btime(ms): 2369; eta: 6:24:42; peak_mem(M): 2278;
INFO 2022-05-15 13:32:18,255 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 65; lr: 0.3; loss: 3.61192; btime(ms): 2239; eta: 6:03:17; peak_mem(M): 2278;
INFO 2022-05-15 13:32:21,587 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 70; lr: 0.3; loss: 3.60226; btime(ms): 2126; eta: 5:44:53; peak_mem(M): 2278;
INFO 2022-05-15 13:32:28,177 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 75; lr: 0.3; loss: 3.57502; btime(ms): 2030; eta: 5:29:02; peak_mem(M): 2278;
INFO 2022-05-15 13:32:31,401 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 80; lr: 0.3; loss: 3.56959; btime(ms): 1983; eta: 5:21:20; peak_mem(M): 2278;
INFO 2022-05-15 13:32:34,856 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 85; lr: 0.3; loss: 3.47361; btime(ms): 1907; eta: 5:08:52; peak_mem(M): 2278;
INFO 2022-05-15 13:32:38,280 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 90; lr: 0.3; loss: 3.42611; btime(ms): 1839; eta: 4:57:42; peak_mem(M): 2278;
INFO 2022-05-15 13:32:45,210 log_hooks.py: 277: Rank: 0; [ep: 0] iter: 95; lr: 0.3; loss: 3.46714; btime(ms): 1780; eta: 4:48:00; peak_mem(M): 2278;
INFO 2022-05-15 13:32:45,584 trainer_main.py: 214: Meters synced
INFO 2022-05-15 13:32:45,588 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 821
INFO 2022-05-15 13:32:45,588 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  753.52 ms  753.58 ms
             forward:   27.62 ms   54.25 ms
        loss_compute:    0.92 ms    0.97 ms
     loss_all_reduce:    0.11 ms    0.13 ms
       meters_update:    6.18 ms    6.20 ms
            backward:    1.90 ms    1.93 ms
      optimizer_step:    0.39 ms    0.38 ms
    train_step_total:  819.43 ms  819.44 ms
INFO 2022-05-15 13:32:45,590 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 14.188}, 'top_5': {'res5': 37.134}}
INFO 2022-05-15 13:32:45,590 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 13:32:45,599 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 13:32:45,599 log_hooks.py: 425: [phase: 0] Saving checkpoint to ./checkpoints/LP/2
INFO 2022-05-15 13:32:45,747 checkpoint.py: 131: Saved checkpoint: ./checkpoints/LP/2/model_phase0.torch
INFO 2022-05-15 13:32:45,748 checkpoint.py: 140: Creating symlink...
INFO 2022-05-15 13:32:45,754 checkpoint.py: 144: Created symlink: ./checkpoints/LP/2/checkpoint.torch
INFO 2022-05-15 13:32:45,755 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 1, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 13:34:21,509 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 13:34:21,509 state_update_hooks.py: 115: Starting phase 1 [test]
INFO 2022-05-15 13:34:37,601 trainer_main.py: 214: Meters synced
INFO 2022-05-15 13:34:37,607 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 804
INFO 2022-05-15 13:34:37,609 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 18.17}, 'top_5': {'res5': 43.01}}
INFO 2022-05-15 13:34:37,609 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 13:34:37,618 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 13:34:37,619 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 2, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 13:36:09,732 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 13:36:09,733 state_update_hooks.py: 115: Starting phase 2 [train]
INFO 2022-05-15 13:36:09,990 log_hooks.py: 277: Rank: 0; [ep: 1] iter: 100; lr: 0.3; loss: 3.36396; btime(ms): 3143; eta: 8:28:10; peak_mem(M): 2605;
INFO 2022-05-15 13:37:41,516 trainer_main.py: 214: Meters synced
INFO 2022-05-15 13:37:41,522 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 936
INFO 2022-05-15 13:37:41,524 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  885.80 ms  885.86 ms
             forward:    5.98 ms   35.07 ms
        loss_compute:    0.73 ms    0.81 ms
     loss_all_reduce:    0.11 ms    0.11 ms
       meters_update:   10.55 ms   10.58 ms
            backward:    1.55 ms    1.61 ms
      optimizer_step:    0.47 ms    0.46 ms
    train_step_total:  936.34 ms  936.34 ms
INFO 2022-05-15 13:37:41,525 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 20.916}, 'top_5': {'res5': 47.382000000000005}}
INFO 2022-05-15 13:37:41,526 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 13:37:41,532 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 13:37:41,533 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 3, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 13:39:12,633 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 13:39:12,634 state_update_hooks.py: 115: Starting phase 3 [test]
INFO 2022-05-15 13:39:26,197 trainer_main.py: 214: Meters synced
INFO 2022-05-15 13:39:26,205 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 678
INFO 2022-05-15 13:39:26,207 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 20.990000000000002}, 'top_5': {'res5': 48.07}}
INFO 2022-05-15 13:39:26,207 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 13:39:26,214 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 13:39:26,215 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 4, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 13:40:56,512 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 13:40:56,513 state_update_hooks.py: 115: Starting phase 4 [train]
INFO 2022-05-15 13:41:00,389 log_hooks.py: 277: Rank: 0; [ep: 2] iter: 200; lr: 0.3; loss: 3.27522; btime(ms): 2781; eta: 7:25:04; peak_mem(M): 2605;
INFO 2022-05-15 13:42:26,339 trainer_main.py: 214: Meters synced
INFO 2022-05-15 13:42:26,343 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 916
INFO 2022-05-15 13:42:26,345 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  861.73 ms  861.81 ms
             forward:    5.90 ms   36.70 ms
        loss_compute:    0.88 ms    1.08 ms
     loss_all_reduce:    0.16 ms    0.16 ms
       meters_update:   12.31 ms   12.33 ms
            backward:    1.49 ms    1.69 ms
      optimizer_step:    0.52 ms    0.52 ms
    train_step_total:  916.37 ms  916.38 ms
INFO 2022-05-15 13:42:26,346 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 23.578}, 'top_5': {'res5': 50.526}}
INFO 2022-05-15 13:42:26,346 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 13:42:26,355 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 13:42:26,356 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 5, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 13:43:58,815 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 13:43:58,817 state_update_hooks.py: 115: Starting phase 5 [test]
INFO 2022-05-15 13:44:14,655 trainer_main.py: 214: Meters synced
INFO 2022-05-15 13:44:14,660 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 792
INFO 2022-05-15 13:44:14,661 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 23.73}, 'top_5': {'res5': 49.53}}
INFO 2022-05-15 13:44:14,662 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 13:44:14,673 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 13:44:14,674 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 6, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 13:45:46,389 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 13:45:46,390 state_update_hooks.py: 115: Starting phase 6 [train]
INFO 2022-05-15 13:47:09,566 trainer_main.py: 214: Meters synced
INFO 2022-05-15 13:47:09,570 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 848
INFO 2022-05-15 13:47:09,571 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  806.34 ms  806.39 ms
             forward:    6.18 ms   32.20 ms
        loss_compute:    0.78 ms    0.79 ms
     loss_all_reduce:    0.11 ms    0.12 ms
       meters_update:    4.78 ms    4.80 ms
            backward:    2.15 ms    2.16 ms
      optimizer_step:    0.40 ms    0.41 ms
    train_step_total:  848.52 ms  848.53 ms
INFO 2022-05-15 13:47:09,572 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 25.234}, 'top_5': {'res5': 52.672}}
INFO 2022-05-15 13:47:09,573 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 13:47:09,582 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 13:47:09,583 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 7, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 13:48:39,491 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 13:48:39,492 state_update_hooks.py: 115: Starting phase 7 [test]
INFO 2022-05-15 13:48:55,181 trainer_main.py: 214: Meters synced
INFO 2022-05-15 13:48:55,186 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 784
INFO 2022-05-15 13:48:55,187 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 23.79}, 'top_5': {'res5': 51.67}}
INFO 2022-05-15 13:48:55,188 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 13:48:55,196 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 13:48:55,197 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 8, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 13:50:25,275 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 13:50:25,277 state_update_hooks.py: 115: Starting phase 8 [train]
INFO 2022-05-15 13:50:32,504 log_hooks.py: 277: Rank: 0; [ep: 4] iter: 400; lr: 0.3; loss: 2.91355; btime(ms): 2582; eta: 6:44:39; peak_mem(M): 2605;
INFO 2022-05-15 13:51:56,150 trainer_main.py: 214: Meters synced
INFO 2022-05-15 13:51:56,157 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 927
INFO 2022-05-15 13:51:56,158 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  874.08 ms  874.14 ms
             forward:    5.77 ms   36.04 ms
        loss_compute:    0.84 ms    1.02 ms
     loss_all_reduce:    0.11 ms    0.10 ms
       meters_update:   12.00 ms   12.02 ms
            backward:    1.26 ms    1.39 ms
      optimizer_step:    0.52 ms    0.52 ms
    train_step_total:  927.08 ms  927.08 ms
INFO 2022-05-15 13:51:56,163 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 26.71}, 'top_5': {'res5': 54.508}}
INFO 2022-05-15 13:51:56,163 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 13:51:56,172 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 13:51:56,173 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 9, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 13:53:25,426 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 13:53:25,427 state_update_hooks.py: 115: Starting phase 9 [test]
INFO 2022-05-15 13:53:38,865 trainer_main.py: 214: Meters synced
INFO 2022-05-15 13:53:38,869 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 672
INFO 2022-05-15 13:53:38,871 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 25.55}, 'top_5': {'res5': 53.38}}
INFO 2022-05-15 13:53:38,871 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 13:53:38,878 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 13:53:38,879 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 10, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 13:55:07,114 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 13:55:07,114 state_update_hooks.py: 115: Starting phase 10 [train]
INFO 2022-05-15 13:56:41,013 trainer_main.py: 214: Meters synced
INFO 2022-05-15 13:56:41,018 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 958
INFO 2022-05-15 13:56:41,019 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  904.76 ms  904.76 ms
             forward:    6.08 ms   38.14 ms
        loss_compute:    1.05 ms    1.29 ms
     loss_all_reduce:    0.11 ms    0.12 ms
       meters_update:    9.11 ms    9.13 ms
            backward:    1.76 ms    1.91 ms
      optimizer_step:    0.47 ms    0.44 ms
    train_step_total:  957.94 ms  957.95 ms
INFO 2022-05-15 13:56:41,021 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 27.538}, 'top_5': {'res5': 55.33}}
INFO 2022-05-15 13:56:41,022 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 13:56:41,033 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 13:56:41,034 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 11, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 13:58:13,676 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 13:58:13,677 state_update_hooks.py: 115: Starting phase 11 [test]
INFO 2022-05-15 13:58:28,969 trainer_main.py: 214: Meters synced
INFO 2022-05-15 13:58:28,974 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 764
INFO 2022-05-15 13:58:28,976 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 25.11}, 'top_5': {'res5': 53.349999999999994}}
INFO 2022-05-15 13:58:28,977 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 13:58:28,987 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 13:58:28,991 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 12, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 13:59:59,843 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 13:59:59,844 state_update_hooks.py: 115: Starting phase 12 [train]
INFO 2022-05-15 14:00:10,139 log_hooks.py: 277: Rank: 0; [ep: 6] iter: 600; lr: 0.3; loss: 3.04653; btime(ms): 2524; eta: 6:27:02; peak_mem(M): 2605;
INFO 2022-05-15 14:01:24,963 trainer_main.py: 214: Meters synced
INFO 2022-05-15 14:01:24,968 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 868
INFO 2022-05-15 14:01:24,969 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  826.34 ms  826.39 ms
             forward:    6.26 ms   32.15 ms
        loss_compute:    0.79 ms    0.80 ms
     loss_all_reduce:    0.11 ms    0.11 ms
       meters_update:    4.82 ms    4.84 ms
            backward:    2.05 ms    2.06 ms
      optimizer_step:    0.39 ms    0.39 ms
    train_step_total:  868.34 ms  868.34 ms
INFO 2022-05-15 14:01:24,970 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 28.571999999999996}, 'top_5': {'res5': 56.477999999999994}}
INFO 2022-05-15 14:01:24,971 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:01:24,978 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:01:24,980 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 13, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 14:02:56,705 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 14:02:56,707 state_update_hooks.py: 115: Starting phase 13 [test]
INFO 2022-05-15 14:03:10,532 trainer_main.py: 214: Meters synced
INFO 2022-05-15 14:03:10,537 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 691
INFO 2022-05-15 14:03:10,538 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 26.619999999999997}, 'top_5': {'res5': 54.37}}
INFO 2022-05-15 14:03:10,539 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:03:10,548 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:03:10,549 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 14, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 14:04:34,135 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 14:04:34,136 state_update_hooks.py: 115: Starting phase 14 [train]
INFO 2022-05-15 14:06:03,412 trainer_main.py: 214: Meters synced
INFO 2022-05-15 14:06:03,416 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 911
INFO 2022-05-15 14:06:03,417 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  856.18 ms  856.24 ms
             forward:    5.70 ms   36.81 ms
        loss_compute:    0.91 ms    1.12 ms
     loss_all_reduce:    0.12 ms    0.11 ms
       meters_update:   12.41 ms   12.47 ms
            backward:    1.16 ms    1.32 ms
      optimizer_step:    0.54 ms    0.55 ms
    train_step_total:  910.77 ms  910.75 ms
INFO 2022-05-15 14:06:03,418 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 29.028}, 'top_5': {'res5': 57.269999999999996}}
INFO 2022-05-15 14:06:03,419 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:06:03,425 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:06:03,426 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 15, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 14:07:33,934 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 14:07:33,936 state_update_hooks.py: 115: Starting phase 15 [test]
INFO 2022-05-15 14:07:47,634 trainer_main.py: 214: Meters synced
INFO 2022-05-15 14:07:47,639 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 685
INFO 2022-05-15 14:07:47,639 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 26.889999999999997}, 'top_5': {'res5': 55.55}}
INFO 2022-05-15 14:07:47,640 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:07:47,646 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:07:47,647 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 16, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 14:09:22,667 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 14:09:22,668 state_update_hooks.py: 115: Starting phase 16 [train]
INFO 2022-05-15 14:09:38,187 log_hooks.py: 277: Rank: 0; [ep: 8] iter: 800; lr: 0.3; loss: 2.89314; btime(ms): 2484; eta: 6:12:43; peak_mem(M): 2605;
INFO 2022-05-15 14:10:49,134 trainer_main.py: 214: Meters synced
INFO 2022-05-15 14:10:49,138 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 882
INFO 2022-05-15 14:10:49,140 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  834.90 ms  835.00 ms
             forward:    6.04 ms   34.57 ms
        loss_compute:    0.80 ms    0.90 ms
     loss_all_reduce:    0.12 ms    0.11 ms
       meters_update:    7.51 ms    7.53 ms
            backward:    1.67 ms    1.77 ms
      optimizer_step:    0.46 ms    0.49 ms
    train_step_total:  882.09 ms  882.11 ms
INFO 2022-05-15 14:10:49,140 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 29.528}, 'top_5': {'res5': 58.36}}
INFO 2022-05-15 14:10:49,141 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:10:49,149 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:10:49,151 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 17, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 14:12:22,069 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 14:12:22,069 state_update_hooks.py: 115: Starting phase 17 [test]
INFO 2022-05-15 14:12:37,904 trainer_main.py: 214: Meters synced
INFO 2022-05-15 14:12:37,910 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 792
INFO 2022-05-15 14:12:37,911 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 27.27}, 'top_5': {'res5': 55.48}}
INFO 2022-05-15 14:12:37,912 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:12:37,920 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:12:37,921 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 18, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 14:14:16,996 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 14:14:16,997 state_update_hooks.py: 115: Starting phase 18 [train]
INFO 2022-05-15 14:15:42,989 trainer_main.py: 214: Meters synced
INFO 2022-05-15 14:15:42,994 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 877
INFO 2022-05-15 14:15:42,995 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  829.35 ms  829.49 ms
             forward:    6.65 ms   34.99 ms
        loss_compute:    0.90 ms    0.97 ms
     loss_all_reduce:    0.11 ms    0.11 ms
       meters_update:    7.35 ms    7.37 ms
            backward:    1.84 ms    1.92 ms
      optimizer_step:    0.49 ms    0.46 ms
    train_step_total:  877.23 ms  877.25 ms
INFO 2022-05-15 14:15:42,996 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 30.186}, 'top_5': {'res5': 58.852000000000004}}
INFO 2022-05-15 14:15:42,997 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:15:43,006 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:15:43,007 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 19, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 14:17:15,158 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 14:17:15,159 state_update_hooks.py: 115: Starting phase 19 [test]
INFO 2022-05-15 14:17:28,644 trainer_main.py: 214: Meters synced
INFO 2022-05-15 14:17:28,649 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 674
INFO 2022-05-15 14:17:28,650 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 27.62}, 'top_5': {'res5': 55.7}}
INFO 2022-05-15 14:17:28,650 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:17:28,659 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:17:28,660 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 20, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 14:19:02,566 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 14:19:02,568 state_update_hooks.py: 115: Starting phase 20 [train]
INFO 2022-05-15 14:19:22,261 log_hooks.py: 277: Rank: 0; [ep: 10] iter: 1000; lr: 0.3; loss: 2.74643; btime(ms): 2474; eta: 6:02:56; peak_mem(M): 2605;
INFO 2022-05-15 14:20:34,597 trainer_main.py: 214: Meters synced
INFO 2022-05-15 14:20:34,604 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 939
INFO 2022-05-15 14:20:34,605 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  884.09 ms  884.19 ms
             forward:    5.47 ms   36.10 ms
        loss_compute:    0.83 ms    1.00 ms
     loss_all_reduce:    0.16 ms    0.15 ms
       meters_update:   13.27 ms   13.31 ms
            backward:    1.26 ms    1.45 ms
      optimizer_step:    0.61 ms    0.61 ms
    train_step_total:  938.84 ms  938.87 ms
INFO 2022-05-15 14:20:34,605 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 30.756}, 'top_5': {'res5': 59.336}}
INFO 2022-05-15 14:20:34,606 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:20:34,614 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:20:34,627 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 21, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 14:22:09,842 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 14:22:09,843 state_update_hooks.py: 115: Starting phase 21 [test]
INFO 2022-05-15 14:22:25,088 trainer_main.py: 214: Meters synced
INFO 2022-05-15 14:22:25,115 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 763
INFO 2022-05-15 14:22:25,116 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 27.389999999999997}, 'top_5': {'res5': 56.120000000000005}}
INFO 2022-05-15 14:22:25,116 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:22:25,125 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:22:25,126 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 22, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 14:24:03,425 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 14:24:03,426 state_update_hooks.py: 115: Starting phase 22 [train]
INFO 2022-05-15 14:25:29,240 trainer_main.py: 214: Meters synced
INFO 2022-05-15 14:25:29,245 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 875
INFO 2022-05-15 14:25:29,246 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  833.27 ms  833.30 ms
             forward:    6.02 ms   32.22 ms
        loss_compute:    0.80 ms    0.82 ms
     loss_all_reduce:    0.10 ms    0.10 ms
       meters_update:    5.23 ms    5.25 ms
            backward:    1.72 ms    1.74 ms
      optimizer_step:    0.42 ms    0.42 ms
    train_step_total:  875.44 ms  875.45 ms
INFO 2022-05-15 14:25:29,247 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 31.19}, 'top_5': {'res5': 59.831999999999994}}
INFO 2022-05-15 14:25:29,248 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:25:29,256 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:25:29,257 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 23, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 14:27:06,058 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 14:27:06,059 state_update_hooks.py: 115: Starting phase 23 [test]
INFO 2022-05-15 14:27:22,509 trainer_main.py: 214: Meters synced
INFO 2022-05-15 14:27:22,514 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 822
INFO 2022-05-15 14:27:22,517 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 28.84}, 'top_5': {'res5': 57.07}}
INFO 2022-05-15 14:27:22,517 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:27:22,527 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:27:22,528 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 24, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 14:28:54,229 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 14:28:54,230 state_update_hooks.py: 115: Starting phase 24 [train]
INFO 2022-05-15 14:29:14,280 log_hooks.py: 277: Rank: 0; [ep: 12] iter: 1200; lr: 0.3; loss: 2.63166; btime(ms): 2473; eta: 5:54:30; peak_mem(M): 2605;
INFO 2022-05-15 14:30:21,679 trainer_main.py: 214: Meters synced
INFO 2022-05-15 14:30:21,684 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 892
INFO 2022-05-15 14:30:21,685 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  839.42 ms  839.47 ms
             forward:    6.38 ms   36.36 ms
        loss_compute:    0.86 ms    1.02 ms
     loss_all_reduce:    0.12 ms    0.11 ms
       meters_update:   11.00 ms   11.02 ms
            backward:    1.58 ms    1.68 ms
      optimizer_step:    0.45 ms    0.47 ms
    train_step_total:  892.12 ms  892.12 ms
INFO 2022-05-15 14:30:21,686 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 31.524}, 'top_5': {'res5': 60.36}}
INFO 2022-05-15 14:30:21,687 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:30:21,695 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:30:21,697 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 25, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 14:31:55,525 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 14:31:55,526 state_update_hooks.py: 115: Starting phase 25 [test]
INFO 2022-05-15 14:32:09,886 trainer_main.py: 214: Meters synced
INFO 2022-05-15 14:32:09,890 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 718
INFO 2022-05-15 14:32:09,891 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 28.470000000000002}, 'top_5': {'res5': 56.720000000000006}}
INFO 2022-05-15 14:32:09,892 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:32:09,899 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:32:09,900 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 26, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 14:33:44,769 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 14:33:44,770 state_update_hooks.py: 115: Starting phase 26 [train]
INFO 2022-05-15 14:35:16,794 trainer_main.py: 214: Meters synced
INFO 2022-05-15 14:35:16,799 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 939
INFO 2022-05-15 14:35:16,800 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  888.49 ms  888.55 ms
             forward:    5.53 ms   35.52 ms
        loss_compute:    0.88 ms    1.02 ms
     loss_all_reduce:    0.11 ms    0.10 ms
       meters_update:    9.46 ms    9.48 ms
            backward:    1.48 ms    1.64 ms
      optimizer_step:    0.51 ms    0.55 ms
    train_step_total:  938.81 ms  938.84 ms
INFO 2022-05-15 14:35:16,801 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 31.712}, 'top_5': {'res5': 60.788}}
INFO 2022-05-15 14:35:16,802 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:35:16,812 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:35:16,813 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 27, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 14:36:43,830 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 14:36:43,832 state_update_hooks.py: 115: Starting phase 27 [test]
INFO 2022-05-15 14:36:58,607 trainer_main.py: 214: Meters synced
INFO 2022-05-15 14:36:58,611 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 738
INFO 2022-05-15 14:36:58,612 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 28.849999999999998}, 'top_5': {'res5': 57.24}}
INFO 2022-05-15 14:36:58,613 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:36:58,622 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:36:58,623 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 28, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 14:38:33,901 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 14:38:33,903 state_update_hooks.py: 115: Starting phase 28 [train]
INFO 2022-05-15 14:38:56,863 log_hooks.py: 277: Rank: 0; [ep: 14] iter: 1400; lr: 0.3; loss: 2.84177; btime(ms): 2466; eta: 5:45:20; peak_mem(M): 2605;
INFO 2022-05-15 14:39:54,943 trainer_main.py: 214: Meters synced
INFO 2022-05-15 14:39:54,951 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 827
INFO 2022-05-15 14:39:54,951 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  783.43 ms  783.48 ms
             forward:    6.50 ms   32.91 ms
        loss_compute:    0.81 ms    0.81 ms
     loss_all_reduce:    0.11 ms    0.11 ms
       meters_update:    5.19 ms    5.22 ms
            backward:    2.02 ms    2.04 ms
      optimizer_step:    0.40 ms    0.41 ms
    train_step_total:  826.72 ms  826.73 ms
INFO 2022-05-15 14:39:54,953 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 32.352}, 'top_5': {'res5': 61.153999999999996}}
INFO 2022-05-15 14:39:54,953 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:39:54,963 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:39:54,965 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 29, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 14:41:30,992 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 14:41:30,992 state_update_hooks.py: 115: Starting phase 29 [test]
INFO 2022-05-15 14:41:46,307 trainer_main.py: 214: Meters synced
INFO 2022-05-15 14:41:46,312 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 765
INFO 2022-05-15 14:41:46,313 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 28.799999999999997}, 'top_5': {'res5': 57.089999999999996}}
INFO 2022-05-15 14:41:46,314 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:41:46,321 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:41:46,322 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 30, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 14:43:16,936 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 14:43:16,937 state_update_hooks.py: 115: Starting phase 30 [train]
INFO 2022-05-15 14:44:49,279 trainer_main.py: 214: Meters synced
INFO 2022-05-15 14:44:49,288 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 942
INFO 2022-05-15 14:44:49,289 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  887.28 ms  887.40 ms
             forward:    5.68 ms   36.83 ms
        loss_compute:    0.78 ms    0.96 ms
     loss_all_reduce:    0.11 ms    0.10 ms
       meters_update:   12.86 ms   12.90 ms
            backward:    1.32 ms    1.50 ms
      optimizer_step:    0.47 ms    0.47 ms
    train_step_total:  942.00 ms  941.98 ms
INFO 2022-05-15 14:44:49,290 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 32.192}, 'top_5': {'res5': 61.354}}
INFO 2022-05-15 14:44:49,291 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:44:49,300 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:44:49,301 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 31, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 14:46:15,312 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 14:46:15,313 state_update_hooks.py: 115: Starting phase 31 [test]
INFO 2022-05-15 14:46:28,920 trainer_main.py: 214: Meters synced
INFO 2022-05-15 14:46:28,925 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 680
INFO 2022-05-15 14:46:28,926 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 29.03}, 'top_5': {'res5': 57.230000000000004}}
INFO 2022-05-15 14:46:28,927 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:46:28,935 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:46:28,936 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 32, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 14:48:00,525 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 14:48:00,526 state_update_hooks.py: 115: Starting phase 32 [train]
INFO 2022-05-15 14:48:30,979 log_hooks.py: 277: Rank: 0; [ep: 16] iter: 1600; lr: 0.3; loss: 2.71582; btime(ms): 2457; eta: 5:35:50; peak_mem(M): 2605;
INFO 2022-05-15 14:49:25,284 trainer_main.py: 214: Meters synced
INFO 2022-05-15 14:49:25,288 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 864
INFO 2022-05-15 14:49:25,289 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  815.76 ms  815.80 ms
             forward:    6.33 ms   35.80 ms
        loss_compute:    0.87 ms    1.01 ms
     loss_all_reduce:    0.11 ms    0.12 ms
       meters_update:    7.79 ms    7.82 ms
            backward:    1.50 ms    1.60 ms
      optimizer_step:    0.43 ms    0.46 ms
    train_step_total:  864.65 ms  864.67 ms
INFO 2022-05-15 14:49:25,290 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 32.836}, 'top_5': {'res5': 61.734}}
INFO 2022-05-15 14:49:25,291 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:49:25,298 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:49:25,299 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 33, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 14:50:59,426 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 14:50:59,428 state_update_hooks.py: 115: Starting phase 33 [test]
INFO 2022-05-15 14:51:15,879 trainer_main.py: 214: Meters synced
INFO 2022-05-15 14:51:15,886 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 822
INFO 2022-05-15 14:51:15,887 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 29.21}, 'top_5': {'res5': 57.24}}
INFO 2022-05-15 14:51:15,888 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:51:15,896 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:51:15,897 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 34, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 14:52:51,881 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 14:52:51,882 state_update_hooks.py: 115: Starting phase 34 [train]
INFO 2022-05-15 14:54:18,604 trainer_main.py: 214: Meters synced
INFO 2022-05-15 14:54:18,610 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 884
INFO 2022-05-15 14:54:18,611 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  839.90 ms  840.00 ms
             forward:    6.07 ms   33.45 ms
        loss_compute:    0.78 ms    0.84 ms
     loss_all_reduce:    0.11 ms    0.11 ms
       meters_update:    6.35 ms    6.37 ms
            backward:    1.79 ms    1.83 ms
      optimizer_step:    0.45 ms    0.44 ms
    train_step_total:  884.70 ms  884.71 ms
INFO 2022-05-15 14:54:18,612 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 33.022}, 'top_5': {'res5': 62.150000000000006}}
INFO 2022-05-15 14:54:18,612 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:54:18,619 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:54:18,620 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 35, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 14:55:52,848 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 14:55:52,849 state_update_hooks.py: 115: Starting phase 35 [test]
INFO 2022-05-15 14:56:07,296 trainer_main.py: 214: Meters synced
INFO 2022-05-15 14:56:07,301 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 722
INFO 2022-05-15 14:56:07,302 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 29.110000000000003}, 'top_5': {'res5': 57.34}}
INFO 2022-05-15 14:56:07,302 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:56:07,310 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:56:07,310 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 36, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 14:57:36,085 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 14:57:36,086 state_update_hooks.py: 115: Starting phase 36 [train]
INFO 2022-05-15 14:58:09,706 log_hooks.py: 277: Rank: 0; [ep: 18] iter: 1800; lr: 0.3; loss: 2.59233; btime(ms): 2452; eta: 5:26:57; peak_mem(M): 2605;
INFO 2022-05-15 14:59:04,828 trainer_main.py: 214: Meters synced
INFO 2022-05-15 14:59:04,836 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 905
INFO 2022-05-15 14:59:04,837 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  847.94 ms  847.97 ms
             forward:    5.64 ms   37.71 ms
        loss_compute:    0.92 ms    1.19 ms
     loss_all_reduce:    0.12 ms    0.10 ms
       meters_update:   14.09 ms   14.12 ms
            backward:    1.28 ms    1.48 ms
      optimizer_step:    0.50 ms    0.55 ms
    train_step_total:  905.31 ms  905.31 ms
INFO 2022-05-15 14:59:04,838 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 33.256}, 'top_5': {'res5': 62.552}}
INFO 2022-05-15 14:59:04,839 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:59:04,847 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 14:59:04,848 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 37, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 15:00:34,394 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 15:00:34,396 state_update_hooks.py: 115: Starting phase 37 [test]
INFO 2022-05-15 15:00:48,815 trainer_main.py: 214: Meters synced
INFO 2022-05-15 15:00:48,820 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 721
INFO 2022-05-15 15:00:48,821 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 28.98}, 'top_5': {'res5': 57.550000000000004}}
INFO 2022-05-15 15:00:48,822 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:00:48,830 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:00:48,831 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 38, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 15:02:27,460 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 15:02:27,461 state_update_hooks.py: 115: Starting phase 38 [train]
INFO 2022-05-15 15:03:52,348 trainer_main.py: 214: Meters synced
INFO 2022-05-15 15:03:52,352 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 866
INFO 2022-05-15 15:03:52,353 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  820.37 ms  820.42 ms
             forward:    6.37 ms   33.54 ms
        loss_compute:    0.84 ms    0.90 ms
     loss_all_reduce:    0.12 ms    0.12 ms
       meters_update:    6.85 ms    6.87 ms
            backward:    1.80 ms    1.83 ms
      optimizer_step:    0.43 ms    0.46 ms
    train_step_total:  865.97 ms  865.98 ms
INFO 2022-05-15 15:03:52,354 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 33.5}, 'top_5': {'res5': 62.858000000000004}}
INFO 2022-05-15 15:03:52,355 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:03:52,363 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:03:52,364 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 39, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 15:05:26,739 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 15:05:26,740 state_update_hooks.py: 115: Starting phase 39 [test]
INFO 2022-05-15 15:05:43,523 trainer_main.py: 214: Meters synced
INFO 2022-05-15 15:05:43,528 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 839
INFO 2022-05-15 15:05:43,529 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 29.98}, 'top_5': {'res5': 57.589999999999996}}
INFO 2022-05-15 15:05:43,530 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:05:43,537 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:05:43,538 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 40, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 15:07:20,310 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 15:07:20,311 state_update_hooks.py: 115: Starting phase 40 [train]
INFO 2022-05-15 15:07:56,022 log_hooks.py: 277: Rank: 0; [ep: 20] iter: 2000; lr: 0.3; loss: 2.58838; btime(ms): 2451; eta: 5:18:40; peak_mem(M): 2605;
INFO 2022-05-15 15:08:49,629 trainer_main.py: 214: Meters synced
INFO 2022-05-15 15:08:49,635 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 911
INFO 2022-05-15 15:08:49,635 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  862.09 ms  862.14 ms
             forward:    6.32 ms   34.46 ms
        loss_compute:    0.80 ms    0.93 ms
     loss_all_reduce:    0.12 ms    0.12 ms
       meters_update:    9.33 ms    9.34 ms
            backward:    1.63 ms    1.69 ms
      optimizer_step:    0.46 ms    0.47 ms
    train_step_total:  911.19 ms  911.18 ms
INFO 2022-05-15 15:08:49,637 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 33.955999999999996}, 'top_5': {'res5': 62.844}}
INFO 2022-05-15 15:08:49,638 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:08:49,649 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:08:49,650 log_hooks.py: 425: [phase: 20] Saving checkpoint to ./checkpoints/LP/2
INFO 2022-05-15 15:08:49,813 checkpoint.py: 131: Saved checkpoint: ./checkpoints/LP/2/model_phase20.torch
INFO 2022-05-15 15:08:49,814 checkpoint.py: 140: Creating symlink...
INFO 2022-05-15 15:08:49,823 checkpoint.py: 144: Created symlink: ./checkpoints/LP/2/checkpoint.torch
INFO 2022-05-15 15:08:49,825 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 41, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 15:10:21,065 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 15:10:21,067 state_update_hooks.py: 115: Starting phase 41 [test]
INFO 2022-05-15 15:10:34,790 trainer_main.py: 214: Meters synced
INFO 2022-05-15 15:10:34,794 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 686
INFO 2022-05-15 15:10:34,795 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 29.409999999999997}, 'top_5': {'res5': 57.35}}
INFO 2022-05-15 15:10:34,796 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:10:34,804 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:10:34,805 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 42, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 15:12:04,358 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 15:12:04,359 state_update_hooks.py: 115: Starting phase 42 [train]
INFO 2022-05-15 15:13:34,140 trainer_main.py: 214: Meters synced
INFO 2022-05-15 15:13:34,144 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 916
INFO 2022-05-15 15:13:34,145 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  857.64 ms  857.77 ms
             forward:    5.89 ms   39.61 ms
        loss_compute:    0.97 ms    1.28 ms
     loss_all_reduce:    0.12 ms    0.11 ms
       meters_update:   12.32 ms   12.35 ms
            backward:    1.76 ms    1.96 ms
      optimizer_step:    0.50 ms    0.49 ms
    train_step_total:  915.88 ms  915.89 ms
INFO 2022-05-15 15:13:34,146 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 34.089999999999996}, 'top_5': {'res5': 63.304}}
INFO 2022-05-15 15:13:34,146 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:13:34,154 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:13:34,156 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 43, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 15:15:00,486 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 15:15:00,488 state_update_hooks.py: 115: Starting phase 43 [test]
INFO 2022-05-15 15:15:16,281 trainer_main.py: 214: Meters synced
INFO 2022-05-15 15:15:16,287 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 789
INFO 2022-05-15 15:15:16,289 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 29.38}, 'top_5': {'res5': 58.29}}
INFO 2022-05-15 15:15:16,290 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:15:16,298 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:15:16,300 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 44, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 15:16:49,625 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 15:16:49,626 state_update_hooks.py: 115: Starting phase 44 [train]
INFO 2022-05-15 15:17:27,434 log_hooks.py: 277: Rank: 0; [ep: 22] iter: 2200; lr: 0.3; loss: 2.58418; btime(ms): 2444; eta: 5:09:41; peak_mem(M): 2605;
INFO 2022-05-15 15:18:12,421 trainer_main.py: 214: Meters synced
INFO 2022-05-15 15:18:12,425 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 844
INFO 2022-05-15 15:18:12,426 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  802.27 ms  802.32 ms
             forward:    6.16 ms   32.17 ms
        loss_compute:    0.97 ms    0.95 ms
     loss_all_reduce:    0.13 ms    0.13 ms
       meters_update:    4.65 ms    4.67 ms
            backward:    2.05 ms    2.06 ms
      optimizer_step:    0.40 ms    0.40 ms
    train_step_total:  844.64 ms  844.64 ms
INFO 2022-05-15 15:18:12,427 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 34.324}, 'top_5': {'res5': 63.486}}
INFO 2022-05-15 15:18:12,428 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:18:12,435 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:18:12,436 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 45, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 15:19:45,533 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 15:19:45,535 state_update_hooks.py: 115: Starting phase 45 [test]
INFO 2022-05-15 15:20:01,209 trainer_main.py: 214: Meters synced
INFO 2022-05-15 15:20:01,213 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 783
INFO 2022-05-15 15:20:01,214 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 29.580000000000002}, 'top_5': {'res5': 58.3}}
INFO 2022-05-15 15:20:01,215 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:20:01,225 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:20:01,226 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 46, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 15:21:29,236 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 15:21:29,237 state_update_hooks.py: 115: Starting phase 46 [train]
INFO 2022-05-15 15:22:57,549 trainer_main.py: 214: Meters synced
INFO 2022-05-15 15:22:57,555 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 901
INFO 2022-05-15 15:22:57,556 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  848.84 ms  848.86 ms
             forward:    5.86 ms   36.00 ms
        loss_compute:    0.81 ms    0.97 ms
     loss_all_reduce:    0.11 ms    0.11 ms
       meters_update:   11.03 ms   11.09 ms
            backward:    1.27 ms    1.53 ms
      optimizer_step:    0.50 ms    0.48 ms
    train_step_total:  900.95 ms  900.97 ms
INFO 2022-05-15 15:22:57,558 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 34.268}, 'top_5': {'res5': 63.693999999999996}}
INFO 2022-05-15 15:22:57,559 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:22:57,567 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:22:57,568 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 47, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 15:24:30,810 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 15:24:30,812 state_update_hooks.py: 115: Starting phase 47 [test]
INFO 2022-05-15 15:24:45,472 trainer_main.py: 214: Meters synced
INFO 2022-05-15 15:24:45,477 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 733
INFO 2022-05-15 15:24:45,478 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 29.62}, 'top_5': {'res5': 58.03}}
INFO 2022-05-15 15:24:45,479 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:24:45,487 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:24:45,488 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 48, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 15:26:20,794 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 15:26:20,796 state_update_hooks.py: 115: Starting phase 48 [train]
INFO 2022-05-15 15:27:05,213 log_hooks.py: 277: Rank: 0; [ep: 24] iter: 2400; lr: 0.3; loss: 2.60111; btime(ms): 2441; eta: 5:01:09; peak_mem(M): 2605;
INFO 2022-05-15 15:27:48,138 trainer_main.py: 214: Meters synced
INFO 2022-05-15 15:27:48,142 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 891
INFO 2022-05-15 15:27:48,143 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  839.50 ms  839.55 ms
             forward:    6.26 ms   35.61 ms
        loss_compute:    0.90 ms    1.07 ms
     loss_all_reduce:    0.11 ms    0.10 ms
       meters_update:   10.33 ms   10.35 ms
            backward:    1.64 ms    1.79 ms
      optimizer_step:    0.54 ms    0.54 ms
    train_step_total:  891.03 ms  891.06 ms
INFO 2022-05-15 15:27:48,144 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 34.46}, 'top_5': {'res5': 63.746}}
INFO 2022-05-15 15:27:48,145 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:27:48,153 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:27:48,154 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 49, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 15:29:17,226 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 15:29:17,227 state_update_hooks.py: 115: Starting phase 49 [test]
INFO 2022-05-15 15:29:33,170 trainer_main.py: 214: Meters synced
INFO 2022-05-15 15:29:33,178 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 797
INFO 2022-05-15 15:29:33,179 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 29.95}, 'top_5': {'res5': 57.98}}
INFO 2022-05-15 15:29:33,180 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:29:33,187 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:29:33,189 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 50, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 15:31:06,615 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 15:31:06,616 state_update_hooks.py: 115: Starting phase 50 [train]
INFO 2022-05-15 15:32:32,759 trainer_main.py: 214: Meters synced
INFO 2022-05-15 15:32:32,764 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 879
INFO 2022-05-15 15:32:32,765 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  835.09 ms  835.15 ms
             forward:    6.25 ms   32.63 ms
        loss_compute:    0.84 ms    0.86 ms
     loss_all_reduce:    0.12 ms    0.13 ms
       meters_update:    5.98 ms    6.00 ms
            backward:    1.84 ms    1.85 ms
      optimizer_step:    0.44 ms    0.46 ms
    train_step_total:  878.80 ms  878.81 ms
INFO 2022-05-15 15:32:32,766 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 34.842}, 'top_5': {'res5': 63.93600000000001}}
INFO 2022-05-15 15:32:32,767 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:32:32,775 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:32:32,776 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 51, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 15:34:08,724 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 15:34:08,725 state_update_hooks.py: 115: Starting phase 51 [test]
INFO 2022-05-15 15:34:23,964 trainer_main.py: 214: Meters synced
INFO 2022-05-15 15:34:23,971 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 762
INFO 2022-05-15 15:34:23,972 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 29.67}, 'top_5': {'res5': 58.440000000000005}}
INFO 2022-05-15 15:34:23,973 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:34:23,981 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:34:23,982 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 52, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 15:35:54,687 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 15:35:54,688 state_update_hooks.py: 115: Starting phase 52 [train]
INFO 2022-05-15 15:36:45,028 log_hooks.py: 277: Rank: 0; [ep: 26] iter: 2600; lr: 0.3; loss: 2.65297; btime(ms): 2439; eta: 4:52:46; peak_mem(M): 2605;
INFO 2022-05-15 15:37:28,427 trainer_main.py: 214: Meters synced
INFO 2022-05-15 15:37:28,432 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 956
INFO 2022-05-15 15:37:28,434 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  903.99 ms  904.00 ms
             forward:    6.02 ms   35.63 ms
        loss_compute:    0.79 ms    0.97 ms
     loss_all_reduce:    0.12 ms    0.11 ms
       meters_update:   11.37 ms   11.39 ms
            backward:    1.49 ms    1.57 ms
      optimizer_step:    0.45 ms    0.53 ms
    train_step_total:  956.30 ms  956.24 ms
INFO 2022-05-15 15:37:28,435 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 34.814}, 'top_5': {'res5': 64.262}}
INFO 2022-05-15 15:37:28,435 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:37:28,441 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:37:28,442 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 53, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 15:38:58,912 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 15:38:58,913 state_update_hooks.py: 115: Starting phase 53 [test]
INFO 2022-05-15 15:39:13,010 trainer_main.py: 214: Meters synced
INFO 2022-05-15 15:39:13,014 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 705
INFO 2022-05-15 15:39:13,015 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 30.23}, 'top_5': {'res5': 58.53}}
INFO 2022-05-15 15:39:13,016 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:39:13,025 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:39:13,026 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 54, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 15:40:50,175 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 15:40:50,177 state_update_hooks.py: 115: Starting phase 54 [train]
INFO 2022-05-15 15:42:17,677 trainer_main.py: 214: Meters synced
INFO 2022-05-15 15:42:17,682 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 892
INFO 2022-05-15 15:42:17,683 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  845.87 ms  845.90 ms
             forward:    6.26 ms   33.92 ms
        loss_compute:    0.89 ms    0.93 ms
     loss_all_reduce:    0.10 ms    0.11 ms
       meters_update:    7.69 ms    7.71 ms
            backward:    1.72 ms    1.80 ms
      optimizer_step:    0.43 ms    0.46 ms
    train_step_total:  892.64 ms  892.66 ms
INFO 2022-05-15 15:42:17,684 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 35.356}, 'top_5': {'res5': 64.238}}
INFO 2022-05-15 15:42:17,685 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:42:17,694 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:42:17,695 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 55, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 15:43:52,722 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 15:43:52,723 state_update_hooks.py: 115: Starting phase 55 [test]
INFO 2022-05-15 15:44:09,370 trainer_main.py: 214: Meters synced
INFO 2022-05-15 15:44:09,376 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 832
INFO 2022-05-15 15:44:09,377 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 29.65}, 'top_5': {'res5': 58.37}}
INFO 2022-05-15 15:44:09,378 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:44:09,386 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:44:09,388 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 56, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 15:45:49,013 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 15:45:49,014 state_update_hooks.py: 115: Starting phase 56 [train]
INFO 2022-05-15 15:46:41,974 log_hooks.py: 277: Rank: 0; [ep: 28] iter: 2800; lr: 0.3; loss: 2.67317; btime(ms): 2443; eta: 4:45:02; peak_mem(M): 2605;
INFO 2022-05-15 15:47:24,128 trainer_main.py: 214: Meters synced
INFO 2022-05-15 15:47:24,133 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 970
INFO 2022-05-15 15:47:24,134 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  924.18 ms  924.22 ms
             forward:    5.91 ms   34.37 ms
        loss_compute:    0.77 ms    0.85 ms
     loss_all_reduce:    0.10 ms    0.11 ms
       meters_update:    7.02 ms    7.08 ms
            backward:    1.60 ms    1.69 ms
      optimizer_step:    0.40 ms    0.38 ms
    train_step_total:  970.34 ms  970.36 ms
INFO 2022-05-15 15:47:24,135 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 35.272}, 'top_5': {'res5': 64.622}}
INFO 2022-05-15 15:47:24,136 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:47:24,146 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:47:24,147 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 57, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 15:49:03,989 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 15:49:03,989 state_update_hooks.py: 115: Starting phase 57 [test]
INFO 2022-05-15 15:49:18,459 trainer_main.py: 214: Meters synced
INFO 2022-05-15 15:49:18,464 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 723
INFO 2022-05-15 15:49:18,466 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 29.2}, 'top_5': {'res5': 58.040000000000006}}
INFO 2022-05-15 15:49:18,466 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:49:18,476 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:49:18,477 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 58, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 15:50:51,744 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 15:50:51,745 state_update_hooks.py: 115: Starting phase 58 [train]
INFO 2022-05-15 15:52:26,063 trainer_main.py: 214: Meters synced
INFO 2022-05-15 15:52:26,067 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 962
INFO 2022-05-15 15:52:26,068 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  907.65 ms  907.70 ms
             forward:    5.53 ms   37.42 ms
        loss_compute:    0.83 ms    1.10 ms
     loss_all_reduce:    0.13 ms    0.11 ms
       meters_update:   11.80 ms   11.82 ms
            backward:    1.31 ms    1.47 ms
      optimizer_step:    0.59 ms    0.61 ms
    train_step_total:  962.18 ms  962.17 ms
INFO 2022-05-15 15:52:26,069 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 35.42}, 'top_5': {'res5': 64.91}}
INFO 2022-05-15 15:52:26,069 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:52:26,079 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:52:26,080 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 59, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 15:53:58,458 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 15:53:58,459 state_update_hooks.py: 115: Starting phase 59 [test]
INFO 2022-05-15 15:54:14,482 trainer_main.py: 214: Meters synced
INFO 2022-05-15 15:54:14,489 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 801
INFO 2022-05-15 15:54:14,489 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 29.349999999999998}, 'top_5': {'res5': 58.32000000000001}}
INFO 2022-05-15 15:54:14,490 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:54:14,500 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:54:14,501 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 60, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 15:55:53,126 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 15:55:53,127 state_update_hooks.py: 115: Starting phase 60 [train]
INFO 2022-05-15 15:56:46,921 log_hooks.py: 277: Rank: 0; [ep: 30] iter: 3000; lr: 0.3; loss: 2.5421; btime(ms): 2448; eta: 4:37:29; peak_mem(M): 2605;
INFO 2022-05-15 15:57:20,301 trainer_main.py: 214: Meters synced
INFO 2022-05-15 15:57:20,305 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 889
INFO 2022-05-15 15:57:20,305 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  845.16 ms  845.22 ms
             forward:    6.29 ms   33.69 ms
        loss_compute:    0.87 ms    0.92 ms
     loss_all_reduce:    0.11 ms    0.11 ms
       meters_update:    5.15 ms    5.16 ms
            backward:    2.09 ms    2.14 ms
      optimizer_step:    0.39 ms    0.38 ms
    train_step_total:  889.30 ms  889.32 ms
INFO 2022-05-15 15:57:20,307 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 35.85}, 'top_5': {'res5': 65.05}}
INFO 2022-05-15 15:57:20,308 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:57:20,317 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:57:20,318 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 61, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 15:58:59,914 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 15:58:59,915 state_update_hooks.py: 115: Starting phase 61 [test]
INFO 2022-05-15 15:59:16,516 trainer_main.py: 214: Meters synced
INFO 2022-05-15 15:59:16,522 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 830
INFO 2022-05-15 15:59:16,526 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 30.380000000000003}, 'top_5': {'res5': 58.5}}
INFO 2022-05-15 15:59:16,527 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:59:16,538 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 15:59:16,539 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 62, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 16:00:50,679 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 16:00:50,681 state_update_hooks.py: 115: Starting phase 62 [train]
INFO 2022-05-15 16:02:24,322 trainer_main.py: 214: Meters synced
INFO 2022-05-15 16:02:24,327 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 955
INFO 2022-05-15 16:02:24,328 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  904.93 ms  904.94 ms
             forward:    5.55 ms   35.20 ms
        loss_compute:    0.72 ms    0.84 ms
     loss_all_reduce:    0.11 ms    0.11 ms
       meters_update:   10.29 ms   10.31 ms
            backward:    1.45 ms    1.52 ms
      optimizer_step:    0.46 ms    0.51 ms
    train_step_total:  955.30 ms  955.30 ms
INFO 2022-05-15 16:02:24,330 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 35.53}, 'top_5': {'res5': 64.994}}
INFO 2022-05-15 16:02:24,331 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:02:24,340 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:02:24,341 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 63, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 16:04:01,067 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 16:04:01,069 state_update_hooks.py: 115: Starting phase 63 [test]
INFO 2022-05-15 16:04:15,210 trainer_main.py: 214: Meters synced
INFO 2022-05-15 16:04:15,215 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 707
INFO 2022-05-15 16:04:15,216 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 29.830000000000002}, 'top_5': {'res5': 58.45}}
INFO 2022-05-15 16:04:15,217 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:04:15,225 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:04:15,226 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 64, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 16:05:49,941 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 16:05:49,943 state_update_hooks.py: 115: Starting phase 64 [train]
INFO 2022-05-15 16:06:52,691 log_hooks.py: 277: Rank: 0; [ep: 32] iter: 3200; lr: 0.3; loss: 2.59503; btime(ms): 2453; eta: 4:29:50; peak_mem(M): 2605;
INFO 2022-05-15 16:07:20,725 trainer_main.py: 214: Meters synced
INFO 2022-05-15 16:07:20,730 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 926
INFO 2022-05-15 16:07:20,730 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  876.71 ms  876.85 ms
             forward:    6.00 ms   35.50 ms
        loss_compute:    0.98 ms    1.09 ms
     loss_all_reduce:    0.12 ms    0.11 ms
       meters_update:    8.45 ms    8.47 ms
            backward:    1.60 ms    1.65 ms
      optimizer_step:    0.43 ms    0.45 ms
    train_step_total:  926.13 ms  926.13 ms
INFO 2022-05-15 16:07:20,731 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 35.812}, 'top_5': {'res5': 65.364}}
INFO 2022-05-15 16:07:20,732 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:07:20,740 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:07:20,741 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 65, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 16:08:56,007 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 16:08:56,008 state_update_hooks.py: 115: Starting phase 65 [test]
INFO 2022-05-15 16:09:12,322 trainer_main.py: 214: Meters synced
INFO 2022-05-15 16:09:12,328 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 815
INFO 2022-05-15 16:09:12,329 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 30.11}, 'top_5': {'res5': 58.41}}
INFO 2022-05-15 16:09:12,330 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:09:12,338 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:09:12,339 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 66, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 16:10:49,792 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 16:10:49,794 state_update_hooks.py: 115: Starting phase 66 [train]
INFO 2022-05-15 16:12:20,361 trainer_main.py: 214: Meters synced
INFO 2022-05-15 16:12:20,366 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 924
INFO 2022-05-15 16:12:20,367 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  881.00 ms  881.05 ms
             forward:    6.19 ms   32.25 ms
        loss_compute:    0.76 ms    0.76 ms
     loss_all_reduce:    0.11 ms    0.11 ms
       meters_update:    5.56 ms    5.59 ms
            backward:    2.10 ms    2.11 ms
      optimizer_step:    0.43 ms    0.43 ms
    train_step_total:  923.94 ms  923.95 ms
INFO 2022-05-15 16:12:20,369 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 36.064}, 'top_5': {'res5': 65.554}}
INFO 2022-05-15 16:12:20,370 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:12:20,380 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:12:20,381 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 67, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 16:14:02,971 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 16:14:02,971 state_update_hooks.py: 115: Starting phase 67 [test]
INFO 2022-05-15 16:14:17,958 trainer_main.py: 214: Meters synced
INFO 2022-05-15 16:14:17,962 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 749
INFO 2022-05-15 16:14:17,963 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 29.94}, 'top_5': {'res5': 58.46}}
INFO 2022-05-15 16:14:17,964 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:14:17,972 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:14:17,973 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 68, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 16:15:56,775 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 16:15:56,777 state_update_hooks.py: 115: Starting phase 68 [train]
INFO 2022-05-15 16:17:06,430 log_hooks.py: 277: Rank: 0; [ep: 34] iter: 3400; lr: 0.3; loss: 2.54806; btime(ms): 2459; eta: 4:22:18; peak_mem(M): 2605;
INFO 2022-05-15 16:17:33,265 trainer_main.py: 214: Meters synced
INFO 2022-05-15 16:17:33,270 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 984
INFO 2022-05-15 16:17:33,271 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  925.71 ms  925.76 ms
             forward:    5.40 ms   39.76 ms
        loss_compute:    1.03 ms    1.41 ms
     loss_all_reduce:    0.11 ms    0.10 ms
       meters_update:   12.92 ms   12.94 ms
            backward:    1.29 ms    1.48 ms
      optimizer_step:    0.50 ms    0.52 ms
    train_step_total:  984.38 ms  984.34 ms
INFO 2022-05-15 16:17:33,272 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 36.142}, 'top_5': {'res5': 65.53}}
INFO 2022-05-15 16:17:33,273 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:17:33,281 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:17:33,283 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 69, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 16:19:05,563 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 16:19:05,564 state_update_hooks.py: 115: Starting phase 69 [test]
INFO 2022-05-15 16:19:19,179 trainer_main.py: 214: Meters synced
INFO 2022-05-15 16:19:19,184 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 680
INFO 2022-05-15 16:19:19,185 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 29.75}, 'top_5': {'res5': 57.76}}
INFO 2022-05-15 16:19:19,185 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:19:19,192 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:19:19,193 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 70, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 16:20:56,265 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 16:20:56,267 state_update_hooks.py: 115: Starting phase 70 [train]
INFO 2022-05-15 16:22:23,480 trainer_main.py: 214: Meters synced
INFO 2022-05-15 16:22:23,484 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 889
INFO 2022-05-15 16:22:23,485 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  843.27 ms  843.35 ms
             forward:    6.05 ms   34.28 ms
        loss_compute:    0.98 ms    1.06 ms
     loss_all_reduce:    0.11 ms    0.11 ms
       meters_update:    6.62 ms    6.64 ms
            backward:    1.80 ms    1.87 ms
      optimizer_step:    0.45 ms    0.42 ms
    train_step_total:  889.72 ms  889.74 ms
INFO 2022-05-15 16:22:23,486 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 36.472}, 'top_5': {'res5': 65.666}}
INFO 2022-05-15 16:22:23,487 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:22:23,495 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:22:23,496 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 71, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 16:23:58,704 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 16:23:58,705 state_update_hooks.py: 115: Starting phase 71 [test]
INFO 2022-05-15 16:24:15,765 trainer_main.py: 214: Meters synced
INFO 2022-05-15 16:24:15,771 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 853
INFO 2022-05-15 16:24:15,772 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 29.56}, 'top_5': {'res5': 58.099999999999994}}
INFO 2022-05-15 16:24:15,773 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:24:15,781 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:24:15,782 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 72, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 16:25:50,401 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 16:25:50,403 state_update_hooks.py: 115: Starting phase 72 [train]
INFO 2022-05-15 16:26:56,687 log_hooks.py: 277: Rank: 0; [ep: 36] iter: 3600; lr: 0.3; loss: 2.44995; btime(ms): 2459; eta: 4:14:07; peak_mem(M): 2605;
INFO 2022-05-15 16:27:19,829 trainer_main.py: 214: Meters synced
INFO 2022-05-15 16:27:19,834 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 912
INFO 2022-05-15 16:27:19,835 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  863.02 ms  863.11 ms
             forward:    5.82 ms   35.47 ms
        loss_compute:    0.79 ms    0.90 ms
     loss_all_reduce:    0.11 ms    0.11 ms
       meters_update:    8.74 ms    8.77 ms
            backward:    1.58 ms    1.71 ms
      optimizer_step:    0.43 ms    0.41 ms
    train_step_total:  912.31 ms  912.31 ms
INFO 2022-05-15 16:27:19,836 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 36.546}, 'top_5': {'res5': 65.68}}
INFO 2022-05-15 16:27:19,836 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:27:19,843 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:27:19,844 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 73, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 16:28:56,560 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 16:28:56,561 state_update_hooks.py: 115: Starting phase 73 [test]
INFO 2022-05-15 16:29:11,457 trainer_main.py: 214: Meters synced
INFO 2022-05-15 16:29:11,462 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 745
INFO 2022-05-15 16:29:11,463 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 29.439999999999998}, 'top_5': {'res5': 58.47}}
INFO 2022-05-15 16:29:11,464 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:29:11,472 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:29:11,473 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 74, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 16:30:46,318 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 16:30:46,319 state_update_hooks.py: 115: Starting phase 74 [train]
INFO 2022-05-15 16:32:20,287 trainer_main.py: 214: Meters synced
INFO 2022-05-15 16:32:20,291 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 958
INFO 2022-05-15 16:32:20,292 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  905.52 ms  905.53 ms
             forward:    5.75 ms   37.00 ms
        loss_compute:    0.83 ms    1.04 ms
     loss_all_reduce:    0.11 ms    0.10 ms
       meters_update:   10.48 ms   10.50 ms
            backward:    1.50 ms    1.68 ms
      optimizer_step:    0.49 ms    0.48 ms
    train_step_total:  958.64 ms  958.64 ms
INFO 2022-05-15 16:32:20,293 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 36.478}, 'top_5': {'res5': 65.912}}
INFO 2022-05-15 16:32:20,293 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:32:20,303 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:32:20,304 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 75, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 16:33:50,833 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 16:33:50,833 state_update_hooks.py: 115: Starting phase 75 [test]
INFO 2022-05-15 16:34:07,180 trainer_main.py: 214: Meters synced
INFO 2022-05-15 16:34:07,188 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 817
INFO 2022-05-15 16:34:07,189 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 30.020000000000003}, 'top_5': {'res5': 58.93000000000001}}
INFO 2022-05-15 16:34:07,190 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:34:07,197 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:34:07,198 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 76, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 16:35:41,178 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 16:35:41,180 state_update_hooks.py: 115: Starting phase 76 [train]
INFO 2022-05-15 16:36:47,087 log_hooks.py: 277: Rank: 0; [ep: 38] iter: 3800; lr: 0.3; loss: 2.6574; btime(ms): 2459; eta: 4:05:55; peak_mem(M): 2605;
INFO 2022-05-15 16:37:04,060 trainer_main.py: 214: Meters synced
INFO 2022-05-15 16:37:04,065 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 845
INFO 2022-05-15 16:37:04,066 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  803.37 ms  803.41 ms
             forward:    6.36 ms   32.22 ms
        loss_compute:    0.78 ms    0.78 ms
     loss_all_reduce:    0.11 ms    0.11 ms
       meters_update:    5.01 ms    5.04 ms
            backward:    1.90 ms    1.92 ms
      optimizer_step:    0.39 ms    0.39 ms
    train_step_total:  845.50 ms  845.50 ms
INFO 2022-05-15 16:37:04,067 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 36.43}, 'top_5': {'res5': 65.82000000000001}}
INFO 2022-05-15 16:37:04,068 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:37:04,074 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:37:04,075 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 77, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 16:38:38,290 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 16:38:38,291 state_update_hooks.py: 115: Starting phase 77 [test]
INFO 2022-05-15 16:38:55,628 trainer_main.py: 214: Meters synced
INFO 2022-05-15 16:38:55,633 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 867
INFO 2022-05-15 16:38:55,635 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 30.28}, 'top_5': {'res5': 58.199999999999996}}
INFO 2022-05-15 16:38:55,635 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:38:55,645 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:38:55,646 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 78, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 16:40:29,022 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 16:40:29,024 state_update_hooks.py: 115: Starting phase 78 [train]
INFO 2022-05-15 16:42:01,874 trainer_main.py: 214: Meters synced
INFO 2022-05-15 16:42:01,882 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 947
INFO 2022-05-15 16:42:01,884 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  895.74 ms  895.77 ms
             forward:    5.81 ms   35.67 ms
        loss_compute:    0.82 ms    0.95 ms
     loss_all_reduce:    0.12 ms    0.11 ms
       meters_update:   10.82 ms   10.84 ms
            backward:    1.30 ms    1.46 ms
      optimizer_step:    0.47 ms    0.45 ms
    train_step_total:  947.24 ms  947.20 ms
INFO 2022-05-15 16:42:01,885 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 36.6}, 'top_5': {'res5': 66.188}}
INFO 2022-05-15 16:42:01,886 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:42:01,898 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:42:01,899 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 79, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 16:43:30,989 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 16:43:30,990 state_update_hooks.py: 115: Starting phase 79 [test]
INFO 2022-05-15 16:43:45,111 trainer_main.py: 214: Meters synced
INFO 2022-05-15 16:43:45,115 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 706
INFO 2022-05-15 16:43:45,116 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 30.18}, 'top_5': {'res5': 58.75}}
INFO 2022-05-15 16:43:45,116 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:43:45,124 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:43:45,126 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 80, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 16:45:21,372 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 16:45:21,373 state_update_hooks.py: 115: Starting phase 80 [train]
INFO 2022-05-15 16:46:40,229 log_hooks.py: 277: Rank: 0; [ep: 40] iter: 4000; lr: 0.3; loss: 2.71996; btime(ms): 2459; eta: 3:57:47; peak_mem(M): 2605;
INFO 2022-05-15 16:46:55,491 trainer_main.py: 214: Meters synced
INFO 2022-05-15 16:46:55,495 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 960
INFO 2022-05-15 16:46:55,496 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  909.59 ms  909.68 ms
             forward:    5.95 ms   35.60 ms
        loss_compute:    0.85 ms    0.99 ms
     loss_all_reduce:    0.11 ms    0.11 ms
       meters_update:    9.72 ms    9.74 ms
            backward:    1.51 ms    1.59 ms
      optimizer_step:    0.50 ms    0.48 ms
    train_step_total:  960.14 ms  960.17 ms
INFO 2022-05-15 16:46:55,497 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 36.906}, 'top_5': {'res5': 66.28}}
INFO 2022-05-15 16:46:55,497 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:46:55,506 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:46:55,507 log_hooks.py: 425: [phase: 40] Saving checkpoint to ./checkpoints/LP/2
INFO 2022-05-15 16:46:55,656 checkpoint.py: 131: Saved checkpoint: ./checkpoints/LP/2/model_phase40.torch
INFO 2022-05-15 16:46:55,657 checkpoint.py: 140: Creating symlink...
INFO 2022-05-15 16:46:55,665 checkpoint.py: 144: Created symlink: ./checkpoints/LP/2/checkpoint.torch
INFO 2022-05-15 16:46:55,666 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 81, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 16:48:31,233 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 16:48:31,234 state_update_hooks.py: 115: Starting phase 81 [test]
INFO 2022-05-15 16:48:46,957 trainer_main.py: 214: Meters synced
INFO 2022-05-15 16:48:46,962 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 786
INFO 2022-05-15 16:48:46,963 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 30.320000000000004}, 'top_5': {'res5': 58.93000000000001}}
INFO 2022-05-15 16:48:46,964 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:48:46,972 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:48:46,973 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 82, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 16:50:23,574 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 16:50:23,577 state_update_hooks.py: 115: Starting phase 82 [train]
INFO 2022-05-15 16:51:51,314 trainer_main.py: 214: Meters synced
INFO 2022-05-15 16:51:51,320 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 895
INFO 2022-05-15 16:51:51,321 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  850.57 ms  850.59 ms
             forward:    5.91 ms   32.83 ms
        loss_compute:    0.71 ms    0.73 ms
     loss_all_reduce:    0.11 ms    0.11 ms
       meters_update:    6.45 ms    6.47 ms
            backward:    1.80 ms    1.83 ms
      optimizer_step:    0.39 ms    0.39 ms
    train_step_total:  895.07 ms  895.06 ms
INFO 2022-05-15 16:51:51,322 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 36.934}, 'top_5': {'res5': 66.49000000000001}}
INFO 2022-05-15 16:51:51,325 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:51:51,334 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:51:51,334 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 83, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 16:53:29,150 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 16:53:29,151 state_update_hooks.py: 115: Starting phase 83 [test]
INFO 2022-05-15 16:53:43,646 trainer_main.py: 214: Meters synced
INFO 2022-05-15 16:53:43,650 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 724
INFO 2022-05-15 16:53:43,651 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 29.959999999999997}, 'top_5': {'res5': 58.46}}
INFO 2022-05-15 16:53:43,652 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:53:43,661 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:53:43,662 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 84, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 16:55:16,383 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 16:55:16,384 state_update_hooks.py: 115: Starting phase 84 [train]
INFO 2022-05-15 16:56:41,986 log_hooks.py: 277: Rank: 0; [ep: 42] iter: 4200; lr: 0.3; loss: 2.45974; btime(ms): 2462; eta: 3:49:48; peak_mem(M): 2605;
INFO 2022-05-15 16:56:54,128 trainer_main.py: 214: Meters synced
INFO 2022-05-15 16:56:54,134 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 997
INFO 2022-05-15 16:56:54,134 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  946.57 ms  946.65 ms
             forward:    5.36 ms   35.50 ms
        loss_compute:    0.72 ms    0.88 ms
     loss_all_reduce:    0.11 ms    0.11 ms
       meters_update:   10.46 ms   10.48 ms
            backward:    1.13 ms    1.28 ms
      optimizer_step:    0.46 ms    0.45 ms
    train_step_total:  997.12 ms  997.19 ms
INFO 2022-05-15 16:56:54,135 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 37.004}, 'top_5': {'res5': 66.534}}
INFO 2022-05-15 16:56:54,136 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:56:54,146 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:56:54,147 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 85, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 16:58:29,285 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 16:58:29,286 state_update_hooks.py: 115: Starting phase 85 [test]
INFO 2022-05-15 16:58:44,268 trainer_main.py: 214: Meters synced
INFO 2022-05-15 16:58:44,273 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 749
INFO 2022-05-15 16:58:44,274 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 29.69}, 'top_5': {'res5': 58.10999999999999}}
INFO 2022-05-15 16:58:44,275 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:58:44,283 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 16:58:44,287 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 86, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 17:00:20,780 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 17:00:20,781 state_update_hooks.py: 115: Starting phase 86 [train]
INFO 2022-05-15 17:01:45,623 trainer_main.py: 214: Meters synced
INFO 2022-05-15 17:01:45,628 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 865
INFO 2022-05-15 17:01:45,629 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  821.98 ms  822.07 ms
             forward:    6.09 ms   33.22 ms
        loss_compute:    0.79 ms    0.80 ms
     loss_all_reduce:    0.10 ms    0.11 ms
       meters_update:    5.36 ms    5.38 ms
            backward:    1.77 ms    1.78 ms
      optimizer_step:    0.40 ms    0.43 ms
    train_step_total:  865.49 ms  865.51 ms
INFO 2022-05-15 17:01:45,630 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 36.886}, 'top_5': {'res5': 66.484}}
INFO 2022-05-15 17:01:45,630 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:01:45,637 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:01:45,638 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 87, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 17:03:19,420 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 17:03:19,421 state_update_hooks.py: 115: Starting phase 87 [test]
INFO 2022-05-15 17:03:35,184 trainer_main.py: 214: Meters synced
INFO 2022-05-15 17:03:35,189 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 788
INFO 2022-05-15 17:03:35,190 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 30.31}, 'top_5': {'res5': 58.89}}
INFO 2022-05-15 17:03:35,191 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:03:35,200 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:03:35,201 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 88, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 17:05:06,722 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 17:05:06,724 state_update_hooks.py: 115: Starting phase 88 [train]
INFO 2022-05-15 17:06:29,826 log_hooks.py: 277: Rank: 0; [ep: 44] iter: 4400; lr: 0.3; loss: 2.56962; btime(ms): 2461; eta: 3:41:32; peak_mem(M): 2605;
INFO 2022-05-15 17:06:37,321 trainer_main.py: 214: Meters synced
INFO 2022-05-15 17:06:37,327 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 924
INFO 2022-05-15 17:06:37,328 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  877.15 ms  877.21 ms
             forward:    5.94 ms   33.51 ms
        loss_compute:    0.78 ms    0.87 ms
     loss_all_reduce:    0.11 ms    0.11 ms
       meters_update:    8.65 ms    8.67 ms
            backward:    1.53 ms    1.59 ms
      optimizer_step:    0.46 ms    0.49 ms
    train_step_total:  924.26 ms  924.27 ms
INFO 2022-05-15 17:06:37,330 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 37.178}, 'top_5': {'res5': 66.506}}
INFO 2022-05-15 17:06:37,330 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:06:37,341 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:06:37,342 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 89, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 17:08:07,344 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 17:08:07,345 state_update_hooks.py: 115: Starting phase 89 [test]
INFO 2022-05-15 17:08:21,192 trainer_main.py: 214: Meters synced
INFO 2022-05-15 17:08:21,197 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 692
INFO 2022-05-15 17:08:21,198 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 30.680000000000003}, 'top_5': {'res5': 58.330000000000005}}
INFO 2022-05-15 17:08:21,199 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:08:21,207 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:08:21,208 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 90, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 17:09:50,047 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 17:09:50,047 state_update_hooks.py: 115: Starting phase 90 [train]
INFO 2022-05-15 17:11:16,561 trainer_main.py: 214: Meters synced
INFO 2022-05-15 17:11:16,566 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 882
INFO 2022-05-15 17:11:16,567 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  829.05 ms  829.13 ms
             forward:    5.96 ms   36.24 ms
        loss_compute:    0.94 ms    1.15 ms
     loss_all_reduce:    0.11 ms    0.10 ms
       meters_update:   11.70 ms   11.72 ms
            backward:    1.48 ms    1.61 ms
      optimizer_step:    0.51 ms    0.53 ms
    train_step_total:  882.58 ms  882.59 ms
INFO 2022-05-15 17:11:16,568 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 37.558}, 'top_5': {'res5': 66.738}}
INFO 2022-05-15 17:11:16,569 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:11:16,576 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:11:16,578 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 91, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 17:12:45,219 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 17:12:45,220 state_update_hooks.py: 115: Starting phase 91 [test]
INFO 2022-05-15 17:13:00,907 trainer_main.py: 214: Meters synced
INFO 2022-05-15 17:13:00,913 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 784
INFO 2022-05-15 17:13:00,914 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 30.009999999999998}, 'top_5': {'res5': 58.13}}
INFO 2022-05-15 17:13:00,914 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:13:00,923 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:13:00,924 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 92, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 17:14:36,398 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 17:14:36,399 state_update_hooks.py: 115: Starting phase 92 [train]
INFO 2022-05-15 17:15:59,951 log_hooks.py: 277: Rank: 0; [ep: 46] iter: 4600; lr: 0.3; loss: 2.38662; btime(ms): 2457; eta: 3:33:00; peak_mem(M): 2605;
INFO 2022-05-15 17:16:03,526 trainer_main.py: 214: Meters synced
INFO 2022-05-15 17:16:03,530 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 889
INFO 2022-05-15 17:16:03,531 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  845.97 ms  846.01 ms
             forward:    6.27 ms   33.07 ms
        loss_compute:    0.84 ms    0.88 ms
     loss_all_reduce:    0.10 ms    0.10 ms
       meters_update:    4.23 ms    4.25 ms
            backward:    1.90 ms    1.93 ms
      optimizer_step:    0.37 ms    0.40 ms
    train_step_total:  888.84 ms  888.85 ms
INFO 2022-05-15 17:16:03,532 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 37.41}, 'top_5': {'res5': 66.64999999999999}}
INFO 2022-05-15 17:16:03,533 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:16:03,540 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:16:03,541 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 93, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 17:17:40,070 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 17:17:40,072 state_update_hooks.py: 115: Starting phase 93 [test]
INFO 2022-05-15 17:17:55,717 trainer_main.py: 214: Meters synced
INFO 2022-05-15 17:17:55,721 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 782
INFO 2022-05-15 17:17:55,721 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 29.849999999999998}, 'top_5': {'res5': 58.47}}
INFO 2022-05-15 17:17:55,722 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:17:55,731 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:17:55,732 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 94, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 17:19:23,587 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 17:19:23,588 state_update_hooks.py: 115: Starting phase 94 [train]
INFO 2022-05-15 17:20:52,870 trainer_main.py: 214: Meters synced
INFO 2022-05-15 17:20:52,879 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 911
INFO 2022-05-15 17:20:52,880 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  856.90 ms  856.89 ms
             forward:    5.88 ms   36.62 ms
        loss_compute:    0.88 ms    1.07 ms
     loss_all_reduce:    0.12 ms    0.12 ms
       meters_update:   11.95 ms   11.97 ms
            backward:    1.42 ms    1.55 ms
      optimizer_step:    0.57 ms    0.59 ms
    train_step_total:  910.82 ms  910.78 ms
INFO 2022-05-15 17:20:52,884 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 37.552}, 'top_5': {'res5': 67.014}}
INFO 2022-05-15 17:20:52,885 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:20:52,896 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:20:52,897 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 95, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 17:22:23,398 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 17:22:23,399 state_update_hooks.py: 115: Starting phase 95 [test]
INFO 2022-05-15 17:22:37,533 trainer_main.py: 214: Meters synced
INFO 2022-05-15 17:22:37,538 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 706
INFO 2022-05-15 17:22:37,539 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 29.94}, 'top_5': {'res5': 58.42}}
INFO 2022-05-15 17:22:37,540 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:22:37,548 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:22:37,549 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 96, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 17:24:09,997 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 17:24:09,998 state_update_hooks.py: 115: Starting phase 96 [train]
INFO 2022-05-15 17:25:39,906 log_hooks.py: 277: Rank: 0; [ep: 48] iter: 4800; lr: 0.3; loss: 2.49859; btime(ms): 2456; eta: 3:24:40; peak_mem(M): 2605;
INFO 2022-05-15 17:25:39,955 trainer_main.py: 214: Meters synced
INFO 2022-05-15 17:25:39,959 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 917
INFO 2022-05-15 17:25:39,960 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  867.68 ms  867.71 ms
             forward:    5.94 ms   35.53 ms
        loss_compute:    0.85 ms    0.98 ms
     loss_all_reduce:    0.11 ms    0.11 ms
       meters_update:    9.05 ms    9.08 ms
            backward:    1.67 ms    1.75 ms
      optimizer_step:    0.45 ms    0.44 ms
    train_step_total:  917.71 ms  917.73 ms
INFO 2022-05-15 17:25:39,961 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 37.502}, 'top_5': {'res5': 66.972}}
INFO 2022-05-15 17:25:39,962 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:25:39,970 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:25:39,971 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 97, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 17:27:12,139 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 17:27:12,140 state_update_hooks.py: 115: Starting phase 97 [test]
INFO 2022-05-15 17:27:28,561 trainer_main.py: 214: Meters synced
INFO 2022-05-15 17:27:28,567 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 821
INFO 2022-05-15 17:27:28,568 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 30.06}, 'top_5': {'res5': 58.37}}
INFO 2022-05-15 17:27:28,569 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:27:28,577 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:27:28,578 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 98, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 17:29:00,692 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 17:29:00,693 state_update_hooks.py: 115: Starting phase 98 [train]
INFO 2022-05-15 17:30:26,467 trainer_main.py: 214: Meters synced
INFO 2022-05-15 17:30:26,474 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 875
INFO 2022-05-15 17:30:26,476 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  830.98 ms  831.02 ms
             forward:    5.98 ms   33.12 ms
        loss_compute:    0.75 ms    0.79 ms
     loss_all_reduce:    0.11 ms    0.11 ms
       meters_update:    6.30 ms    6.33 ms
            backward:    1.60 ms    1.63 ms
      optimizer_step:    0.40 ms    0.39 ms
    train_step_total:  875.02 ms  875.01 ms
INFO 2022-05-15 17:30:26,477 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 37.45}, 'top_5': {'res5': 66.866}}
INFO 2022-05-15 17:30:26,478 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:30:26,486 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:30:26,488 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 99, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 17:32:03,955 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 17:32:03,957 state_update_hooks.py: 115: Starting phase 99 [test]
INFO 2022-05-15 17:32:18,037 trainer_main.py: 214: Meters synced
INFO 2022-05-15 17:32:18,041 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 704
INFO 2022-05-15 17:32:18,042 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 29.609999999999996}, 'top_5': {'res5': 57.620000000000005}}
INFO 2022-05-15 17:32:18,043 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:32:18,052 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:32:18,053 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 100, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 17:33:51,179 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 17:33:51,180 state_update_hooks.py: 115: Starting phase 100 [train]
INFO 2022-05-15 17:35:24,305 trainer_main.py: 214: Meters synced
INFO 2022-05-15 17:35:24,312 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 950
INFO 2022-05-15 17:35:24,312 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  892.46 ms  892.57 ms
             forward:    5.61 ms   37.47 ms
        loss_compute:    0.89 ms    1.11 ms
     loss_all_reduce:    0.15 ms    0.14 ms
       meters_update:   14.62 ms   14.64 ms
            backward:    1.19 ms    1.41 ms
      optimizer_step:    0.52 ms    0.54 ms
    train_step_total:  950.02 ms  950.03 ms
INFO 2022-05-15 17:35:24,313 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 37.768}, 'top_5': {'res5': 67.124}}
INFO 2022-05-15 17:35:24,314 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:35:24,324 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:35:24,326 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 101, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 17:36:53,855 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 17:36:53,856 state_update_hooks.py: 115: Starting phase 101 [test]
INFO 2022-05-15 17:37:08,150 trainer_main.py: 214: Meters synced
INFO 2022-05-15 17:37:08,155 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 714
INFO 2022-05-15 17:37:08,155 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 30.19}, 'top_5': {'res5': 58.550000000000004}}
INFO 2022-05-15 17:37:08,156 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:37:08,166 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:37:08,167 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 102, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 17:38:44,855 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 17:38:44,856 state_update_hooks.py: 115: Starting phase 102 [train]
INFO 2022-05-15 17:38:45,107 log_hooks.py: 277: Rank: 0; [ep: 51] iter: 5000; lr: 0.3; loss: 2.44943; btime(ms): 2480; eta: 3:18:26; peak_mem(M): 2605;
INFO 2022-05-15 17:40:17,344 trainer_main.py: 214: Meters synced
INFO 2022-05-15 17:40:17,349 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 943
INFO 2022-05-15 17:40:17,350 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  898.13 ms  898.19 ms
             forward:    6.04 ms   33.76 ms
        loss_compute:    0.80 ms    0.87 ms
     loss_all_reduce:    0.10 ms    0.11 ms
       meters_update:    6.09 ms    6.11 ms
            backward:    1.76 ms    1.79 ms
      optimizer_step:    0.45 ms    0.44 ms
    train_step_total:  943.55 ms  943.56 ms
INFO 2022-05-15 17:40:17,351 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 37.884}, 'top_5': {'res5': 67.206}}
INFO 2022-05-15 17:40:17,352 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:40:17,361 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:40:17,363 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 103, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 17:41:50,468 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 17:41:50,469 state_update_hooks.py: 115: Starting phase 103 [test]
INFO 2022-05-15 17:42:05,834 trainer_main.py: 214: Meters synced
INFO 2022-05-15 17:42:05,839 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 768
INFO 2022-05-15 17:42:05,840 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 30.409999999999997}, 'top_5': {'res5': 58.120000000000005}}
INFO 2022-05-15 17:42:05,842 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:42:05,851 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:42:05,852 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 104, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 17:43:34,881 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 17:43:34,883 state_update_hooks.py: 115: Starting phase 104 [train]
INFO 2022-05-15 17:45:01,318 trainer_main.py: 214: Meters synced
INFO 2022-05-15 17:45:01,323 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 882
INFO 2022-05-15 17:45:01,323 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  831.23 ms  831.31 ms
             forward:    5.76 ms   35.48 ms
        loss_compute:    0.79 ms    0.94 ms
     loss_all_reduce:    0.11 ms    0.11 ms
       meters_update:    9.08 ms    9.10 ms
            backward:    1.68 ms    1.77 ms
      optimizer_step:    0.45 ms    0.47 ms
    train_step_total:  881.79 ms  881.78 ms
INFO 2022-05-15 17:45:01,325 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 37.672}, 'top_5': {'res5': 67.232}}
INFO 2022-05-15 17:45:01,326 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:45:01,334 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:45:01,335 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 105, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 17:46:33,438 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 17:46:33,439 state_update_hooks.py: 115: Starting phase 105 [test]
INFO 2022-05-15 17:46:47,598 trainer_main.py: 214: Meters synced
INFO 2022-05-15 17:46:47,602 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 708
INFO 2022-05-15 17:46:47,603 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 31.06}, 'top_5': {'res5': 58.78}}
INFO 2022-05-15 17:46:47,604 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:46:47,613 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:46:47,614 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 106, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 17:48:18,007 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 17:48:18,008 state_update_hooks.py: 115: Starting phase 106 [train]
INFO 2022-05-15 17:48:21,886 log_hooks.py: 277: Rank: 0; [ep: 53] iter: 5200; lr: 0.3; loss: 2.55526; btime(ms): 2477; eta: 3:09:56; peak_mem(M): 2605;
INFO 2022-05-15 17:49:47,449 trainer_main.py: 214: Meters synced
INFO 2022-05-15 17:49:47,454 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 912
INFO 2022-05-15 17:49:47,454 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  860.54 ms  860.59 ms
             forward:    5.68 ms   34.92 ms
        loss_compute:    0.84 ms    1.00 ms
     loss_all_reduce:    0.11 ms    0.10 ms
       meters_update:   11.55 ms   11.57 ms
            backward:    1.49 ms    1.59 ms
      optimizer_step:    0.56 ms    0.48 ms
    train_step_total:  912.45 ms  912.43 ms
INFO 2022-05-15 17:49:47,456 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 38.03}, 'top_5': {'res5': 67.41}}
INFO 2022-05-15 17:49:47,456 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:49:47,465 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:49:47,466 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 107, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 17:51:14,051 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 17:51:14,051 state_update_hooks.py: 115: Starting phase 107 [test]
INFO 2022-05-15 17:51:29,212 trainer_main.py: 214: Meters synced
INFO 2022-05-15 17:51:29,218 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 758
INFO 2022-05-15 17:51:29,218 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 30.42}, 'top_5': {'res5': 58.45}}
INFO 2022-05-15 17:51:29,219 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:51:29,227 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:51:29,228 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 108, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 17:53:03,727 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 17:53:03,728 state_update_hooks.py: 115: Starting phase 108 [train]
INFO 2022-05-15 17:54:24,918 trainer_main.py: 214: Meters synced
INFO 2022-05-15 17:54:24,922 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 828
INFO 2022-05-15 17:54:24,923 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  785.48 ms  785.52 ms
             forward:    6.10 ms   32.56 ms
        loss_compute:    0.83 ms    0.85 ms
     loss_all_reduce:    0.12 ms    0.13 ms
       meters_update:    5.09 ms    5.12 ms
            backward:    2.04 ms    2.05 ms
      optimizer_step:    0.39 ms    0.40 ms
    train_step_total:  828.25 ms  828.26 ms
INFO 2022-05-15 17:54:24,924 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 38.080000000000005}, 'top_5': {'res5': 67.47800000000001}}
INFO 2022-05-15 17:54:24,925 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:54:24,932 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:54:24,933 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 109, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 17:55:56,739 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 17:55:56,740 state_update_hooks.py: 115: Starting phase 109 [test]
INFO 2022-05-15 17:56:11,332 trainer_main.py: 214: Meters synced
INFO 2022-05-15 17:56:11,340 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 730
INFO 2022-05-15 17:56:11,341 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 30.220000000000002}, 'top_5': {'res5': 58.34}}
INFO 2022-05-15 17:56:11,341 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:56:11,350 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:56:11,351 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 110, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 17:57:40,165 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 17:57:40,166 state_update_hooks.py: 115: Starting phase 110 [train]
INFO 2022-05-15 17:57:47,578 log_hooks.py: 277: Rank: 0; [ep: 55] iter: 5400; lr: 0.3; loss: 2.45562; btime(ms): 2473; eta: 3:01:21; peak_mem(M): 2605;
INFO 2022-05-15 17:59:14,782 trainer_main.py: 214: Meters synced
INFO 2022-05-15 17:59:14,787 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 965
INFO 2022-05-15 17:59:14,788 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  916.61 ms  916.73 ms
             forward:    5.54 ms   33.90 ms
        loss_compute:    0.72 ms    0.80 ms
     loss_all_reduce:    0.11 ms    0.11 ms
       meters_update:   10.14 ms   10.16 ms
            backward:    1.28 ms    1.33 ms
      optimizer_step:    0.47 ms    0.47 ms
    train_step_total:  965.26 ms  965.27 ms
INFO 2022-05-15 17:59:14,789 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 38.044}, 'top_5': {'res5': 67.55}}
INFO 2022-05-15 17:59:14,790 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:59:14,800 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 17:59:14,800 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 111, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 18:00:45,583 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 18:00:45,584 state_update_hooks.py: 115: Starting phase 111 [test]
INFO 2022-05-15 18:01:00,031 trainer_main.py: 214: Meters synced
INFO 2022-05-15 18:01:00,035 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 722
INFO 2022-05-15 18:01:00,036 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 29.609999999999996}, 'top_5': {'res5': 58.209999999999994}}
INFO 2022-05-15 18:01:00,036 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:01:00,044 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:01:00,045 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 112, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 18:02:30,867 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 18:02:30,868 state_update_hooks.py: 115: Starting phase 112 [train]
INFO 2022-05-15 18:03:56,693 trainer_main.py: 214: Meters synced
INFO 2022-05-15 18:03:56,698 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 875
INFO 2022-05-15 18:03:56,699 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  824.62 ms  824.64 ms
             forward:    5.85 ms   35.78 ms
        loss_compute:    0.87 ms    1.05 ms
     loss_all_reduce:    0.11 ms    0.10 ms
       meters_update:   10.08 ms   10.09 ms
            backward:    1.36 ms    1.51 ms
      optimizer_step:    0.46 ms    0.49 ms
    train_step_total:  875.56 ms  875.57 ms
INFO 2022-05-15 18:03:56,699 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 38.094}, 'top_5': {'res5': 67.63}}
INFO 2022-05-15 18:03:56,700 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:03:56,708 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:03:56,709 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 113, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 18:05:23,832 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 18:05:23,833 state_update_hooks.py: 115: Starting phase 113 [test]
INFO 2022-05-15 18:05:38,754 trainer_main.py: 214: Meters synced
INFO 2022-05-15 18:05:38,760 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 746
INFO 2022-05-15 18:05:38,761 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 30.349999999999998}, 'top_5': {'res5': 58.02}}
INFO 2022-05-15 18:05:38,762 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:05:38,774 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:05:38,776 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 114, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 18:07:10,636 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 18:07:10,637 state_update_hooks.py: 115: Starting phase 114 [train]
INFO 2022-05-15 18:07:21,468 log_hooks.py: 277: Rank: 0; [ep: 57] iter: 5600; lr: 0.3; loss: 2.50288; btime(ms): 2470; eta: 2:52:54; peak_mem(M): 2605;
INFO 2022-05-15 18:08:35,165 trainer_main.py: 214: Meters synced
INFO 2022-05-15 18:08:35,170 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 862
INFO 2022-05-15 18:08:35,171 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  820.12 ms  820.17 ms
             forward:    6.37 ms   32.35 ms
        loss_compute:    0.78 ms    0.78 ms
     loss_all_reduce:    0.11 ms    0.11 ms
       meters_update:    5.00 ms    5.03 ms
            backward:    1.75 ms    1.76 ms
      optimizer_step:    0.39 ms    0.40 ms
    train_step_total:  862.32 ms  862.32 ms
INFO 2022-05-15 18:08:35,172 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 38.116}, 'top_5': {'res5': 67.624}}
INFO 2022-05-15 18:08:35,173 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:08:35,181 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:08:35,182 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 115, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 18:10:07,583 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 18:10:07,584 state_update_hooks.py: 115: Starting phase 115 [test]
INFO 2022-05-15 18:10:22,560 trainer_main.py: 214: Meters synced
INFO 2022-05-15 18:10:22,567 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 749
INFO 2022-05-15 18:10:22,567 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 30.0}, 'top_5': {'res5': 58.379999999999995}}
INFO 2022-05-15 18:10:22,568 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:10:22,575 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:10:22,576 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 116, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 18:11:50,156 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 18:11:50,157 state_update_hooks.py: 115: Starting phase 116 [train]
INFO 2022-05-15 18:13:20,849 trainer_main.py: 214: Meters synced
INFO 2022-05-15 18:13:20,854 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 925
INFO 2022-05-15 18:13:20,856 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  869.92 ms  870.00 ms
             forward:    6.06 ms   37.75 ms
        loss_compute:    0.92 ms    1.20 ms
     loss_all_reduce:    0.15 ms    0.11 ms
       meters_update:   11.74 ms   11.76 ms
            backward:    1.51 ms    1.65 ms
      optimizer_step:    0.51 ms    0.61 ms
    train_step_total:  925.22 ms  925.17 ms
INFO 2022-05-15 18:13:20,856 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 38.422}, 'top_5': {'res5': 67.688}}
INFO 2022-05-15 18:13:20,857 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:13:20,865 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:13:20,867 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 117, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 18:14:52,889 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 18:14:52,890 state_update_hooks.py: 115: Starting phase 117 [test]
INFO 2022-05-15 18:15:07,835 trainer_main.py: 214: Meters synced
INFO 2022-05-15 18:15:07,840 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 747
INFO 2022-05-15 18:15:07,841 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 30.94}, 'top_5': {'res5': 58.85}}
INFO 2022-05-15 18:15:07,842 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:15:07,853 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:15:07,854 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 118, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 18:16:42,894 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 18:16:42,895 state_update_hooks.py: 115: Starting phase 118 [train]
INFO 2022-05-15 18:16:58,193 log_hooks.py: 277: Rank: 0; [ep: 59] iter: 5800; lr: 0.3; loss: 2.48227; btime(ms): 2467; eta: 2:44:31; peak_mem(M): 2605;
INFO 2022-05-15 18:18:06,530 trainer_main.py: 214: Meters synced
INFO 2022-05-15 18:18:06,534 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 853
INFO 2022-05-15 18:18:06,535 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  806.17 ms  806.25 ms
             forward:    5.74 ms   34.24 ms
        loss_compute:    0.77 ms    0.87 ms
     loss_all_reduce:    0.10 ms    0.10 ms
       meters_update:    7.96 ms    7.99 ms
            backward:    1.56 ms    1.66 ms
      optimizer_step:    0.43 ms    0.42 ms
    train_step_total:  853.21 ms  853.23 ms
INFO 2022-05-15 18:18:06,536 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 38.448}, 'top_5': {'res5': 67.89}}
INFO 2022-05-15 18:18:06,537 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:18:06,546 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:18:06,546 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 119, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 18:19:37,293 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 18:19:37,294 state_update_hooks.py: 115: Starting phase 119 [test]
INFO 2022-05-15 18:19:52,517 trainer_main.py: 214: Meters synced
INFO 2022-05-15 18:19:52,525 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 761
INFO 2022-05-15 18:19:52,526 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 29.759999999999998}, 'top_5': {'res5': 58.06}}
INFO 2022-05-15 18:19:52,527 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:19:52,534 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:19:52,535 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 120, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 18:21:24,030 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 18:21:24,031 state_update_hooks.py: 115: Starting phase 120 [train]
INFO 2022-05-15 18:22:50,402 trainer_main.py: 214: Meters synced
INFO 2022-05-15 18:22:50,406 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 881
INFO 2022-05-15 18:22:50,408 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  835.46 ms  835.52 ms
             forward:    6.19 ms   33.46 ms
        loss_compute:    0.82 ms    0.90 ms
     loss_all_reduce:    0.11 ms    0.11 ms
       meters_update:    6.91 ms    6.93 ms
            backward:    1.90 ms    1.98 ms
      optimizer_step:    0.44 ms    0.45 ms
    train_step_total:  881.11 ms  881.12 ms
INFO 2022-05-15 18:22:50,409 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 41.252}, 'top_5': {'res5': 69.78}}
INFO 2022-05-15 18:22:50,410 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:22:50,420 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:22:50,421 log_hooks.py: 425: [phase: 60] Saving checkpoint to ./checkpoints/LP/2
INFO 2022-05-15 18:22:50,586 checkpoint.py: 131: Saved checkpoint: ./checkpoints/LP/2/model_phase60.torch
INFO 2022-05-15 18:22:50,587 checkpoint.py: 140: Creating symlink...
INFO 2022-05-15 18:22:50,596 checkpoint.py: 144: Created symlink: ./checkpoints/LP/2/checkpoint.torch
INFO 2022-05-15 18:22:50,597 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 121, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 18:24:24,846 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 18:24:24,847 state_update_hooks.py: 115: Starting phase 121 [test]
INFO 2022-05-15 18:24:39,003 trainer_main.py: 214: Meters synced
INFO 2022-05-15 18:24:39,009 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 708
INFO 2022-05-15 18:24:39,009 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.71}, 'top_5': {'res5': 60.589999999999996}}
INFO 2022-05-15 18:24:39,010 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:24:39,017 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:24:39,018 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 122, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 18:26:06,422 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 18:26:06,423 state_update_hooks.py: 115: Starting phase 122 [train]
INFO 2022-05-15 18:26:25,593 log_hooks.py: 277: Rank: 0; [ep: 61] iter: 6000; lr: 0.03; loss: 2.47779; btime(ms): 2464; eta: 2:36:04; peak_mem(M): 2605;
INFO 2022-05-15 18:27:36,234 trainer_main.py: 214: Meters synced
INFO 2022-05-15 18:27:36,241 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 916
INFO 2022-05-15 18:27:36,243 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  860.54 ms  860.57 ms
             forward:    5.51 ms   36.69 ms
        loss_compute:    0.83 ms    1.08 ms
     loss_all_reduce:    0.11 ms    0.10 ms
       meters_update:   13.72 ms   13.75 ms
            backward:    1.30 ms    1.50 ms
      optimizer_step:    0.53 ms    0.51 ms
    train_step_total:  916.22 ms  916.27 ms
INFO 2022-05-15 18:27:36,244 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 41.506}, 'top_5': {'res5': 70.148}}
INFO 2022-05-15 18:27:36,245 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:27:36,254 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:27:36,256 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 123, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 18:28:58,219 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 18:28:58,220 state_update_hooks.py: 115: Starting phase 123 [test]
INFO 2022-05-15 18:29:10,428 trainer_main.py: 214: Meters synced
INFO 2022-05-15 18:29:10,433 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 610
INFO 2022-05-15 18:29:10,434 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.78}, 'top_5': {'res5': 60.519999999999996}}
INFO 2022-05-15 18:29:10,435 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:29:10,442 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:29:10,443 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 124, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 18:30:35,203 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 18:30:35,205 state_update_hooks.py: 115: Starting phase 124 [train]
INFO 2022-05-15 18:31:50,987 trainer_main.py: 214: Meters synced
INFO 2022-05-15 18:31:50,992 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 773
INFO 2022-05-15 18:31:50,993 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  726.50 ms  726.58 ms
             forward:    6.23 ms   34.76 ms
        loss_compute:    0.89 ms    0.96 ms
     loss_all_reduce:    0.11 ms    0.11 ms
       meters_update:    6.66 ms    6.68 ms
            backward:    1.73 ms    1.79 ms
      optimizer_step:    0.42 ms    0.43 ms
    train_step_total:  773.08 ms  773.07 ms
INFO 2022-05-15 18:31:50,994 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 41.678}, 'top_5': {'res5': 70.084}}
INFO 2022-05-15 18:31:50,995 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:31:51,001 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:31:51,002 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 125, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 18:33:14,047 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 18:33:14,048 state_update_hooks.py: 115: Starting phase 125 [test]
INFO 2022-05-15 18:33:28,403 trainer_main.py: 214: Meters synced
INFO 2022-05-15 18:33:28,408 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 718
INFO 2022-05-15 18:33:28,409 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.79}, 'top_5': {'res5': 60.51}}
INFO 2022-05-15 18:33:28,410 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:33:28,416 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:33:28,416 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 126, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 18:34:50,213 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 18:34:50,214 state_update_hooks.py: 115: Starting phase 126 [train]
INFO 2022-05-15 18:35:08,581 log_hooks.py: 277: Rank: 0; [ep: 63] iter: 6200; lr: 0.03; loss: 2.30132; btime(ms): 2455; eta: 2:27:18; peak_mem(M): 2605;
INFO 2022-05-15 18:36:09,516 trainer_main.py: 214: Meters synced
INFO 2022-05-15 18:36:09,524 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 809
INFO 2022-05-15 18:36:09,525 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  758.05 ms  758.13 ms
             forward:    6.01 ms   35.86 ms
        loss_compute:    0.92 ms    1.10 ms
     loss_all_reduce:    0.11 ms    0.10 ms
       meters_update:    9.16 ms    9.22 ms
            backward:    1.69 ms    1.82 ms
      optimizer_step:    0.44 ms    0.42 ms
    train_step_total:  808.99 ms  809.01 ms
INFO 2022-05-15 18:36:09,526 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 41.724}, 'top_5': {'res5': 70.132}}
INFO 2022-05-15 18:36:09,527 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:36:09,539 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:36:09,540 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 127, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 18:37:36,229 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 18:37:36,230 state_update_hooks.py: 115: Starting phase 127 [test]
INFO 2022-05-15 18:37:48,698 trainer_main.py: 214: Meters synced
INFO 2022-05-15 18:37:48,703 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 623
INFO 2022-05-15 18:37:48,704 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.82}, 'top_5': {'res5': 60.35}}
INFO 2022-05-15 18:37:48,705 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:37:48,711 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:37:48,712 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 128, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 18:39:09,742 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 18:39:09,743 state_update_hooks.py: 115: Starting phase 128 [train]
INFO 2022-05-15 18:40:34,545 trainer_main.py: 214: Meters synced
INFO 2022-05-15 18:40:34,551 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 865
INFO 2022-05-15 18:40:34,552 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  808.28 ms  808.32 ms
             forward:    5.58 ms   37.69 ms
        loss_compute:    0.90 ms    1.16 ms
     loss_all_reduce:    0.11 ms    0.10 ms
       meters_update:   13.39 ms   13.50 ms
            backward:    1.35 ms    1.50 ms
      optimizer_step:    0.58 ms    0.61 ms
    train_step_total:  865.13 ms  865.14 ms
INFO 2022-05-15 18:40:34,553 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 41.734}, 'top_5': {'res5': 70.14}}
INFO 2022-05-15 18:40:34,554 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:40:34,560 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:40:34,561 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 129, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 18:41:53,979 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 18:41:53,980 state_update_hooks.py: 115: Starting phase 129 [test]
INFO 2022-05-15 18:42:08,184 trainer_main.py: 214: Meters synced
INFO 2022-05-15 18:42:08,188 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 710
INFO 2022-05-15 18:42:08,190 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.66}, 'top_5': {'res5': 60.370000000000005}}
INFO 2022-05-15 18:42:08,191 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:42:08,197 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:42:08,198 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 130, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 18:43:37,004 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 18:43:37,005 state_update_hooks.py: 115: Starting phase 130 [train]
INFO 2022-05-15 18:44:00,514 log_hooks.py: 277: Rank: 0; [ep: 65] iter: 6400; lr: 0.03; loss: 2.48393; btime(ms): 2447; eta: 2:18:42; peak_mem(M): 2605;
INFO 2022-05-15 18:44:53,391 trainer_main.py: 214: Meters synced
INFO 2022-05-15 18:44:53,398 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 779
INFO 2022-05-15 18:44:53,399 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  734.68 ms  734.75 ms
             forward:    6.59 ms   33.43 ms
        loss_compute:    1.00 ms    1.04 ms
     loss_all_reduce:    0.12 ms    0.13 ms
       meters_update:    5.66 ms    5.68 ms
            backward:    1.96 ms    2.01 ms
      optimizer_step:    0.40 ms    0.40 ms
    train_step_total:  779.22 ms  779.23 ms
INFO 2022-05-15 18:44:53,400 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 41.72}, 'top_5': {'res5': 70.14200000000001}}
INFO 2022-05-15 18:44:53,401 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:44:53,409 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:44:53,410 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 131, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 18:46:15,701 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 18:46:15,702 state_update_hooks.py: 115: Starting phase 131 [test]
INFO 2022-05-15 18:46:30,028 trainer_main.py: 214: Meters synced
INFO 2022-05-15 18:46:30,035 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 716
INFO 2022-05-15 18:46:30,036 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.84}, 'top_5': {'res5': 60.440000000000005}}
INFO 2022-05-15 18:46:30,037 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:46:30,044 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:46:30,045 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 132, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 18:47:52,456 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 18:47:52,458 state_update_hooks.py: 115: Starting phase 132 [train]
INFO 2022-05-15 18:49:14,085 trainer_main.py: 214: Meters synced
INFO 2022-05-15 18:49:14,090 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 832
INFO 2022-05-15 18:49:14,091 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  783.40 ms  783.43 ms
             forward:    5.93 ms   33.90 ms
        loss_compute:    0.79 ms    0.89 ms
     loss_all_reduce:    0.12 ms    0.12 ms
       meters_update:   10.45 ms   10.46 ms
            backward:    1.53 ms    1.61 ms
      optimizer_step:    0.47 ms    0.48 ms
    train_step_total:  832.69 ms  832.70 ms
INFO 2022-05-15 18:49:14,092 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 41.758}, 'top_5': {'res5': 70.18799999999999}}
INFO 2022-05-15 18:49:14,093 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:49:14,100 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:49:14,100 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 133, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 18:50:39,234 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 18:50:39,235 state_update_hooks.py: 115: Starting phase 133 [test]
INFO 2022-05-15 18:50:51,040 trainer_main.py: 214: Meters synced
INFO 2022-05-15 18:50:51,044 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 590
INFO 2022-05-15 18:50:51,045 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.800000000000004}, 'top_5': {'res5': 60.62}}
INFO 2022-05-15 18:50:51,046 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:50:51,052 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:50:51,052 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 134, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 18:52:10,251 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 18:52:10,252 state_update_hooks.py: 115: Starting phase 134 [train]
INFO 2022-05-15 18:52:38,309 log_hooks.py: 277: Rank: 0; [ep: 67] iter: 6600; lr: 0.03; loss: 2.23016; btime(ms): 2438; eta: 2:10:04; peak_mem(M): 2605;
INFO 2022-05-15 18:53:33,224 trainer_main.py: 214: Meters synced
INFO 2022-05-15 18:53:33,230 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 846
INFO 2022-05-15 18:53:33,232 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  787.12 ms  787.13 ms
             forward:    5.45 ms   38.25 ms
        loss_compute:    0.97 ms    1.30 ms
     loss_all_reduce:    0.14 ms    0.12 ms
       meters_update:   14.48 ms   14.51 ms
            backward:    1.86 ms    2.08 ms
      optimizer_step:    0.70 ms    0.65 ms
    train_step_total:  846.45 ms  846.46 ms
INFO 2022-05-15 18:53:33,232 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 41.796}, 'top_5': {'res5': 70.234}}
INFO 2022-05-15 18:53:33,233 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:53:33,239 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:53:33,240 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 135, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 18:54:54,385 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 18:54:54,386 state_update_hooks.py: 115: Starting phase 135 [test]
INFO 2022-05-15 18:55:08,722 trainer_main.py: 214: Meters synced
INFO 2022-05-15 18:55:08,729 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 717
INFO 2022-05-15 18:55:08,730 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.92}, 'top_5': {'res5': 60.36}}
INFO 2022-05-15 18:55:08,731 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:55:08,737 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:55:08,738 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 136, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 18:56:34,974 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 18:56:34,975 state_update_hooks.py: 115: Starting phase 136 [train]
INFO 2022-05-15 18:57:49,359 trainer_main.py: 214: Meters synced
INFO 2022-05-15 18:57:49,364 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 759
INFO 2022-05-15 18:57:49,364 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  714.45 ms  714.53 ms
             forward:    6.53 ms   33.21 ms
        loss_compute:    0.88 ms    0.92 ms
     loss_all_reduce:    0.11 ms    0.11 ms
       meters_update:    5.81 ms    5.83 ms
            backward:    1.96 ms    1.97 ms
      optimizer_step:    0.39 ms    0.42 ms
    train_step_total:  758.80 ms  758.81 ms
INFO 2022-05-15 18:57:49,366 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 41.726}, 'top_5': {'res5': 70.126}}
INFO 2022-05-15 18:57:49,366 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:57:49,372 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:57:49,373 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 137, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 18:59:11,522 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 18:59:11,523 state_update_hooks.py: 115: Starting phase 137 [test]
INFO 2022-05-15 18:59:25,194 trainer_main.py: 214: Meters synced
INFO 2022-05-15 18:59:25,204 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 684
INFO 2022-05-15 18:59:25,206 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.74}, 'top_5': {'res5': 60.28}}
INFO 2022-05-15 18:59:25,206 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:59:25,214 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 18:59:25,215 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 138, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 19:00:43,142 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 19:00:43,144 state_update_hooks.py: 115: Starting phase 138 [train]
INFO 2022-05-15 19:01:10,324 log_hooks.py: 277: Rank: 0; [ep: 69] iter: 6800; lr: 0.03; loss: 2.33923; btime(ms): 2430; eta: 2:01:30; peak_mem(M): 2605;
INFO 2022-05-15 19:02:02,268 trainer_main.py: 214: Meters synced
INFO 2022-05-15 19:02:02,273 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 807
INFO 2022-05-15 19:02:02,275 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  755.90 ms  755.98 ms
             forward:    6.27 ms   34.22 ms
        loss_compute:    0.77 ms    0.85 ms
     loss_all_reduce:    0.12 ms    0.12 ms
       meters_update:   12.13 ms   12.15 ms
            backward:    1.39 ms    1.51 ms
      optimizer_step:    0.50 ms    0.48 ms
    train_step_total:  807.18 ms  807.19 ms
INFO 2022-05-15 19:02:02,276 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 41.766}, 'top_5': {'res5': 70.206}}
INFO 2022-05-15 19:02:02,276 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:02:02,283 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:02:02,284 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 139, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 19:03:24,856 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 19:03:24,857 state_update_hooks.py: 115: Starting phase 139 [test]
INFO 2022-05-15 19:03:37,310 trainer_main.py: 214: Meters synced
INFO 2022-05-15 19:03:37,315 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 622
INFO 2022-05-15 19:03:37,316 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.71}, 'top_5': {'res5': 60.56}}
INFO 2022-05-15 19:03:37,317 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:03:37,323 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:03:37,324 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 140, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 19:04:55,523 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 19:04:55,524 state_update_hooks.py: 115: Starting phase 140 [train]
INFO 2022-05-15 19:06:19,079 trainer_main.py: 214: Meters synced
INFO 2022-05-15 19:06:19,080 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 852
INFO 2022-05-15 19:06:19,081 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  796.54 ms  796.59 ms
             forward:    5.70 ms   37.05 ms
        loss_compute:    0.90 ms    1.18 ms
     loss_all_reduce:    0.12 ms    0.11 ms
       meters_update:   13.16 ms   13.18 ms
            backward:    1.42 ms    1.66 ms
      optimizer_step:    0.53 ms    0.53 ms
    train_step_total:  852.38 ms  852.37 ms
INFO 2022-05-15 19:06:19,082 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 41.756}, 'top_5': {'res5': 70.332}}
INFO 2022-05-15 19:06:19,083 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:06:19,089 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:06:19,090 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 141, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 19:07:37,959 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 19:07:37,960 state_update_hooks.py: 115: Starting phase 141 [test]
INFO 2022-05-15 19:07:51,723 trainer_main.py: 214: Meters synced
INFO 2022-05-15 19:07:51,728 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 688
INFO 2022-05-15 19:07:51,729 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.89}, 'top_5': {'res5': 60.58}}
INFO 2022-05-15 19:07:51,729 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:07:51,738 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:07:51,738 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 142, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 19:09:15,854 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 19:09:15,857 state_update_hooks.py: 115: Starting phase 142 [train]
INFO 2022-05-15 19:09:50,690 log_hooks.py: 277: Rank: 0; [ep: 71] iter: 7000; lr: 0.03; loss: 2.33919; btime(ms): 2422; eta: 1:53:03; peak_mem(M): 2605;
INFO 2022-05-15 19:10:34,535 trainer_main.py: 214: Meters synced
INFO 2022-05-15 19:10:34,539 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 802
INFO 2022-05-15 19:10:34,540 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  757.52 ms  757.61 ms
             forward:    6.94 ms   33.51 ms
        loss_compute:    0.93 ms    0.98 ms
     loss_all_reduce:    0.11 ms    0.11 ms
       meters_update:    6.17 ms    6.18 ms
            backward:    1.95 ms    1.98 ms
      optimizer_step:    0.43 ms    0.45 ms
    train_step_total:  802.61 ms  802.62 ms
INFO 2022-05-15 19:10:34,541 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 41.674}, 'top_5': {'res5': 70.232}}
INFO 2022-05-15 19:10:34,541 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:10:34,549 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:10:34,550 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 143, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 19:11:56,845 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 19:11:56,845 state_update_hooks.py: 115: Starting phase 143 [test]
INFO 2022-05-15 19:12:10,847 trainer_main.py: 214: Meters synced
INFO 2022-05-15 19:12:10,852 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 700
INFO 2022-05-15 19:12:10,853 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.800000000000004}, 'top_5': {'res5': 60.56}}
INFO 2022-05-15 19:12:10,854 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:12:10,860 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:12:10,861 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 144, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 19:13:30,318 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 19:13:30,319 state_update_hooks.py: 115: Starting phase 144 [train]
INFO 2022-05-15 19:14:50,825 trainer_main.py: 214: Meters synced
INFO 2022-05-15 19:14:50,829 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 821
INFO 2022-05-15 19:14:50,831 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  771.50 ms  771.56 ms
             forward:    5.98 ms   34.53 ms
        loss_compute:    0.86 ms    0.99 ms
     loss_all_reduce:    0.13 ms    0.13 ms
       meters_update:    9.97 ms    9.98 ms
            backward:    1.65 ms    1.80 ms
      optimizer_step:    0.48 ms    0.48 ms
    train_step_total:  821.27 ms  821.29 ms
INFO 2022-05-15 19:14:50,832 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 41.904}, 'top_5': {'res5': 70.13000000000001}}
INFO 2022-05-15 19:14:50,833 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:14:50,843 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:14:50,844 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 145, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 19:16:11,632 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 19:16:11,633 state_update_hooks.py: 115: Starting phase 145 [test]
INFO 2022-05-15 19:16:23,593 trainer_main.py: 214: Meters synced
INFO 2022-05-15 19:16:23,598 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 598
INFO 2022-05-15 19:16:23,599 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.64}, 'top_5': {'res5': 60.62}}
INFO 2022-05-15 19:16:23,600 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:16:23,607 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:16:23,608 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 146, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 19:17:41,057 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 19:17:41,058 state_update_hooks.py: 115: Starting phase 146 [train]
INFO 2022-05-15 19:18:18,975 log_hooks.py: 277: Rank: 0; [ep: 73] iter: 7200; lr: 0.03; loss: 2.34035; btime(ms): 2414; eta: 1:44:36; peak_mem(M): 2605;
INFO 2022-05-15 19:19:03,217 trainer_main.py: 214: Meters synced
INFO 2022-05-15 19:19:03,218 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 838
INFO 2022-05-15 19:19:03,219 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  782.55 ms  782.66 ms
             forward:    5.73 ms   37.16 ms
        loss_compute:    0.89 ms    1.14 ms
     loss_all_reduce:    0.11 ms    0.10 ms
       meters_update:   13.27 ms   13.30 ms
            backward:    1.20 ms    1.38 ms
      optimizer_step:    0.47 ms    0.48 ms
    train_step_total:  838.15 ms  838.18 ms
INFO 2022-05-15 19:19:03,220 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 41.798}, 'top_5': {'res5': 70.352}}
INFO 2022-05-15 19:19:03,221 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:19:03,225 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:19:03,225 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 147, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 19:20:23,043 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 19:20:23,044 state_update_hooks.py: 115: Starting phase 147 [test]
INFO 2022-05-15 19:20:37,208 trainer_main.py: 214: Meters synced
INFO 2022-05-15 19:20:37,213 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 708
INFO 2022-05-15 19:20:37,214 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.72}, 'top_5': {'res5': 60.67}}
INFO 2022-05-15 19:20:37,214 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:20:37,221 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:20:37,221 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 148, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 19:22:03,282 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 19:22:03,284 state_update_hooks.py: 115: Starting phase 148 [train]
INFO 2022-05-15 19:23:16,349 trainer_main.py: 214: Meters synced
INFO 2022-05-15 19:23:16,353 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 745
INFO 2022-05-15 19:23:16,353 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  701.96 ms  701.97 ms
             forward:    6.67 ms   32.84 ms
        loss_compute:    0.90 ms    0.92 ms
     loss_all_reduce:    0.11 ms    0.20 ms
       meters_update:    5.40 ms    5.42 ms
            backward:    1.87 ms    1.91 ms
      optimizer_step:    0.39 ms    0.38 ms
    train_step_total:  745.34 ms  745.34 ms
INFO 2022-05-15 19:23:16,354 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 41.902}, 'top_5': {'res5': 70.334}}
INFO 2022-05-15 19:23:16,355 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:23:16,361 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:23:16,362 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 149, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 19:24:39,624 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 19:24:39,626 state_update_hooks.py: 115: Starting phase 149 [test]
INFO 2022-05-15 19:24:53,727 trainer_main.py: 214: Meters synced
INFO 2022-05-15 19:24:53,733 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 705
INFO 2022-05-15 19:24:53,734 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.79}, 'top_5': {'res5': 60.56}}
INFO 2022-05-15 19:24:53,735 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:24:53,742 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:24:53,742 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 150, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 19:26:08,684 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 19:26:08,685 state_update_hooks.py: 115: Starting phase 150 [train]
INFO 2022-05-15 19:26:46,097 log_hooks.py: 277: Rank: 0; [ep: 75] iter: 7400; lr: 0.03; loss: 2.25966; btime(ms): 2405; eta: 1:36:14; peak_mem(M): 2605;
INFO 2022-05-15 19:27:26,882 trainer_main.py: 214: Meters synced
INFO 2022-05-15 19:27:26,888 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 797
INFO 2022-05-15 19:27:26,889 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  744.19 ms  744.26 ms
             forward:    6.60 ms   36.45 ms
        loss_compute:    0.96 ms    1.15 ms
     loss_all_reduce:    0.13 ms    0.12 ms
       meters_update:   11.39 ms   11.42 ms
            backward:    1.53 ms    1.71 ms
      optimizer_step:    0.48 ms    0.52 ms
    train_step_total:  797.67 ms  797.69 ms
INFO 2022-05-15 19:27:26,893 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 41.808}, 'top_5': {'res5': 70.332}}
INFO 2022-05-15 19:27:26,894 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:27:26,907 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:27:26,908 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 151, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 19:28:46,620 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 19:28:46,621 state_update_hooks.py: 115: Starting phase 151 [test]
INFO 2022-05-15 19:28:58,365 trainer_main.py: 214: Meters synced
INFO 2022-05-15 19:28:58,370 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 587
INFO 2022-05-15 19:28:58,371 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.72}, 'top_5': {'res5': 60.57}}
INFO 2022-05-15 19:28:58,371 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:28:58,377 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:28:58,378 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 152, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 19:30:16,058 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 19:30:16,059 state_update_hooks.py: 115: Starting phase 152 [train]
INFO 2022-05-15 19:31:38,264 trainer_main.py: 214: Meters synced
INFO 2022-05-15 19:31:38,268 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 838
INFO 2022-05-15 19:31:38,270 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  783.18 ms  783.23 ms
             forward:    5.85 ms   36.39 ms
        loss_compute:    0.96 ms    1.19 ms
     loss_all_reduce:    0.12 ms    0.12 ms
       meters_update:   13.45 ms   13.48 ms
            backward:    1.41 ms    1.55 ms
      optimizer_step:    0.56 ms    0.56 ms
    train_step_total:  838.62 ms  838.64 ms
INFO 2022-05-15 19:31:38,271 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 41.852000000000004}, 'top_5': {'res5': 70.30199999999999}}
INFO 2022-05-15 19:31:38,271 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:31:38,279 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:31:38,279 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 153, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 19:32:56,990 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 19:32:56,991 state_update_hooks.py: 115: Starting phase 153 [test]
INFO 2022-05-15 19:33:11,095 trainer_main.py: 214: Meters synced
INFO 2022-05-15 19:33:11,103 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 705
INFO 2022-05-15 19:33:11,104 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.769999999999996}, 'top_5': {'res5': 60.6}}
INFO 2022-05-15 19:33:11,104 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:33:11,111 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:33:11,112 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 154, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 19:34:34,544 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 19:34:34,545 state_update_hooks.py: 115: Starting phase 154 [train]
INFO 2022-05-15 19:35:14,530 log_hooks.py: 277: Rank: 0; [ep: 77] iter: 7600; lr: 0.03; loss: 2.25559; btime(ms): 2398; eta: 1:27:56; peak_mem(M): 2605;
INFO 2022-05-15 19:35:47,563 trainer_main.py: 214: Meters synced
INFO 2022-05-15 19:35:47,568 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 745
INFO 2022-05-15 19:35:47,569 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  700.34 ms  700.38 ms
             forward:    6.81 ms   32.74 ms
        loss_compute:    0.86 ms    0.88 ms
     loss_all_reduce:    0.14 ms    0.11 ms
       meters_update:    6.60 ms    6.62 ms
            backward:    1.77 ms    1.81 ms
      optimizer_step:    0.42 ms    0.43 ms
    train_step_total:  744.85 ms  744.84 ms
INFO 2022-05-15 19:35:47,570 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 41.868}, 'top_5': {'res5': 70.14}}
INFO 2022-05-15 19:35:47,570 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:35:47,576 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:35:47,577 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 155, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 19:37:08,434 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 19:37:08,435 state_update_hooks.py: 115: Starting phase 155 [test]
INFO 2022-05-15 19:37:22,389 trainer_main.py: 214: Meters synced
INFO 2022-05-15 19:37:22,393 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 697
INFO 2022-05-15 19:37:22,395 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.82}, 'top_5': {'res5': 60.589999999999996}}
INFO 2022-05-15 19:37:22,396 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:37:22,403 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:37:22,404 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 156, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 19:38:42,103 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 19:38:42,104 state_update_hooks.py: 115: Starting phase 156 [train]
INFO 2022-05-15 19:39:58,528 trainer_main.py: 214: Meters synced
INFO 2022-05-15 19:39:58,534 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 779
INFO 2022-05-15 19:39:58,535 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  728.66 ms  728.74 ms
             forward:    6.17 ms   34.95 ms
        loss_compute:    0.92 ms    1.06 ms
     loss_all_reduce:    0.12 ms    0.11 ms
       meters_update:   10.45 ms   10.47 ms
            backward:    1.62 ms    1.68 ms
      optimizer_step:    0.53 ms    0.58 ms
    train_step_total:  779.60 ms  779.59 ms
INFO 2022-05-15 19:39:58,536 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 41.922}, 'top_5': {'res5': 70.38}}
INFO 2022-05-15 19:39:58,536 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:39:58,543 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:39:58,544 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 157, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 19:41:17,222 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 19:41:17,225 state_update_hooks.py: 115: Starting phase 157 [test]
INFO 2022-05-15 19:41:28,871 trainer_main.py: 214: Meters synced
INFO 2022-05-15 19:41:28,876 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 582
INFO 2022-05-15 19:41:28,877 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.75}, 'top_5': {'res5': 60.550000000000004}}
INFO 2022-05-15 19:41:28,878 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:41:28,884 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:41:28,885 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 158, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 19:42:45,265 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 19:42:45,266 state_update_hooks.py: 115: Starting phase 158 [train]
INFO 2022-05-15 19:43:31,562 log_hooks.py: 277: Rank: 0; [ep: 79] iter: 7800; lr: 0.03; loss: 2.23436; btime(ms): 2389; eta: 1:19:39; peak_mem(M): 2605;
INFO 2022-05-15 19:44:03,279 trainer_main.py: 214: Meters synced
INFO 2022-05-15 19:44:03,285 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 796
INFO 2022-05-15 19:44:03,286 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  732.46 ms  732.62 ms
             forward:    6.00 ms   40.36 ms
        loss_compute:    0.98 ms    1.29 ms
     loss_all_reduce:    0.11 ms    0.10 ms
       meters_update:   16.72 ms   16.75 ms
            backward:    1.49 ms    1.68 ms
      optimizer_step:    0.61 ms    0.66 ms
    train_step_total:  795.79 ms  795.85 ms
INFO 2022-05-15 19:44:03,287 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 41.86}, 'top_5': {'res5': 70.35}}
INFO 2022-05-15 19:44:03,288 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:44:03,295 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:44:03,296 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 159, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 19:45:17,979 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 19:45:17,980 state_update_hooks.py: 115: Starting phase 159 [test]
INFO 2022-05-15 19:45:31,424 trainer_main.py: 214: Meters synced
INFO 2022-05-15 19:45:31,431 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 672
INFO 2022-05-15 19:45:31,432 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.940000000000005}, 'top_5': {'res5': 60.73}}
INFO 2022-05-15 19:45:31,432 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:45:31,438 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:45:31,439 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 160, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 19:46:50,726 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 19:46:50,727 state_update_hooks.py: 115: Starting phase 160 [train]
INFO 2022-05-15 19:48:01,304 trainer_main.py: 214: Meters synced
INFO 2022-05-15 19:48:01,310 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 720
INFO 2022-05-15 19:48:01,311 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  674.81 ms  674.88 ms
             forward:    6.25 ms   32.97 ms
        loss_compute:    0.84 ms    0.85 ms
     loss_all_reduce:    0.11 ms    0.11 ms
       meters_update:    6.99 ms    7.02 ms
            backward:    1.75 ms    1.81 ms
      optimizer_step:    0.44 ms    0.44 ms
    train_step_total:  719.94 ms  719.96 ms
INFO 2022-05-15 19:48:01,312 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 41.94}, 'top_5': {'res5': 70.57}}
INFO 2022-05-15 19:48:01,313 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:48:01,320 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:48:01,321 log_hooks.py: 425: [phase: 80] Saving checkpoint to ./checkpoints/LP/2
INFO 2022-05-15 19:48:01,469 checkpoint.py: 131: Saved checkpoint: ./checkpoints/LP/2/model_phase80.torch
INFO 2022-05-15 19:48:01,470 checkpoint.py: 140: Creating symlink...
INFO 2022-05-15 19:48:01,479 checkpoint.py: 144: Created symlink: ./checkpoints/LP/2/checkpoint.torch
INFO 2022-05-15 19:48:01,480 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 161, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 19:49:19,335 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 19:49:19,336 state_update_hooks.py: 115: Starting phase 161 [test]
INFO 2022-05-15 19:49:33,087 trainer_main.py: 214: Meters synced
INFO 2022-05-15 19:49:33,095 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 687
INFO 2022-05-15 19:49:33,096 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.84}, 'top_5': {'res5': 60.77}}
INFO 2022-05-15 19:49:33,097 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:49:33,107 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:49:33,108 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 162, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 19:50:47,853 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 19:50:47,855 state_update_hooks.py: 115: Starting phase 162 [train]
INFO 2022-05-15 19:51:29,932 log_hooks.py: 277: Rank: 0; [ep: 81] iter: 8000; lr: 0.003; loss: 2.43583; btime(ms): 2380; eta: 1:11:24; peak_mem(M): 2605;
INFO 2022-05-15 19:51:57,148 trainer_main.py: 214: Meters synced
INFO 2022-05-15 19:51:57,153 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 707
INFO 2022-05-15 19:51:57,154 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  657.53 ms  657.57 ms
             forward:    6.08 ms   34.02 ms
        loss_compute:    0.82 ms    0.89 ms
     loss_all_reduce:    0.12 ms    0.12 ms
       meters_update:   10.16 ms   10.18 ms
            backward:    1.56 ms    1.64 ms
      optimizer_step:    0.46 ms    0.47 ms
    train_step_total:  706.87 ms  706.89 ms
INFO 2022-05-15 19:51:57,155 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 42.082}, 'top_5': {'res5': 70.578}}
INFO 2022-05-15 19:51:57,155 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:51:57,162 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:51:57,162 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 163, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 19:53:10,528 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 19:53:10,530 state_update_hooks.py: 115: Starting phase 163 [test]
INFO 2022-05-15 19:53:21,250 trainer_main.py: 214: Meters synced
INFO 2022-05-15 19:53:21,255 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 536
INFO 2022-05-15 19:53:21,256 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.769999999999996}, 'top_5': {'res5': 60.69}}
INFO 2022-05-15 19:53:21,257 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:53:21,263 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:53:21,264 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 164, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 19:54:34,330 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 19:54:34,336 state_update_hooks.py: 115: Starting phase 164 [train]
INFO 2022-05-15 19:55:53,540 trainer_main.py: 214: Meters synced
INFO 2022-05-15 19:55:53,546 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 808
INFO 2022-05-15 19:55:53,549 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  744.34 ms  744.40 ms
             forward:    6.22 ms   40.24 ms
        loss_compute:    1.17 ms    1.52 ms
     loss_all_reduce:    0.12 ms    0.15 ms
       meters_update:   16.61 ms   16.72 ms
            backward:    1.55 ms    1.86 ms
      optimizer_step:    0.68 ms    0.69 ms
    train_step_total:  807.99 ms  807.91 ms
INFO 2022-05-15 19:55:53,550 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 42.178}, 'top_5': {'res5': 70.574}}
INFO 2022-05-15 19:55:53,551 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:55:53,559 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:55:53,560 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 165, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 19:57:04,833 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 19:57:04,834 state_update_hooks.py: 115: Starting phase 165 [test]
INFO 2022-05-15 19:57:16,077 trainer_main.py: 214: Meters synced
INFO 2022-05-15 19:57:16,082 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 562
INFO 2022-05-15 19:57:16,083 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.940000000000005}, 'top_5': {'res5': 60.6}}
INFO 2022-05-15 19:57:16,084 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:57:16,091 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:57:16,092 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 166, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 19:58:33,795 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 19:58:33,796 state_update_hooks.py: 115: Starting phase 166 [train]
INFO 2022-05-15 19:59:21,978 log_hooks.py: 277: Rank: 0; [ep: 83] iter: 8200; lr: 0.003; loss: 2.25143; btime(ms): 2370; eta: 1:03:12; peak_mem(M): 2605;
INFO 2022-05-15 19:59:44,701 trainer_main.py: 214: Meters synced
INFO 2022-05-15 19:59:44,705 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 723
INFO 2022-05-15 19:59:44,706 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  672.66 ms  672.70 ms
             forward:    5.89 ms   33.98 ms
        loss_compute:    1.07 ms    1.12 ms
     loss_all_reduce:    0.13 ms    0.15 ms
       meters_update:   10.10 ms   10.15 ms
            backward:    1.76 ms    1.83 ms
      optimizer_step:    0.45 ms    0.48 ms
    train_step_total:  723.30 ms  723.32 ms
INFO 2022-05-15 19:59:44,707 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 42.15}, 'top_5': {'res5': 70.584}}
INFO 2022-05-15 19:59:44,708 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:59:44,714 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 19:59:44,715 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 167, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 20:00:59,933 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 20:00:59,934 state_update_hooks.py: 115: Starting phase 167 [test]
INFO 2022-05-15 20:01:13,381 trainer_main.py: 214: Meters synced
INFO 2022-05-15 20:01:13,387 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 672
INFO 2022-05-15 20:01:13,388 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.800000000000004}, 'top_5': {'res5': 60.68}}
INFO 2022-05-15 20:01:13,389 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:01:13,396 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:01:13,397 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 168, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 20:02:28,909 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 20:02:28,910 state_update_hooks.py: 115: Starting phase 168 [train]
INFO 2022-05-15 20:03:39,190 trainer_main.py: 214: Meters synced
INFO 2022-05-15 20:03:39,195 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 717
INFO 2022-05-15 20:03:39,196 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  668.80 ms  668.83 ms
             forward:    6.55 ms   34.39 ms
        loss_compute:    0.91 ms    1.01 ms
     loss_all_reduce:    0.14 ms    0.11 ms
       meters_update:    8.48 ms    8.51 ms
            backward:    1.62 ms    1.67 ms
      optimizer_step:    0.46 ms    0.47 ms
    train_step_total:  716.93 ms  716.92 ms
INFO 2022-05-15 20:03:39,197 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 42.282}, 'top_5': {'res5': 70.588}}
INFO 2022-05-15 20:03:39,198 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:03:39,204 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:03:39,205 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 169, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 20:04:58,833 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 20:04:58,834 state_update_hooks.py: 115: Starting phase 169 [test]
INFO 2022-05-15 20:05:11,299 trainer_main.py: 214: Meters synced
INFO 2022-05-15 20:05:11,305 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 623
INFO 2022-05-15 20:05:11,306 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.68}, 'top_5': {'res5': 60.540000000000006}}
INFO 2022-05-15 20:05:11,306 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:05:11,313 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:05:11,314 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 170, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 20:06:23,099 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 20:06:23,101 state_update_hooks.py: 115: Starting phase 170 [train]
INFO 2022-05-15 20:07:16,749 log_hooks.py: 277: Rank: 0; [ep: 85] iter: 8400; lr: 0.003; loss: 2.183; btime(ms): 2360; eta: 0:55:05; peak_mem(M): 2605;
INFO 2022-05-15 20:07:37,367 trainer_main.py: 214: Meters synced
INFO 2022-05-15 20:07:37,372 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 757
INFO 2022-05-15 20:07:37,373 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  698.91 ms  698.90 ms
             forward:    5.69 ms   37.54 ms
        loss_compute:    1.07 ms    1.34 ms
     loss_all_reduce:    0.12 ms    0.09 ms
       meters_update:   14.92 ms   14.94 ms
            backward:    1.33 ms    1.50 ms
      optimizer_step:    0.54 ms    0.59 ms
    train_step_total:  757.56 ms  757.59 ms
INFO 2022-05-15 20:07:37,374 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 42.192}, 'top_5': {'res5': 70.572}}
INFO 2022-05-15 20:07:37,375 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:07:37,383 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:07:37,383 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 171, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 20:08:49,631 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 20:08:49,632 state_update_hooks.py: 115: Starting phase 171 [test]
INFO 2022-05-15 20:09:00,880 trainer_main.py: 214: Meters synced
INFO 2022-05-15 20:09:00,887 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 562
INFO 2022-05-15 20:09:00,888 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.98}, 'top_5': {'res5': 60.57}}
INFO 2022-05-15 20:09:00,889 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:09:00,895 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:09:00,896 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 172, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 20:10:17,125 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 20:10:17,126 state_update_hooks.py: 115: Starting phase 172 [train]
INFO 2022-05-15 20:11:31,373 trainer_main.py: 214: Meters synced
INFO 2022-05-15 20:11:31,377 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 757
INFO 2022-05-15 20:11:31,378 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  703.39 ms  703.51 ms
             forward:    6.02 ms   37.51 ms
        loss_compute:    0.96 ms    1.19 ms
     loss_all_reduce:    0.12 ms    0.10 ms
       meters_update:   10.67 ms   10.74 ms
            backward:    1.61 ms    1.80 ms
      optimizer_step:    0.52 ms    0.47 ms
    train_step_total:  757.39 ms  757.42 ms
INFO 2022-05-15 20:11:31,379 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 42.176}, 'top_5': {'res5': 70.544}}
INFO 2022-05-15 20:11:31,380 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:11:31,385 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:11:31,386 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 173, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 20:12:43,182 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 20:12:43,183 state_update_hooks.py: 115: Starting phase 173 [test]
INFO 2022-05-15 20:12:56,716 trainer_main.py: 214: Meters synced
INFO 2022-05-15 20:12:56,723 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 676
INFO 2022-05-15 20:12:56,727 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.83}, 'top_5': {'res5': 60.660000000000004}}
INFO 2022-05-15 20:12:56,727 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:12:56,736 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:12:56,737 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 174, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 20:14:14,114 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 20:14:14,115 state_update_hooks.py: 115: Starting phase 174 [train]
INFO 2022-05-15 20:15:04,989 log_hooks.py: 277: Rank: 0; [ep: 87] iter: 8600; lr: 0.003; loss: 2.25011; btime(ms): 2351; eta: 0:47:01; peak_mem(M): 2605;
INFO 2022-05-15 20:15:22,099 trainer_main.py: 214: Meters synced
INFO 2022-05-15 20:15:22,106 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 693
INFO 2022-05-15 20:15:22,107 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  651.04 ms  651.08 ms
             forward:    6.43 ms   32.10 ms
        loss_compute:    0.78 ms    0.78 ms
     loss_all_reduce:    0.11 ms    0.12 ms
       meters_update:    5.41 ms    5.43 ms
            backward:    1.85 ms    1.85 ms
      optimizer_step:    0.38 ms    0.39 ms
    train_step_total:  693.48 ms  693.48 ms
INFO 2022-05-15 20:15:22,109 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 42.324}, 'top_5': {'res5': 70.512}}
INFO 2022-05-15 20:15:22,109 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:15:22,114 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:15:22,115 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 175, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 20:16:39,342 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 20:16:39,343 state_update_hooks.py: 115: Starting phase 175 [test]
INFO 2022-05-15 20:16:52,423 trainer_main.py: 214: Meters synced
INFO 2022-05-15 20:16:52,428 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 654
INFO 2022-05-15 20:16:52,429 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.84}, 'top_5': {'res5': 60.650000000000006}}
INFO 2022-05-15 20:16:52,430 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:16:52,436 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:16:52,437 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 176, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 20:18:00,881 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 20:18:00,882 state_update_hooks.py: 115: Starting phase 176 [train]
INFO 2022-05-15 20:19:15,093 trainer_main.py: 214: Meters synced
INFO 2022-05-15 20:19:15,100 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 757
INFO 2022-05-15 20:19:15,100 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  697.45 ms  697.60 ms
             forward:    6.19 ms   37.50 ms
        loss_compute:    0.90 ms    1.06 ms
     loss_all_reduce:    0.13 ms    0.11 ms
       meters_update:   16.26 ms   16.30 ms
            backward:    1.45 ms    1.58 ms
      optimizer_step:    0.58 ms    0.63 ms
    train_step_total:  757.03 ms  757.05 ms
INFO 2022-05-15 20:19:15,101 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 42.224000000000004}, 'top_5': {'res5': 70.54}}
INFO 2022-05-15 20:19:15,102 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:19:15,108 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:19:15,109 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 177, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 20:20:29,230 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 20:20:29,231 state_update_hooks.py: 115: Starting phase 177 [test]
INFO 2022-05-15 20:20:40,900 trainer_main.py: 214: Meters synced
INFO 2022-05-15 20:20:40,905 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 583
INFO 2022-05-15 20:20:40,906 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.910000000000004}, 'top_5': {'res5': 60.629999999999995}}
INFO 2022-05-15 20:20:40,907 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:20:40,913 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:20:40,914 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 178, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 20:21:54,885 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 20:21:54,886 state_update_hooks.py: 115: Starting phase 178 [train]
INFO 2022-05-15 20:22:57,750 log_hooks.py: 277: Rank: 0; [ep: 89] iter: 8800; lr: 0.003; loss: 2.22602; btime(ms): 2342; eta: 0:39:02; peak_mem(M): 2605;
INFO 2022-05-15 20:23:12,079 trainer_main.py: 214: Meters synced
INFO 2022-05-15 20:23:12,083 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 787
INFO 2022-05-15 20:23:12,084 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  730.33 ms  730.32 ms
             forward:    6.08 ms   36.45 ms
        loss_compute:    1.00 ms    1.16 ms
     loss_all_reduce:    0.12 ms    0.12 ms
       meters_update:   14.83 ms   14.89 ms
            backward:    1.44 ms    1.52 ms
      optimizer_step:    0.58 ms    0.61 ms
    train_step_total:  787.43 ms  787.42 ms
INFO 2022-05-15 20:23:12,085 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 42.248000000000005}, 'top_5': {'res5': 70.604}}
INFO 2022-05-15 20:23:12,086 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:23:12,092 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:23:12,093 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 179, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 20:24:23,152 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 20:24:23,154 state_update_hooks.py: 115: Starting phase 179 [test]
INFO 2022-05-15 20:24:36,392 trainer_main.py: 214: Meters synced
INFO 2022-05-15 20:24:36,397 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 662
INFO 2022-05-15 20:24:36,398 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.81}, 'top_5': {'res5': 60.67}}
INFO 2022-05-15 20:24:36,399 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:24:36,403 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:24:36,404 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 180, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 20:25:53,207 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 20:25:53,208 state_update_hooks.py: 115: Starting phase 180 [train]
INFO 2022-05-15 20:26:58,563 trainer_main.py: 214: Meters synced
INFO 2022-05-15 20:26:58,566 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 666
INFO 2022-05-15 20:26:58,567 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  621.83 ms  621.88 ms
             forward:    6.87 ms   33.47 ms
        loss_compute:    0.89 ms    0.92 ms
     loss_all_reduce:    0.11 ms    0.12 ms
       meters_update:    6.05 ms    6.07 ms
            backward:    1.87 ms    1.92 ms
      optimizer_step:    0.43 ms    0.44 ms
    train_step_total:  666.65 ms  666.67 ms
INFO 2022-05-15 20:26:58,568 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 42.07}, 'top_5': {'res5': 70.598}}
INFO 2022-05-15 20:26:58,569 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:26:58,576 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:26:58,577 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 181, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 20:28:16,898 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 20:28:16,899 state_update_hooks.py: 115: Starting phase 181 [test]
INFO 2022-05-15 20:28:29,912 trainer_main.py: 214: Meters synced
INFO 2022-05-15 20:28:29,918 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 650
INFO 2022-05-15 20:28:29,918 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.89}, 'top_5': {'res5': 60.72}}
INFO 2022-05-15 20:28:29,919 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:28:29,926 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:28:29,927 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 182, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 20:29:37,268 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 20:29:37,269 state_update_hooks.py: 115: Starting phase 182 [train]
INFO 2022-05-15 20:30:37,137 log_hooks.py: 277: Rank: 0; [ep: 91] iter: 9000; lr: 0.003; loss: 2.2498; btime(ms): 2333; eta: 0:31:06; peak_mem(M): 2605;
INFO 2022-05-15 20:30:49,032 trainer_main.py: 214: Meters synced
INFO 2022-05-15 20:30:49,037 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 732
INFO 2022-05-15 20:30:49,038 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  677.99 ms  678.06 ms
             forward:    6.46 ms   36.54 ms
        loss_compute:    0.89 ms    1.06 ms
     loss_all_reduce:    0.12 ms    0.11 ms
       meters_update:   12.01 ms   12.03 ms
            backward:    1.61 ms    1.70 ms
      optimizer_step:    0.52 ms    0.48 ms
    train_step_total:  732.06 ms  732.04 ms
INFO 2022-05-15 20:30:49,039 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 42.186}, 'top_5': {'res5': 70.584}}
INFO 2022-05-15 20:30:49,039 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:30:49,045 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:30:49,046 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 183, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 20:32:02,979 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 20:32:02,980 state_update_hooks.py: 115: Starting phase 183 [test]
INFO 2022-05-15 20:32:14,174 trainer_main.py: 214: Meters synced
INFO 2022-05-15 20:32:14,178 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 559
INFO 2022-05-15 20:32:14,179 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.9}, 'top_5': {'res5': 60.78}}
INFO 2022-05-15 20:32:14,180 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:32:14,185 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:32:14,186 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 184, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 20:33:27,882 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 20:33:27,883 state_update_hooks.py: 115: Starting phase 184 [train]
INFO 2022-05-15 20:34:45,030 trainer_main.py: 214: Meters synced
INFO 2022-05-15 20:34:45,036 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 787
INFO 2022-05-15 20:34:45,037 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  735.54 ms  735.60 ms
             forward:    5.80 ms   32.72 ms
        loss_compute:    0.68 ms    0.72 ms
     loss_all_reduce:    0.12 ms    0.12 ms
       meters_update:   13.92 ms   13.96 ms
            backward:    1.36 ms    1.44 ms
      optimizer_step:    0.56 ms    0.56 ms
    train_step_total:  786.97 ms  786.97 ms
INFO 2022-05-15 20:34:45,038 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 42.116}, 'top_5': {'res5': 70.536}}
INFO 2022-05-15 20:34:45,038 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:34:45,045 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:34:45,045 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 185, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 20:35:56,140 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 20:35:56,141 state_update_hooks.py: 115: Starting phase 185 [test]
INFO 2022-05-15 20:36:08,160 trainer_main.py: 214: Meters synced
INFO 2022-05-15 20:36:08,165 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 601
INFO 2022-05-15 20:36:08,166 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.83}, 'top_5': {'res5': 60.64000000000001}}
INFO 2022-05-15 20:36:08,166 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:36:08,173 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:36:08,174 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 186, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 20:37:28,074 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 20:37:28,075 state_update_hooks.py: 115: Starting phase 186 [train]
INFO 2022-05-15 20:38:29,266 log_hooks.py: 277: Rank: 0; [ep: 93] iter: 9200; lr: 0.003; loss: 2.23017; btime(ms): 2325; eta: 0:23:15; peak_mem(M): 2605;
INFO 2022-05-15 20:38:37,189 trainer_main.py: 214: Meters synced
INFO 2022-05-15 20:38:37,193 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 705
INFO 2022-05-15 20:38:37,194 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  654.53 ms  654.57 ms
             forward:    6.60 ms   35.07 ms
        loss_compute:    0.97 ms    1.07 ms
     loss_all_reduce:    0.12 ms    0.12 ms
       meters_update:    9.04 ms    9.06 ms
            backward:    2.01 ms    2.12 ms
      optimizer_step:    0.44 ms    0.49 ms
    train_step_total:  705.02 ms  705.04 ms
INFO 2022-05-15 20:38:37,195 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 42.104}, 'top_5': {'res5': 70.568}}
INFO 2022-05-15 20:38:37,196 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:38:37,204 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:38:37,204 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 187, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 20:39:51,722 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 20:39:51,723 state_update_hooks.py: 115: Starting phase 187 [test]
INFO 2022-05-15 20:40:04,974 trainer_main.py: 214: Meters synced
INFO 2022-05-15 20:40:04,980 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 662
INFO 2022-05-15 20:40:04,981 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.75}, 'top_5': {'res5': 60.69}}
INFO 2022-05-15 20:40:04,981 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:40:04,987 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:40:04,988 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 188, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 20:41:19,901 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 20:41:19,902 state_update_hooks.py: 115: Starting phase 188 [train]
INFO 2022-05-15 20:42:30,724 trainer_main.py: 214: Meters synced
INFO 2022-05-15 20:42:30,729 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 722
INFO 2022-05-15 20:42:30,730 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  671.42 ms  671.40 ms
             forward:    7.86 ms   35.49 ms
        loss_compute:    0.89 ms    0.98 ms
     loss_all_reduce:    0.12 ms    0.11 ms
       meters_update:   10.05 ms   10.07 ms
            backward:    1.77 ms    1.87 ms
      optimizer_step:    0.50 ms    0.49 ms
    train_step_total:  722.45 ms  722.47 ms
INFO 2022-05-15 20:42:30,731 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 42.198}, 'top_5': {'res5': 70.6}}
INFO 2022-05-15 20:42:30,732 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:42:30,739 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:42:30,740 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 189, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 20:43:47,481 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 20:43:47,482 state_update_hooks.py: 115: Starting phase 189 [test]
INFO 2022-05-15 20:43:58,516 trainer_main.py: 214: Meters synced
INFO 2022-05-15 20:43:58,521 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 551
INFO 2022-05-15 20:43:58,521 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.79}, 'top_5': {'res5': 60.77}}
INFO 2022-05-15 20:43:58,522 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:43:58,527 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:43:58,528 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 190, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 20:45:09,034 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 20:45:09,036 state_update_hooks.py: 115: Starting phase 190 [train]
INFO 2022-05-15 20:46:19,300 log_hooks.py: 277: Rank: 0; [ep: 95] iter: 9400; lr: 0.003; loss: 2.321; btime(ms): 2317; eta: 0:15:26; peak_mem(M): 2605;
INFO 2022-05-15 20:46:25,400 trainer_main.py: 214: Meters synced
INFO 2022-05-15 20:46:25,405 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 779
INFO 2022-05-15 20:46:25,406 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  719.84 ms  719.94 ms
             forward:    6.01 ms   37.67 ms
        loss_compute:    0.92 ms    1.12 ms
     loss_all_reduce:    0.12 ms    0.12 ms
       meters_update:   15.57 ms   15.64 ms
            backward:    1.45 ms    1.50 ms
      optimizer_step:    0.52 ms    0.65 ms
    train_step_total:  779.03 ms  779.07 ms
INFO 2022-05-15 20:46:25,407 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 42.146}, 'top_5': {'res5': 70.55799999999999}}
INFO 2022-05-15 20:46:25,408 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:46:25,414 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:46:25,415 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 191, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 20:47:35,322 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 20:47:35,323 state_update_hooks.py: 115: Starting phase 191 [test]
INFO 2022-05-15 20:47:46,462 trainer_main.py: 214: Meters synced
INFO 2022-05-15 20:47:46,467 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 557
INFO 2022-05-15 20:47:46,468 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.84}, 'top_5': {'res5': 60.760000000000005}}
INFO 2022-05-15 20:47:46,469 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:47:46,475 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:47:46,475 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 192, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 20:49:02,361 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 20:49:02,362 state_update_hooks.py: 115: Starting phase 192 [train]
INFO 2022-05-15 20:50:14,068 trainer_main.py: 214: Meters synced
INFO 2022-05-15 20:50:14,073 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 731
INFO 2022-05-15 20:50:14,074 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  678.56 ms  678.60 ms
             forward:    6.36 ms   36.48 ms
        loss_compute:    0.97 ms    1.15 ms
     loss_all_reduce:    0.11 ms    0.11 ms
       meters_update:   10.58 ms   10.60 ms
            backward:    1.62 ms    1.79 ms
      optimizer_step:    0.57 ms    0.55 ms
    train_step_total:  731.47 ms  731.41 ms
INFO 2022-05-15 20:50:14,075 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 42.05}, 'top_5': {'res5': 70.71}}
INFO 2022-05-15 20:50:14,076 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:50:14,083 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:50:14,083 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 193, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 20:51:29,462 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 20:51:29,464 state_update_hooks.py: 115: Starting phase 193 [test]
INFO 2022-05-15 20:51:43,131 trainer_main.py: 214: Meters synced
INFO 2022-05-15 20:51:43,136 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 683
INFO 2022-05-15 20:51:43,137 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.92}, 'top_5': {'res5': 60.650000000000006}}
INFO 2022-05-15 20:51:43,138 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:51:43,145 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:51:43,146 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 194, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 20:53:01,052 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 20:53:01,053 state_update_hooks.py: 115: Starting phase 194 [train]
INFO 2022-05-15 20:54:09,157 log_hooks.py: 277: Rank: 0; [ep: 97] iter: 9600; lr: 0.003; loss: 2.25445; btime(ms): 2309; eta: 0:07:41; peak_mem(M): 2605;
INFO 2022-05-15 20:54:12,056 trainer_main.py: 214: Meters synced
INFO 2022-05-15 20:54:12,061 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 724
INFO 2022-05-15 20:54:12,063 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  679.47 ms  679.53 ms
             forward:    6.60 ms   32.46 ms
        loss_compute:    0.81 ms    0.81 ms
     loss_all_reduce:    0.12 ms    0.12 ms
       meters_update:    7.30 ms    7.33 ms
            backward:    1.76 ms    1.77 ms
      optimizer_step:    0.45 ms    0.45 ms
    train_step_total:  724.29 ms  724.29 ms
INFO 2022-05-15 20:54:12,064 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 42.302}, 'top_5': {'res5': 70.596}}
INFO 2022-05-15 20:54:12,064 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:54:12,072 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:54:12,072 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 195, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 20:55:31,603 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 20:55:31,604 state_update_hooks.py: 115: Starting phase 195 [test]
INFO 2022-05-15 20:55:44,856 trainer_main.py: 214: Meters synced
INFO 2022-05-15 20:55:44,862 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 662
INFO 2022-05-15 20:55:44,862 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.75}, 'top_5': {'res5': 60.61}}
INFO 2022-05-15 20:55:44,863 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:55:44,867 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:55:44,868 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 196, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 20:56:57,885 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 20:56:57,886 state_update_hooks.py: 115: Starting phase 196 [train]
INFO 2022-05-15 20:58:18,832 trainer_main.py: 214: Meters synced
INFO 2022-05-15 20:58:18,838 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 826
INFO 2022-05-15 20:58:18,839 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  767.12 ms  767.17 ms
             forward:    6.30 ms   38.15 ms
        loss_compute:    0.95 ms    1.22 ms
     loss_all_reduce:    0.13 ms    0.12 ms
       meters_update:   14.33 ms   14.35 ms
            backward:    1.46 ms    1.66 ms
      optimizer_step:    0.60 ms    0.65 ms
    train_step_total:  825.75 ms  825.79 ms
INFO 2022-05-15 20:58:18,839 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 42.246}, 'top_5': {'res5': 70.48400000000001}}
INFO 2022-05-15 20:58:18,840 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:58:18,846 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:58:18,847 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 197, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 20:59:36,857 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 20:59:36,858 state_update_hooks.py: 115: Starting phase 197 [test]
INFO 2022-05-15 20:59:49,124 trainer_main.py: 214: Meters synced
INFO 2022-05-15 20:59:49,128 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 613
INFO 2022-05-15 20:59:49,129 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.769999999999996}, 'top_5': {'res5': 60.75000000000001}}
INFO 2022-05-15 20:59:49,129 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:59:49,136 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 20:59:49,137 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 198, 'num_samples': 50000, 'total_size': 50000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 21:01:12,592 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 21:01:12,593 state_update_hooks.py: 115: Starting phase 198 [train]
INFO 2022-05-15 21:02:33,933 trainer_main.py: 214: Meters synced
INFO 2022-05-15 21:02:33,937 log_hooks.py: 568: Average train batch time (ms) for 98 batches: 830
INFO 2022-05-15 21:02:33,937 log_hooks.py: 577: Train step time breakdown (rank 0):
               Timer     Host    CudaEvent
         read_sample:  778.46 ms  778.55 ms
             forward:    6.21 ms   35.33 ms
        loss_compute:    1.05 ms    1.23 ms
     loss_all_reduce:    0.12 ms    0.12 ms
       meters_update:   10.24 ms   10.26 ms
            backward:    1.79 ms    1.90 ms
      optimizer_step:    0.44 ms    0.50 ms
    train_step_total:  829.77 ms  829.78 ms
INFO 2022-05-15 21:02:33,939 log_hooks.py: 498: Rank: 0, name: train_accuracy_list_meter, value: {'top_1': {'res5': 42.14}, 'top_5': {'res5': 70.57600000000001}}
INFO 2022-05-15 21:02:33,939 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 21:02:33,947 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 21:02:33,948 log_hooks.py: 425: [phase: 99] Saving checkpoint to ./checkpoints/LP/2
INFO 2022-05-15 21:02:34,095 checkpoint.py: 131: Saved checkpoint: ./checkpoints/LP/2/model_final_checkpoint_phase99.torch
INFO 2022-05-15 21:02:34,095 checkpoint.py: 140: Creating symlink...
INFO 2022-05-15 21:02:34,103 checkpoint.py: 144: Created symlink: ./checkpoints/LP/2/checkpoint.torch
INFO 2022-05-15 21:02:34,104 __init__.py: 101: Distributed Sampler config:
{'num_replicas': 1, 'rank': 0, 'epoch': 199, 'num_samples': 10000, 'total_size': 10000, 'shuffle': True, 'seed': 0}
INFO 2022-05-15 21:03:57,737 trainer_main.py: 333: Phase advanced. Rank: 0
INFO 2022-05-15 21:03:57,738 state_update_hooks.py: 115: Starting phase 199 [test]
INFO 2022-05-15 21:04:11,931 trainer_main.py: 214: Meters synced
INFO 2022-05-15 21:04:11,936 log_hooks.py: 568: Average test batch time (ms) for 20 batches: 709
INFO 2022-05-15 21:04:11,938 log_hooks.py: 498: Rank: 0, name: test_accuracy_list_meter, value: {'top_1': {'res5': 32.79}, 'top_5': {'res5': 60.75000000000001}}
INFO 2022-05-15 21:04:11,938 io.py:  63: Saving data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 21:04:11,946 io.py:  89: Saved data to file: ./checkpoints/LP/2/metrics.json
INFO 2022-05-15 21:04:12,044 train.py: 131: All Done!
INFO 2022-05-15 21:04:12,044 logger.py:  73: Shutting down loggers...
INFO 2022-05-15 21:04:12,051 distributed_launcher.py: 168: All Done!
INFO 2022-05-15 21:04:12,053 logger.py:  73: Shutting down loggers...
